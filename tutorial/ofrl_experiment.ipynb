{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluatin of OPE estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement data collection procedure on the RTBGym environment\n",
    "\n",
    "# import OFRL modules\n",
    "import ofrl\n",
    "from rtbgym import RTBEnv, CustomizedRTBEnv\n",
    "from ofrl.dataset import SyntheticDataset\n",
    "from ofrl.policy import OnlineHead, ContinuousEvalHead\n",
    "from ofrl.policy import ContinuousTruncatedGaussianHead as TruncatedGaussianHead\n",
    "from ofrl.ope.online import (\n",
    "    calc_on_policy_policy_value,\n",
    "    visualize_on_policy_policy_value,\n",
    ")\n",
    "from ofrl.utils import MinMaxScaler, MinMaxActionScaler\n",
    "\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.algos import RandomPolicy\n",
    "# from d3rlpy.preprocessing import MinMaxScaler, MinMaxActionScaler\n",
    "from ofrl.utils import MinMaxScaler, MinMaxActionScaler\n",
    "\n",
    "# import from other libraries\n",
    "import gym\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log directory\n",
    "from pathlib import Path\n",
    "Path(\"logs/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "random_state = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) Setup environment\n",
    "env = gym.make(\"RTBEnv-continuous-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for api compatibility to d3rlpy\n",
    "from ofrl.utils import OldGymAPIWrapper\n",
    "env_ = OldGymAPIWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behavior policy\n",
    "from d3rlpy.algos import SAC\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "\n",
    "# model\n",
    "sac = SAC(\n",
    "    actor_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    critic_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    action_scaler=MinMaxActionScaler(\n",
    "        minimum=env_.action_space.low,   # 0.1\n",
    "        maximum=env_.action_space.high,  # 10\n",
    "    ),\n",
    ")\n",
    "# setup replay buffer\n",
    "buffer = ReplayBuffer(\n",
    "    maxlen=10000,\n",
    "    env=env_,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac.fit_online(\n",
    "    env_,\n",
    "    buffer,\n",
    "    eval_env=env_,\n",
    "    n_steps=100000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    update_start_step=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "sac.save_model(\"d3rlpy_logs/sac.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 23:32.37 [warning  ] Parameters will be reinitialized.\n"
     ]
    }
   ],
   "source": [
    "# reload model\n",
    "sac.build_with_env(env_)\n",
    "sac.load_model(\"d3rlpy_logs/sac.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy = TruncatedGaussianHead(\n",
    "    sac, \n",
    "    minimum=env.action_space.low,\n",
    "    maximum=env.action_space.high,\n",
    "    sigma=np.array([1.0]),\n",
    "    name=\"sac_sigma_1.0\",\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset class\n",
    "dataset = SyntheticDataset(\n",
    "    env=env,\n",
    "    state_keys=env.obs_keys,\n",
    "    max_episode_steps=env.step_per_episode,\n",
    "    info_keys={\n",
    "        \"search_volume\": int,\n",
    "        \"impression\": int,\n",
    "        \"click\": int,\n",
    "        \"conversion\": int,\n",
    "        \"average_bid_price\": float,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect logged data by a behavior policy\n",
    "# skip if there is a preserved logged dataset\n",
    "train_logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000, \n",
    "    obtain_info=True,\n",
    "    random_state=random_state,\n",
    ")\n",
    "test_logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000, \n",
    "    obtain_info=True,\n",
    "    random_state=random_state + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/train_dataset_continuous_sac.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_logged_dataset, f)\n",
    "with open(\"logs/test_dataset_continuous_sac.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_logged_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/train_dataset_continuous_sac.pkl\", \"rb\") as f:\n",
    "    train_logged_dataset = pickle.load(f)\n",
    "with open(\"logs/test_dataset_continuous_sac.pkl\", \"rb\") as f:\n",
    "    test_logged_dataset = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Reinforce Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement offline RL procedure using ofrl and d3rlpy\n",
    "\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from d3rlpy.algos import DiscreteCQL\n",
    "\n",
    "# (3) Learning a new policy from offline logged data (using d3rlpy)\n",
    "# convert dataset into d3rlpy's dataset\n",
    "offlinerl_dataset = MDPDataset(\n",
    "    observations=train_logged_dataset[\"state\"],\n",
    "    actions=train_logged_dataset[\"action\"],\n",
    "    rewards=train_logged_dataset[\"reward\"],\n",
    "    terminals=train_logged_dataset[\"done\"],\n",
    "    episode_terminals=train_logged_dataset[\"done\"],\n",
    "    discrete_action=True,\n",
    ")\n",
    "# initialize the algorithm\n",
    "cql = DiscreteCQL()\n",
    "# train an offline policy\n",
    "cql.fit(\n",
    "    offlinerl_dataset,\n",
    "    n_steps=10000,\n",
    "    scorers={},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ope modules from OFRL\n",
    "from ofrl.ope import CreateOPEInput\n",
    "from ofrl.ope import OffPolicyEvaluation as OPE\n",
    "# basic estimators\n",
    "from ofrl.ope import ContinuousDirectMethod as DM\n",
    "from ofrl.ope import ContinuousTrajectoryWiseImportanceSampling as TIS\n",
    "from ofrl.ope import ContinuousPerDecisionImportanceSampling as PDIS\n",
    "from ofrl.ope import ContinuousDoublyRobust as DR\n",
    "# self normalized estimators\n",
    "from ofrl.ope import ContinuousSelfNormalizedTrajectoryWiseImportanceSampling as SNTIS\n",
    "from ofrl.ope import ContinuousSelfNormalizedPerDecisionImportanceSampling as SNPDIS\n",
    "from ofrl.ope import ContinuousSelfNormalizedDoublyRobust as SNDR\n",
    "# marginal estimators\n",
    "from ofrl.ope import ContinuousStateActionMarginalImportanceSampling as SAMIS\n",
    "from ofrl.ope import ContinuousStateActionMarginalDoublyRobust as SAMDR\n",
    "from ofrl.ope import ContinuousStateMarginalImportanceSampling as SMIS\n",
    "from ofrl.ope import ContinuousStateMarginalDoublyRobust as SMDR\n",
    "from ofrl.ope import ContinuousStateActionMarginalSelfNormalizedImportanceSampling as SAMSNIS\n",
    "from ofrl.ope import ContinuousStateActionMarginalSelfNormalizedDoublyRobust as SAMSNDR\n",
    "from ofrl.ope import ContinuousStateMarginalSelfNormalizedImportanceSampling as SMSNIS\n",
    "from ofrl.ope import ContinuousStateMarginalSelfNormalizedDoublyRobust as SMSNDR\n",
    "# double reinforcement learning estimators\n",
    "from ofrl.ope import ContinuousDoubleReinforcementLearning as DRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure configs\n",
    "query = \"(est == 'DM' or est == 'IPS') and num_data <= 6400\"\n",
    "xlabels = [100, 400, 1600, 6400]\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "sns.lineplot(\n",
    "    linewidth=5,\n",
    "    dashes=False,\n",
    "    legend=False,\n",
    "    x=\"num_data\",\n",
    "    y=\"se\",\n",
    "    hue=\"est\",\n",
    "    ax=ax,\n",
    "    data=result_df.query(\"(est == 'DM' or est == 'IPS') and num_data <= 12800\"),\n",
    ")\n",
    "# title and legend\n",
    "ax.legend([\"IPS\", \"DM\"], loc=\"upper right\", fontsize=25)\n",
    "# yaxis\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylabel(\"mean squared error (MSE)\", fontsize=25)\n",
    "ax.tick_params(axis=\"y\", labelsize=15)\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "# xaxis\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"number of samples in the log data\", fontsize=25)\n",
    "ax.set_xticks(xlabels)\n",
    "ax.set_xticklabels(xlabels, fontsize=15)\n",
    "ax.xaxis.set_label_coords(0.5, -0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Sep 10 2022, 14:58:52) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "70404ee114725fce8ed9e697d67827f8546c678889944e6d695790702cbfe1f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
