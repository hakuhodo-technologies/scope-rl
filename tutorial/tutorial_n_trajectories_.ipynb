{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "from d3rlpy.algos import SAC\n",
    "from d3rlpy.algos import DoubleDQN as DDQN\n",
    "from d3rlpy.algos import CQL\n",
    "from d3rlpy.algos import IQL\n",
    "from d3rlpy.algos import BCQ\n",
    "from d3rlpy.algos import DiscreteCQL\n",
    "from d3rlpy.algos import DiscreteBCQ\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy, ConstantEpsilonGreedy\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "\n",
    "from scope_rl.dataset import SyntheticDataset\n",
    "from scope_rl.policy import BaseHead\n",
    "from scope_rl.policy import GaussianHead\n",
    "from scope_rl.policy import EpsilonGreedyHead\n",
    "from scope_rl.policy import SoftmaxHead\n",
    "from scope_rl.policy import TrainCandidatePolicies\n",
    "\n",
    "from scope_rl.ope.online import visualize_on_policy_policy_value\n",
    "from scope_rl.ope.online import calc_on_policy_policy_value\n",
    "\n",
    "from scope_rl.utils import MinMaxActionScaler\n",
    "from scope_rl.utils import OldGymAPIWrapper\n",
    "from scope_rl.types import LoggedDataset\n",
    "\n",
    "from experiments.utils import torch_seed, format_runtime\n",
    "\n",
    "from basicgym import BasicEnv\n",
    "\n",
    "from tutorial.function import train_behavior_policy\n",
    "from tutorial.function import obtain_logged_dataset\n",
    "from tutorial.function import train_candidate_policies\n",
    "# from experiments.main import off_policy_evaluation\n",
    "from tutorial.function import off_policy_evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(\n",
    "    bias_result_df,\n",
    "    variance_result_df,\n",
    "    mse_result_df,\n",
    "    estimators,\n",
    "    ESTIMATORS,\n",
    "    x_scales,\n",
    "    x_label,\n",
    "    log_dir=\"../tutorial/logs/\",\n",
    "    yscale_log = False,\n",
    "    xscale_log = False,\n",
    "):\n",
    "\n",
    "    color_dict = {\n",
    "        \"dm\": \"#E24A33\",\n",
    "        \"tis\": \"#348ABD\",\n",
    "        \"pdis\": \"#988ED5\",\n",
    "        \"dr\": \"#777777\",\n",
    "        \"sntis\": \"#8EBA42\",\n",
    "        \"snpdis\": \"#FBC15E\",\n",
    "        \"sndr\": \"#FFB5B8\",\n",
    "        \"sm_is\": \"#FFA726\",\n",
    "        \"sm_dr\": \"#17BECF\",\n",
    "        \"sm_snis\": \"#F781BF\",\n",
    "        \"sm_sndr\": \"#8C564B\",\n",
    "        # \"sam_is\": \"#00FF00\",\n",
    "        \"sam_is\": \"#FFA726\",\n",
    "        \"sam_dr\": \"#008080\",\n",
    "        \"sam_snis\": \"#000080\",\n",
    "        \"sam_sndr\": \"#4B0082\",\n",
    "        # \"drl\": \"#800000\"\n",
    "        \"drl\": \"#F781BF\"\n",
    "    }\n",
    "    # color_dict = {\n",
    "    #     \"red\": \"#E24A33\",\n",
    "    #     \"blue\": \"#348ABD\",\n",
    "    #     \"purple\": \"#988ED5\",\n",
    "    #     \"gray\": \"#777777\",\n",
    "    #     \"green\": \"#8EBA42\",\n",
    "    #     \"yellow\": \"#FBC15E\",\n",
    "    #     \"pink\": \"#FFB5B8\",\n",
    "    #     \"brown\": \"#8c564b\",\n",
    "    #     \"light blue\": \"#17becf\",\n",
    "    #     \"olive\": \"#bcbd22\",\n",
    "    # }\n",
    "    # cd = color_dict\n",
    "\n",
    "    plt.style.use(\"ggplot\")\n",
    "    # colors = [cd[\"red\"], cd[\"blue\"], cd[\"purple\"], cd[\"gray\"], cd[\"yellow\"], cd['green'], cd['pink'], cd['brown'], cd['light blue'], cd['olive']]\n",
    "    # plt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)\n",
    "    markers = [\"o\", \"v\", \"^\", \"s\", \"p\", \"P\", \"*\", \"h\", \"X\", \"D\", \"d\"]\n",
    "    # colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    # n_colors = len(colors)\n",
    "\n",
    "    log_dir=log_dir\n",
    "\n",
    "    metric_list = ['bias', 'variance', 'mse']\n",
    "\n",
    "    for metric in metric_list:\n",
    "        plt.style.use('ggplot')\n",
    "        fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "        if metric =='bias':\n",
    "            result_df = bias_result_df\n",
    "        elif metric == 'variance':\n",
    "            result_df = variance_result_df\n",
    "        else:\n",
    "            result_df = mse_result_df\n",
    "\n",
    "        for i, estimator in enumerate(estimators):\n",
    "            data = result_df[result_df['est']==estimator]\n",
    "            data = data.query(f\"({min(x_scales)}<= {x_label} <= {max(x_scales)})\")\n",
    "            ax.plot(\n",
    "                np.array(x_scales),\n",
    "                data[metric],\n",
    "                color=color_dict[estimator],\n",
    "                marker=markers[i],\n",
    "                label=ESTIMATORS[i],\n",
    "            )\n",
    "            \n",
    "            ax.legend(ESTIMATORS, loc=\"upper right\", fontsize=20)\n",
    "\n",
    "            ax.fill_between(\n",
    "                np.array(x_scales),\n",
    "                data['lower'],\n",
    "                data['upper'],\n",
    "                color=color_dict[estimator],\n",
    "                alpha=0.3,\n",
    "                label='',\n",
    "            )\n",
    "\n",
    "        # title and legend\n",
    "        ax.legend(loc=\"upper right\", fontsize=20)\n",
    "        # yaxis\n",
    "        if yscale_log:\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.set_ylabel(metric, fontsize=25)\n",
    "        ax.tick_params(axis=\"y\", labelsize=15)\n",
    "        ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "        # xaxis\n",
    "        if xscale_log:\n",
    "            ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(f\"number of trajectories\", fontsize=25)\n",
    "        # ax.set_xlabel(f\"number of {x_label}\", fontsize=25)\n",
    "        ax.set_xticks(x_scales)\n",
    "        ax.set_xticklabels(x_scales, fontsize=15)\n",
    "        ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "        path_ = Path(log_dir + \"/results/fig\")\n",
    "        path_.mkdir(exist_ok=True, parents=True)\n",
    "        save_path = Path(path_ / f\"{metric}_result_fig_{x_label}_{ESTIMATORS}.png\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    variable_name,\n",
    "    n_random_state,\n",
    "    log_dir=\"../tutorial/logs/logs_n_trajectories\",\n",
    "    behavior_tau = 1.0,\n",
    "    candidate_epsilons = [1.0],\n",
    "    behavior_sigma = 1.0,\n",
    "    candidate_sigmas = [1.0],\n",
    "    action_type = 'discrete',\n",
    "):\n",
    "    #discrete\n",
    "    env_name=\"BasicEnv-discrete-v0\"\n",
    "    action_type='discrete'\n",
    "    behavior_policy_name=f\"ddqn_softmax_{behavior_tau}\"\n",
    "    candidate_policy_name=f\"cql_eps_{candidate_epsilons[0]}\"\n",
    "\n",
    "    #continuous\n",
    "    # env_name=\"BasicEnv-continuous-v0\"\n",
    "    # action_type='continuous'\n",
    "    # behavior_policy_name=f\"sac_gauss_{behavior_sigma}\"\n",
    "    # candidate_policy_name=f\"cql_b1_gauss_{candidate_sigmas}\"\n",
    "\n",
    "    base_random_state=12345\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    step_per_trajectory_list = [10, 15, 25, 40]\n",
    "    # step_per_trajectory_list =  [5, 10, 20, 40, 80]\n",
    "    step_per_trajectory = 10\n",
    "    \n",
    "    # n_trajectories_list=[3200, 4800, 6400]\n",
    "    # n_trajectories_list=[1600, 3200, 6400, 12800]\n",
    "    n_trajectories_list=[1600, 3200, 6400, 12800, 25600]\n",
    "    n_trajectories=8000\n",
    "    n_actions_list = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "    n_actions=5\n",
    "    n_step_pdis=0\n",
    "\n",
    "    bias_df_list = []\n",
    "    variance_df_list = []\n",
    "    mse_df_list = []\n",
    "\n",
    "    random_ = check_random_state(base_random_state)\n",
    "    n_bootstrap_samples = 100\n",
    "    alpha=0.05\n",
    "\n",
    "    if variable_name == 'n_trajectories':\n",
    "        variable_list = n_trajectories_list\n",
    "    elif variable_name == 'n_actions':\n",
    "        variable_list = n_actions_list\n",
    "    elif variable_name == 'step_per_trajectory':\n",
    "        variable_list = step_per_trajectory_list\n",
    "    else:\n",
    "        ValueError\n",
    "\n",
    "\n",
    "    for variable in variable_list:\n",
    "\n",
    "        if variable_name == 'n_trajectories':\n",
    "            n_trajectories = variable\n",
    "        elif variable_name == 'n_actions':\n",
    "            n_actions == variable\n",
    "        elif variable_name == 'step_per_trajectory':\n",
    "            step_per_trajectory =variable\n",
    "\n",
    "        env = BasicEnv(\n",
    "            action_type=action_type, \n",
    "            n_actions=n_actions,\n",
    "            random_state=base_random_state, \n",
    "            step_per_episode=step_per_trajectory,\n",
    "        )\n",
    "\n",
    "        behavior_policy = train_behavior_policy(\n",
    "            env_name=env_name,\n",
    "            env=env,\n",
    "            behavior_sigma=behavior_sigma,\n",
    "            behavior_tau=behavior_tau,\n",
    "            device=device,\n",
    "            base_random_state=base_random_state,\n",
    "            log_dir=log_dir,\n",
    "            variable=variable,\n",
    "            variable_name=variable_name,\n",
    "        )\n",
    "\n",
    "        train_logged_dataset, test_logged_dataset = obtain_logged_dataset(\n",
    "            env_name=env_name,\n",
    "            env=env,\n",
    "            behavior_policy=behavior_policy,\n",
    "            n_trajectories=n_trajectories,\n",
    "            n_random_state=n_random_state,\n",
    "            base_random_state=base_random_state,\n",
    "            log_dir=log_dir,\n",
    "            variable=variable,\n",
    "            variable_name=variable_name,\n",
    "        )\n",
    "\n",
    "        candidate_policies = train_candidate_policies(\n",
    "            env_name=env_name,\n",
    "            env=env,\n",
    "            n_trajectories=n_trajectories,\n",
    "            train_logged_dataset=train_logged_dataset,\n",
    "            candidate_sigmas=candidate_sigmas,\n",
    "            candidate_epsilons=candidate_epsilons,\n",
    "            device=device,\n",
    "            base_random_state=base_random_state,\n",
    "            log_dir=log_dir,\n",
    "            variable=variable,\n",
    "            variable_name=variable_name,\n",
    "        )\n",
    "\n",
    "        input_dict, policy_value_dict = off_policy_evaluation(\n",
    "            env_name=env_name,\n",
    "            env=env,\n",
    "            n_trajectories=n_trajectories,\n",
    "            test_logged_dataset=test_logged_dataset,\n",
    "            candidate_policies=candidate_policies,\n",
    "            n_step_pdis=n_step_pdis,\n",
    "            device=device,\n",
    "            base_random_state=base_random_state,\n",
    "            log_dir=log_dir,\n",
    "            variable=variable,\n",
    "            variable_name=variable_name,\n",
    "        )\n",
    "\n",
    "        input_dict_ = input_dict.get(\n",
    "            behavior_policy_name=behavior_policy_name,\n",
    "            dataset_id=0,\n",
    "        )\n",
    "\n",
    "        dict = {i : DataFrame() for i in input_dict_.keys()}\n",
    "        bias_dict = {i : 0 for i in input_dict_.keys()}\n",
    "        variance_dict = {i : 0 for i in input_dict_.keys()}\n",
    "        mse_dict = {i : 0 for i in input_dict_.keys()}\n",
    "        lower_bias = []\n",
    "        upper_bias = []\n",
    "        lower_variance = []\n",
    "        upper_variance = []\n",
    "        lower_mse = []\n",
    "        upper_mse = []\n",
    "\n",
    "        for dataset_id_ in range(n_random_state):\n",
    "            for eval_policy in input_dict_.keys():\n",
    "                dict[eval_policy] = pd.concat([dict[eval_policy] , DataFrame(policy_value_dict[behavior_policy_name][dataset_id_][eval_policy], index=[dataset_id_])])\n",
    "\n",
    "        for eval_policy in input_dict_.keys():\n",
    "            bias_dict[eval_policy] = abs(dict[eval_policy].mean(axis=0) - dict[eval_policy].mean(axis=0)['on_policy'])\n",
    "            variance_dict[eval_policy] = dict[eval_policy].var(axis=0)\n",
    "            mse_dict[eval_policy] = bias_dict[eval_policy]**2 + variance_dict[eval_policy]\n",
    "\n",
    "        for estimator in dict[eval_policy].columns.values:\n",
    "            samples = dict[eval_policy][estimator]\n",
    "            # samples = np.nan_to_num(samples, posinf=1e2)\n",
    "            # samples = np.clip(samples, 0.0, 1e2)\n",
    "            boot_samples_bias = [\n",
    "                np.mean(random_.choice(samples - dict[eval_policy].mean(axis=0)['on_policy'], size=samples.shape[0]))\n",
    "                for i in range(n_bootstrap_samples)\n",
    "            ]\n",
    "            boot_samples_bias = list(map(abs, boot_samples_bias))\n",
    "\n",
    "            boot_samples_variance = [\n",
    "                np.var(random_.choice(samples, size=samples.shape[0]), ddof=1)\n",
    "                for i in range(n_bootstrap_samples)\n",
    "            ]\n",
    "            boot_samples_mse = boot_samples_bias**2 + boot_samples_variance\n",
    "\n",
    "            lower_bias.append(np.percentile(boot_samples_bias, 100 * (alpha / 2)))\n",
    "            upper_bias.append(np.percentile(boot_samples_bias, 100 * (1.0 - alpha / 2)))\n",
    "            lower_variance.append(np.percentile(boot_samples_variance, 100 * (alpha / 2)))\n",
    "            upper_variance.append(np.percentile(boot_samples_variance, 100 * (1.0 - alpha / 2)))\n",
    "            lower_mse.append(np.percentile(boot_samples_mse, 100 * (alpha / 2)))\n",
    "            upper_mse.append(np.percentile(boot_samples_mse, 100 * (1.0 - alpha / 2)))\n",
    "\n",
    "\n",
    "        bias_df = DataFrame(DataFrame(bias_dict[candidate_policy_name]).stack())\\\n",
    "        .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"bias\"})\n",
    "        bias_df['lower']=lower_bias\n",
    "        bias_df['upper']=upper_bias\n",
    "        bias_df[variable_name] = variable\n",
    "        bias_df_list.append(bias_df)\n",
    "        variance_df = DataFrame(DataFrame(variance_dict[candidate_policy_name]).stack())\\\n",
    "        .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"variance\"})\n",
    "        variance_df['lower']=lower_variance\n",
    "        variance_df['upper']=upper_variance\n",
    "        variance_df[variable_name] = variable\n",
    "        variance_df_list.append(variance_df)\n",
    "        mse_df = DataFrame(DataFrame(mse_dict[candidate_policy_name]).stack())\\\n",
    "        .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"mse\"})\n",
    "        mse_df['lower']=lower_mse\n",
    "        mse_df['upper']=upper_mse\n",
    "        mse_df[variable_name] = variable\n",
    "        mse_df_list.append(mse_df)\n",
    "\n",
    "\n",
    "    # aggregate all results \n",
    "    bias_result_df = pd.concat(bias_df_list).reset_index(level=0)\n",
    "    variance_result_df = pd.concat(variance_df_list).reset_index(level=0)\n",
    "    mse_result_df = pd.concat(mse_df_list).reset_index(level=0)\n",
    "    \n",
    "\n",
    "    path_ = Path(log_dir + f\"/results/df\")\n",
    "    path_.mkdir(exist_ok=True, parents=True)\n",
    "    path_bias = Path(path_ / f\"bias_result_df_{variable_name}.pkl\")\n",
    "    path_variance = Path(path_ / f\"variance_result_df_{variable_name}.pkl\")\n",
    "    path_mse = Path(path_ / f\"mse_result_df_{variable_name}.pkl\")\n",
    "\n",
    "    with open(path_bias, \"wb\") as f:\n",
    "        pickle.dump(bias_result_df, f)\n",
    "    with open(path_variance, \"wb\") as f:\n",
    "        pickle.dump(variance_result_df, f)\n",
    "    with open(path_mse, \"wb\") as f:\n",
    "        pickle.dump(mse_result_df, f)\n",
    "\n",
    "    return bias_result_df, variance_result_df, mse_result_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path_ = Path(\"logs_tau=0.0\" + f\"/results/df\")\n",
    "# path_ = Path(\"logs_epsilon=0.5\" + f\"/results/df\")\n",
    "\n",
    "# # variable_name='step_per_trajectory'\n",
    "# variable_name='n_trajectories'\n",
    "# # variable_name='n_actions'\n",
    "\n",
    "\n",
    "# path_bias = Path(path_ / f\"bias_result_df_{variable_name}.pkl\")\n",
    "# path_variance = Path(path_ / f\"variance_result_df_{variable_name}.pkl\")\n",
    "# path_mse = Path(path_ / f\"mse_result_df_{variable_name}.pkl\")\n",
    "# with open(path_bias, \"rb\") as f:\n",
    "#     bias_result_df = pickle.load(f)\n",
    "# with open(path_variance, \"rb\") as f:\n",
    "#     variance_result_df = pickle.load(f)\n",
    "# with open(path_mse, \"rb\") as f:\n",
    "#     mse_result_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_estimators = [\"DM\", \"TIS\", \"PDIS\", \"DR\", \"SNTIS\", \"SNPDIS\", \"SNDR\"]\n",
    "state_marginal_estimators = [\"SMIS\", \"SMDR\", \"SMSNIS\", \"SMSNDR\"]\n",
    "state_action_marginal_estimators = [\"SAMIS\", \"SAMDR\", \"SAMSNIS\", \"SAMSNDR\"]\n",
    "drl_estimators = [\"DRL\"]\n",
    "all_estimators = basic_estimators + state_marginal_estimators + state_action_marginal_estimators + drl_estimators\n",
    "\n",
    "basic_estimators_name = [\"dm\", \"tis\", \"pdis\", \"dr\", \"sntis\", \"snpdis\", \"sndr\"]\n",
    "state_marginal_estimators_name = [\"sm_is\", \"sm_dr\", \"sm_snis\", \"sm_sndr\"]\n",
    "state_action_marginal_estimators_name = [\"sam_is\", \"sam_dr\", \"sam_snis\", \"sam_sndr\"]\n",
    "drl_estimators_name = [\"drl\"]\n",
    "all_estimators_name = basic_estimators_name + state_marginal_estimators_name + state_action_marginal_estimators_name + drl_estimators_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_result_df, variance_result_df, mse_result_df =main(\n",
    "    variable_name = \"n_trajectories\",\n",
    "    n_random_state = 100,\n",
    "    log_dir=\"../tutorial/logs/logs_n_trajectories\",\n",
    "    behavior_tau = 3.0,\n",
    "    # behavior_tau = 3.5,\n",
    "    candidate_epsilons = [0.5],\n",
    "    behavior_sigma = 1.0,\n",
    "    candidate_sigmas = [1.0],\n",
    "    action_type = 'discrete',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias_result_df, variance_result_df, mse_result_df =main(\n",
    "#     variable_name = \"n_trajectories\",\n",
    "#     n_random_state = 2,\n",
    "#     log_dir=\"../tutorial/logs_epsilon=0.5/\",\n",
    "#     behavior_tau = 0.0,\n",
    "#     candidate_epsilons = [0.5],\n",
    "#     behavior_sigma = 1.0,\n",
    "#     candidate_sigmas = [1.0],\n",
    "#     action_type = 'discrete',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ESTIMATORS=[\"DM\", \"TIS\"]\n",
    "# estimators=[\"dm\", \"tis\"]\n",
    "# ESTIMATORS=[\"TIS\", \"PDIS\"]\n",
    "# estimators=[\"tis\", \"pdis\"]\n",
    "# ESTIMATORS=[\"DR\", \"PDIS\"]\n",
    "# estimators=[\"dr\", \"pdis\"]\n",
    "# ESTIMATORS = [\"PDIS\", \"SAMIS\"]\n",
    "# estimators = [\"pdis\", \"sam_is\"]\n",
    "# ESTIMATORS = [\"SAMDR\", \"DRL\"]\n",
    "# estimators = [\"sam_dr\", \"drl\"]\n",
    "ESTIMATORS = [\"SAMDR\", \"DRL\"]\n",
    "estimators = [\"sam_dr\", \"drl\"]\n",
    "# ESTIMATORS = [\"DR\", \"SAMDR\", \"DRL\"]\n",
    "# estimators = [\"dr\", \"sam_dr\", \"drl\"]\n",
    "# ESTIMATORS = [\"SAMDR\"]\n",
    "# estimators = [\"sam_dr\"]\n",
    "# ESTIMATORS = [\"SAMIS\"]\n",
    "# estimators = [\"sam_is\"]\n",
    "\n",
    "# index = basic_estimators_name.index('dm')\n",
    "# basic_estimators.pop(index)\n",
    "# basic_estimators_name.pop(index)\n",
    "\n",
    "# ESTIMATORS=basic_estimators\n",
    "# estimators=basic_estimators_name\n",
    "# x_scales=[ 3200,4800, 6400]\n",
    "# x_scales=[200, 400, 800, 1600, 3200,4800, 6400, 8000]\n",
    "# x_scales=[1600, 3200, 6400, 12800]\n",
    "x_scales=[1600, 3200, 6400, 12800, 25600]\n",
    "x_label='n_trajectories'\n",
    "# x_scales=[2, 4, 6, 10, 12, 14, 16, 18, 20]\n",
    "# x_scales=[2, 4, 6, 8, 10, 12, 14, 16]\n",
    "# x_label='n_actions'\n",
    "# x_scales=[10, 15, 25, 40]\n",
    "# x_label='step_per_trajectory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(\n",
    "    bias_result_df,\n",
    "    variance_result_df,\n",
    "    mse_result_df,\n",
    "    ESTIMATORS=ESTIMATORS,\n",
    "    estimators=estimators,\n",
    "    x_scales=x_scales,\n",
    "    x_label=x_label,\n",
    "    # yscale_log=True,\n",
    "    xscale_log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# tmp_log_dir = \"logs\"\n",
    "# path_ = Path(tmp_log_dir + f\"/results/df\")\n",
    "\n",
    "# # variable_name='step_per_trajectory'\n",
    "# variable_name='n_trajectories'\n",
    "# # variable_name='n_actions'\n",
    "\n",
    "# path_bias = Path(path_ / f\"bias_result_df_{variable_name}.pkl\")\n",
    "# path_variance = Path(path_ / f\"variance_result_df_{variable_name}.pkl\")\n",
    "# path_mse = Path(path_ / f\"mse_result_df_{variable_name}.pkl\")\n",
    "# with open(path_bias, \"rb\") as f:\n",
    "#     bias_result_df = pickle.load(f)\n",
    "# with open(path_variance, \"rb\") as f:\n",
    "#     variance_result_df = pickle.load(f)\n",
    "# with open(path_mse, \"rb\") as f:\n",
    "#     mse_result_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias_result_df[\"sam_is\"] = \"sope\"\n",
    "# row_samis = bias_result_df.query('est == \"sope\" and n_steps_pdis == 0')\n",
    "# row_samis[\"est\"]=\"sam_is\"\n",
    "# n_steps_pdis_list = [0, 5, 10, 15]\n",
    "# for n_steps_pdis in n_steps_pdis_list:\n",
    "#     row_samis[\"n_steps_pdis\"]=n_steps_pdis\n",
    "#     bias_result_df = pd.concat([bias_result_df, row_samis])\n",
    "\n",
    "# # bias_result_df[\"sam_is\"] = \"sope\"\n",
    "# # row_samis = bias_result_df.query('est == \"sope\" and n_trajectories == 1600')\n",
    "# # row_samis[\"est\"]=\"sam_is\"\n",
    "# # n_trajectories_list = [1600, 3200, 6400, 12800, 25600]\n",
    "# # for n_trajectories in n_trajectories_list:\n",
    "# #     row_samis[\"n_trajectories\"]=n_trajectories\n",
    "# #     # bias_result_df.loc[len(bias_result_df)] = row_samis\n",
    "# #     bias_result_df = pd.concat([bias_result_df, row_samis])\n",
    "\n",
    "\n",
    "\n",
    "# # row_samis = bias_result_df[bias_result_df[\"est\"] == \"sam_is\", bias_result_df[\"n_steps_pdis\"] == 0]\n",
    "# # row_samis[\"est\"]=\"sope\"\n",
    "# # for n_steps_pdis in n_steps_pdis_list:\n",
    "# #     row_samis[\"n_steps_pdis\"]=n_steps_pdis\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "negocia_ofrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
