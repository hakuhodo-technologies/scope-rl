{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from d3rlpy.algos import SAC\n",
    "from d3rlpy.algos import DoubleDQN as DDQN\n",
    "from d3rlpy.algos import CQL\n",
    "from d3rlpy.algos import IQL\n",
    "from d3rlpy.algos import BCQ\n",
    "from d3rlpy.algos import DiscreteCQL\n",
    "from d3rlpy.algos import DiscreteBCQ\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy, ConstantEpsilonGreedy\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "\n",
    "from scope_rl.dataset import SyntheticDataset\n",
    "from scope_rl.policy import BaseHead\n",
    "from scope_rl.policy import ContinuousGaussianHead as GaussianHead\n",
    "from scope_rl.policy import DiscreteEpsilonGreedyHead as EpsilonGreedyHead\n",
    "from scope_rl.policy import DiscreteSoftmaxHead as SoftmaxHead\n",
    "from scope_rl.policy import OffPolicyLearning\n",
    "\n",
    "from scope_rl.ope.online import visualize_on_policy_policy_value\n",
    "from scope_rl.ope.online import calc_on_policy_policy_value\n",
    "\n",
    "from scope_rl.utils import MinMaxActionScaler\n",
    "from scope_rl.utils import OldGymAPIWrapper\n",
    "from scope_rl.types import LoggedDataset\n",
    "\n",
    "from experiments.utils import torch_seed, format_runtime\n",
    "\n",
    "from basicgym import BasicEnv\n",
    "\n",
    "from tutorial.function import train_behavior_policy\n",
    "from tutorial.function import obtain_logged_dataset\n",
    "from tutorial.function import train_candidate_policies\n",
    "# from experiments.main import off_policy_evaluation\n",
    "from tutorial.function import off_policy_evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_fig(\n",
    "    bias_result_df,\n",
    "    estimators,\n",
    "    ESTIMATORS,\n",
    "    x_scales,\n",
    "    x_label,\n",
    "    yscale_log = False,\n",
    "    xscale_log = False,\n",
    "):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "    sns.lineplot(\n",
    "        linewidth=5,\n",
    "        dashes=False,\n",
    "        legend=False,\n",
    "        x=x_label,\n",
    "        y=\"bias\",\n",
    "        hue=\"est\",\n",
    "        ax=ax,\n",
    "        data=bias_result_df.query(f\"(est == {estimators} and {min(x_scales)}<= {x_label} <= {max(x_scales)})\"),\n",
    "        ci=None,\n",
    "    )\n",
    "    # title and legend\n",
    "    ax.legend(ESTIMATORS, loc=\"upper right\", fontsize=25)\n",
    "    # yaxis\n",
    "    if yscale_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"bias\", fontsize=25)\n",
    "    ax.tick_params(axis=\"y\", labelsize=15)\n",
    "    ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "    # xaxis\n",
    "    if xscale_log:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(f\"number of {x_label}\", fontsize=25)\n",
    "    ax.set_xticks(x_scales)\n",
    "    ax.set_xticklabels(x_scales, fontsize=15)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    path_ = Path(log_dir + \"results/fig\")\n",
    "    path_.mkdir(exist_ok=True, parents=True)\n",
    "    save_path = Path(path_ / f\"bias_result_fig_{x_label}.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_fig(\n",
    "    variance_result_df,\n",
    "    estimators,\n",
    "    ESTIMATORS,\n",
    "    x_scales,\n",
    "    x_label,\n",
    "    yscale_log = False,\n",
    "    xscale_log = False,\n",
    "):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "    sns.lineplot(\n",
    "        linewidth=5,\n",
    "        dashes=False,\n",
    "        legend=False,\n",
    "        x=x_label,\n",
    "        y=\"variance\",\n",
    "        hue=\"est\",\n",
    "        ax=ax,\n",
    "        data=variance_result_df.query(f\"(est == {estimators} and {min(x_scales)}<= {x_label} <= {max(x_scales)})\"),\n",
    "        ci=None,\n",
    "    )\n",
    "    # title and legend\n",
    "    ax.legend(ESTIMATORS, loc=\"upper right\", fontsize=25)\n",
    "    # yaxis\n",
    "    if yscale_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"variance\", fontsize=25)\n",
    "    ax.tick_params(axis=\"y\", labelsize=15)\n",
    "    ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "    # xaxis\n",
    "    if xscale_log:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(f\"number of {x_label}\", fontsize=25)\n",
    "    ax.set_xticks(x_scales)\n",
    "    ax.set_xticklabels(x_scales, fontsize=15)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    path_ = Path(log_dir + \"results/fig\")\n",
    "    path_.mkdir(exist_ok=True, parents=True)\n",
    "    save_path = Path(path_ / f\"variance_result_fig_{x_label}.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_fig(\n",
    "    mse_result_df,\n",
    "    estimators,\n",
    "    ESTIMATORS,\n",
    "    x_scales,\n",
    "    x_label,\n",
    "    yscale_log = False,\n",
    "    xscale_log = False,\n",
    "):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "    sns.lineplot(\n",
    "        linewidth=5,\n",
    "        dashes=False,\n",
    "        legend=False,\n",
    "        x=x_label,\n",
    "        y=\"mse\",\n",
    "        hue=\"est\",\n",
    "        ax=ax,\n",
    "        data=mse_result_df.query(f\"(est == {estimators} and {min(x_scales)}<= {x_label} <= {max(x_scales)})\"),\n",
    "        ci=None,\n",
    "    )\n",
    "    # title and legend\n",
    "    ax.legend(ESTIMATORS, loc=\"upper right\", fontsize=25)\n",
    "    # yaxis\n",
    "    if yscale_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"mse\", fontsize=25)\n",
    "    ax.tick_params(axis=\"y\", labelsize=15)\n",
    "    ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "    # xaxis\n",
    "    if xscale_log:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(f\"number of {x_label}\", fontsize=25)\n",
    "    ax.set_xticks(x_scales)\n",
    "    ax.set_xticklabels(x_scales, fontsize=15)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    path_ = Path(log_dir + \"results/fig\")\n",
    "    path_.mkdir(exist_ok=True, parents=True)\n",
    "    save_path = Path(path_ / f\"mse_result_fig_{x_label}.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f010cc534e1443e4b0fbc5c06947fcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9ca8326f024f9cbb81da010d3f3f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_datasets: dataset_id]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7582c15152694a33a259ef43b246f4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7609339ac44644838b6448ddd02dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5503006ac64c55ab0b2c205c87ba4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feadc21ca7d484e9fa0d6a51e90ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa74799b7a824d1a90d085a9ce25cf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d1660b6ba543cc817dc31427a36c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env = gym.make(env_name)\n",
    "#discrete\n",
    "env_name=\"BasicEnv-discrete-v0\"\n",
    "action_type='discrete'\n",
    "behavior_policy_name=\"ddqn_softmax_0.0\"\n",
    "candidate_policy_name=\"cql_b1_eps_0.0\"\n",
    "# behavior_policy_name=\"ddqn_softmax_1.0\"\n",
    "# candidate_policy_name=\"cql_b1_eps_0.1\"\n",
    "\n",
    "#continuous\n",
    "# env_name=\"BasicEnv-continuous-v0\"\n",
    "# action_type='continuous'\n",
    "# behavior_policy_name=\"sac_gauss_1.0\"\n",
    "# candidate_policy_name=\"cql_b1_gauss_0.0\"\n",
    "\n",
    "# behavior_tau=1.0\n",
    "# candidate_epsilons=[0.1]\n",
    "behavior_tau=0.0\n",
    "candidate_epsilons=[0.0]\n",
    "behavior_sigma=0.0\n",
    "candidate_sigmas=[0.0]\n",
    "\n",
    "base_random_state=12345\n",
    "log_dir=\"../tutorial/logs/\"\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "step_per_trajectory_list =  [5, 10, 20, 40, 60, 80, 100]\n",
    "step_per_trajectory = 10\n",
    "n_trajectories_list=[200, 400, 800, 1600, 3200, 4800, 6400, 8000]\n",
    "n_trajectories=10000\n",
    "n_actions_list = [2, 4, 6, 10, 12, 14]\n",
    "n_actions=5\n",
    "# n_actions=5\n",
    "n_random_state=10\n",
    "\n",
    "bias_df_list = []\n",
    "variance_df_list = []\n",
    "mse_df_list = []\n",
    "\n",
    "# variable_name = 'n_trajectories'\n",
    "variable_name = 'n_actions'\n",
    "# variable_name = 'step_per_trajectory'\n",
    "\n",
    "# for n_trajectories in n_trajectories_list:\n",
    "for n_actions in n_actions_list:\n",
    "# for step_per_trajectory in step_per_trajectory_list:\n",
    "\n",
    "    # variable=n_trajectories\n",
    "    variable=n_actions\n",
    "    # variable=step_per_trajectory\n",
    "\n",
    "    env = BasicEnv(\n",
    "        action_type=action_type, \n",
    "        n_actions=n_actions,\n",
    "        random_state=base_random_state, \n",
    "        step_per_episode=step_per_trajectory,\n",
    "    )\n",
    "\n",
    "    behavior_policy = train_behavior_policy(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        behavior_sigma=behavior_sigma,\n",
    "        behavior_tau=behavior_tau,\n",
    "        device=device,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    train_logged_dataset, test_logged_dataset = obtain_logged_dataset(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        behavior_policy=behavior_policy,\n",
    "        n_trajectories=n_trajectories,\n",
    "        n_random_state=n_random_state,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    candidate_policies = train_candidate_policies(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        n_trajectories=n_trajectories,\n",
    "        train_logged_dataset=train_logged_dataset,\n",
    "        candidate_sigmas=candidate_sigmas,\n",
    "        candidate_epsilons=candidate_epsilons,\n",
    "        device=device,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    input_dict, policy_value_dict = off_policy_evaluation(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        n_trajectories=n_trajectories,\n",
    "        test_logged_dataset=test_logged_dataset,\n",
    "        candidate_policies=candidate_policies,\n",
    "        device=device,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    input_dict_ = input_dict.get(\n",
    "        behavior_policy_name=behavior_policy_name,\n",
    "        dataset_id=0,\n",
    "    )\n",
    "\n",
    "    dict = {i : DataFrame() for i in input_dict_.keys()}\n",
    "    bias_dict = {i : 0 for i in input_dict_.keys()}\n",
    "    variance_dict = {i : 0 for i in input_dict_.keys()}\n",
    "    mse_dict = {i : 0 for i in input_dict_.keys()}\n",
    "\n",
    "    for dataset_id_ in range(n_random_state):\n",
    "        for eval_policy in input_dict_.keys():\n",
    "            dict[eval_policy] = pd.concat([dict[eval_policy] , DataFrame(policy_value_dict[behavior_policy_name][dataset_id_][eval_policy], index=[dataset_id_])])\n",
    "\n",
    "    for eval_policy in input_dict_.keys():\n",
    "        bias_dict[eval_policy] = abs(dict[eval_policy].mean(axis=0) - dict[eval_policy].mean(axis=0)['on_policy'])\n",
    "        variance_dict[eval_policy] = dict[eval_policy].var(axis=0)\n",
    "        mse_dict[eval_policy] = bias_dict[eval_policy]**2 + variance_dict[eval_policy]\n",
    "\n",
    "    bias_df = DataFrame(DataFrame(bias_dict[candidate_policy_name]).stack())\\\n",
    "    .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"bias\"})\n",
    "    bias_df[variable_name] = variable\n",
    "    bias_df_list.append(bias_df)\n",
    "    variance_df = DataFrame(DataFrame(variance_dict[candidate_policy_name]).stack())\\\n",
    "    .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"variance\"})\n",
    "    variance_df[variable_name] = variable\n",
    "    variance_df_list.append(variance_df)\n",
    "    mse_df = DataFrame(DataFrame(mse_dict[candidate_policy_name]).stack())\\\n",
    "    .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"mse\"})\n",
    "    mse_df[variable_name] = variable\n",
    "    mse_df_list.append(mse_df)\n",
    "\n",
    "# aggregate all results \n",
    "bias_result_df = pd.concat(bias_df_list).reset_index(level=0)\n",
    "variance_result_df = pd.concat(variance_df_list).reset_index(level=0)\n",
    "mse_result_df = pd.concat(mse_df_list).reset_index(level=0)\n",
    "\n",
    "path_ = Path(\"logs\" + f\"/results/df\")\n",
    "path_.mkdir(exist_ok=True, parents=True)\n",
    "path_bias = Path(path_ / f\"bias_result_df_{variable_name}.pkl\")\n",
    "path_variance = Path(path_ / f\"variance_result_df_{variable_name}.pkl\")\n",
    "path_mse = Path(path_ / f\"mse_result_df_{variable_name}.pkl\")\n",
    "\n",
    "with open(path_bias, \"wb\") as f:\n",
    "    pickle.dump(bias_result_df, f)\n",
    "with open(path_variance, \"wb\") as f:\n",
    "    pickle.dump(variance_result_df, f)\n",
    "with open(path_mse, \"wb\") as f:\n",
    "    pickle.dump(mse_result_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(dict['cql_b1_eps_0.1'].mean(axis=0) - dict['cql_b1_eps_0.1'].mean(axis=0)['on_policy'])\n",
    "# (dict['cql_b1_eps_0.1'].sub(dict['cql_b1_eps_0.1']['on_policy'], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([bias_result_df.drop('bias', axis=1), bias_result_df['bias'], variance_result_df['variance'], mse_result_df['mse']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_estimators = [\"DM\", \"TIS\", \"PDIS\", \"DR\", \"SNTIS\", \"SNPDIS\", \"SNDR\"]\n",
    "state_marginal_estimators = [\"SMIS\", \"SMDR\", \"SMSNIS\", \"SMSNDR\"]\n",
    "state_action_marginal_estimators = [\"SAMIS\", \"SAMDR\", \"SAMSNIS\", \"SAMSNDR\"]\n",
    "drl_estimators = [\"DRL\"]\n",
    "all_estimators = basic_estimators + state_marginal_estimators + state_action_marginal_estimators + drl_estimators\n",
    "\n",
    "basic_estimators_name = [\"dm\", \"tis\", \"pdis\", \"dr\", \"sntis\", \"snpdis\", \"sndr\"]\n",
    "state_marginal_estimators_name = [\"sm_is\", \"sm_dr\", \"sm_snis\", \"sm_sndr\"]\n",
    "state_action_marginal_estimators_name = [\"sam_is\", \"sam_dr\", \"sam_snis\", \"sam_sndr\"]\n",
    "drl_estimators_name = [\"drl\"]\n",
    "all_estimators_name = basic_estimators_name + state_marginal_estimators_name + state_action_marginal_estimators_name + drl_estimators_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_trajectories\n",
    "# x_scales=[200, 400, 800, 1600, 3200,4800, 6400, 8000]\n",
    "# x_label='n_trajectories'\n",
    "\n",
    "# step_per_trajectory\n",
    "# x_scales=[5, 10, 20, 40, 60, 80, 100]\n",
    "# x_label='step_per_trajectory'\n",
    "\n",
    "n_actions\n",
    "x_scales=[4, 6, 8, 10, 12, 14]\n",
    "x_label='n_actions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_fig(\n",
    "    bias_result_df,\n",
    "    ESTIMATORS=basic_estimators,\n",
    "    estimators=basic_estimators_name,\n",
    "    x_scales=x_scales,\n",
    "    x_label=x_label,\n",
    "    # yscale_log=True,\n",
    "    xscale_log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_fig(\n",
    "    variance_result_df,\n",
    "    ESTIMATORS=basic_estimators,\n",
    "    estimators=basic_estimators_name,\n",
    "    x_scales=x_scales,\n",
    "    x_label=x_label,\n",
    "    # yscale_log=True,\n",
    "    # xscale_log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fig(\n",
    "    mse_result_df,\n",
    "    ESTIMATORS=basic_estimators,\n",
    "    estimators=basic_estimators_name,\n",
    "    x_scales=x_scales,\n",
    "    x_label=x_label,\n",
    "    # xscale_log=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "negocia_ofrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
