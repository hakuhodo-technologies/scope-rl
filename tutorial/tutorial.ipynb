{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from d3rlpy.algos import SAC\n",
    "from d3rlpy.algos import DoubleDQN as DDQN\n",
    "from d3rlpy.algos import CQL\n",
    "from d3rlpy.algos import IQL\n",
    "from d3rlpy.algos import BCQ\n",
    "from d3rlpy.algos import DiscreteCQL\n",
    "from d3rlpy.algos import DiscreteBCQ\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy, ConstantEpsilonGreedy\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "\n",
    "from scope_rl.dataset import SyntheticDataset\n",
    "from scope_rl.policy import BaseHead\n",
    "from scope_rl.policy import ContinuousGaussianHead as GaussianHead\n",
    "from scope_rl.policy import DiscreteEpsilonGreedyHead as EpsilonGreedyHead\n",
    "from scope_rl.policy import DiscreteSoftmaxHead as SoftmaxHead\n",
    "from scope_rl.policy import OffPolicyLearning\n",
    "\n",
    "from scope_rl.ope.online import visualize_on_policy_policy_value\n",
    "from scope_rl.ope.online import calc_on_policy_policy_value\n",
    "\n",
    "from scope_rl.utils import MinMaxActionScaler\n",
    "from scope_rl.utils import OldGymAPIWrapper\n",
    "from scope_rl.types import LoggedDataset\n",
    "\n",
    "from experiments.utils import torch_seed, format_runtime\n",
    "\n",
    "from basicgym import BasicEnv\n",
    "\n",
    "from tutorial.function import train_behavior_policy\n",
    "from tutorial.function import obtain_logged_dataset\n",
    "from tutorial.function import train_candidate_policies\n",
    "# from experiments.main import off_policy_evaluation\n",
    "from tutorial.function import off_policy_evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_fig(\n",
    "    bias_result_df,\n",
    "    estimators,\n",
    "    ESTIMATORS,\n",
    "    x_scales,\n",
    "    x_label,\n",
    "    yscale_log = False,\n",
    "    xscale_log = False,\n",
    "):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "    sns.lineplot(\n",
    "        linewidth=5,\n",
    "        dashes=False,\n",
    "        legend=False,\n",
    "        x=x_label,\n",
    "        y=\"bias\",\n",
    "        hue=\"est\",\n",
    "        ax=ax,\n",
    "        data=bias_result_df.query(f\"(est == {estimators} and {min(x_scales)}<= {x_label} <= {max(x_scales)})\"),\n",
    "        ci=None,\n",
    "    )\n",
    "    # title and legend\n",
    "    ax.legend(ESTIMATORS, loc=\"upper right\", fontsize=25)\n",
    "    # yaxis\n",
    "    if yscale_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"bias\", fontsize=25)\n",
    "    ax.tick_params(axis=\"y\", labelsize=15)\n",
    "    ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "    # xaxis\n",
    "    if xscale_log:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(f\"number of {x_label}\", fontsize=25)\n",
    "    ax.set_xticks(x_scales)\n",
    "    ax.set_xticklabels(x_scales, fontsize=15)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    path_ = Path(log_dir + \"results/fig\")\n",
    "    path_.mkdir(exist_ok=True, parents=True)\n",
    "    save_path = Path(path_ / f\"bias_result_fig_{x_label}.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_fig(\n",
    "    variance_result_df,\n",
    "    estimators,\n",
    "    ESTIMATORS,\n",
    "    x_scales,\n",
    "    x_label,\n",
    "    yscale_log = False,\n",
    "    xscale_log = False,\n",
    "):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "    sns.lineplot(\n",
    "        linewidth=5,\n",
    "        dashes=False,\n",
    "        legend=False,\n",
    "        x=x_label,\n",
    "        y=\"variance\",\n",
    "        hue=\"est\",\n",
    "        ax=ax,\n",
    "        data=variance_result_df.query(f\"(est == {estimators} and {min(x_scales)}<= {x_label} <= {max(x_scales)})\"),\n",
    "        ci=None,\n",
    "    )\n",
    "    # title and legend\n",
    "    ax.legend(ESTIMATORS, loc=\"upper right\", fontsize=25)\n",
    "    # yaxis\n",
    "    if yscale_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"variance\", fontsize=25)\n",
    "    ax.tick_params(axis=\"y\", labelsize=15)\n",
    "    ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "    # xaxis\n",
    "    if xscale_log:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(f\"number of {x_label}\", fontsize=25)\n",
    "    ax.set_xticks(x_scales)\n",
    "    ax.set_xticklabels(x_scales, fontsize=15)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    path_ = Path(log_dir + \"results/fig\")\n",
    "    path_.mkdir(exist_ok=True, parents=True)\n",
    "    save_path = Path(path_ / f\"variance_result_fig_{x_label}.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_fig(\n",
    "    mse_result_df,\n",
    "    estimators,\n",
    "    ESTIMATORS,\n",
    "    x_scales,\n",
    "    x_label,\n",
    "    yscale_log = False,\n",
    "    xscale_log = False,\n",
    "):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "    sns.lineplot(\n",
    "        linewidth=5,\n",
    "        dashes=False,\n",
    "        legend=False,\n",
    "        x=x_label,\n",
    "        y=\"mse\",\n",
    "        hue=\"est\",\n",
    "        ax=ax,\n",
    "        data=mse_result_df.query(f\"(est == {estimators} and {min(x_scales)}<= {x_label} <= {max(x_scales)})\"),\n",
    "        ci=None,\n",
    "    )\n",
    "    # title and legend\n",
    "    ax.legend(ESTIMATORS, loc=\"upper right\", fontsize=25)\n",
    "    # yaxis\n",
    "    if yscale_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"mse\", fontsize=25)\n",
    "    ax.tick_params(axis=\"y\", labelsize=15)\n",
    "    ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "    # xaxis\n",
    "    if xscale_log:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(f\"number of {x_label}\", fontsize=25)\n",
    "    ax.set_xticks(x_scales)\n",
    "    ax.set_xticklabels(x_scales, fontsize=15)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    path_ = Path(log_dir + \"results/fig\")\n",
    "    path_.mkdir(exist_ok=True, parents=True)\n",
    "    save_path = Path(path_ / f\"mse_result_fig_{x_label}.png\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f010cc534e1443e4b0fbc5c06947fcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9ca8326f024f9cbb81da010d3f3f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_datasets: dataset_id]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7582c15152694a33a259ef43b246f4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7609339ac44644838b6448ddd02dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5503006ac64c55ab0b2c205c87ba4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feadc21ca7d484e9fa0d6a51e90ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa74799b7a824d1a90d085a9ce25cf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d1660b6ba543cc817dc31427a36c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476dc3969b134ab6b6fbb468bc01d209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e519ff1f81304bf5a284b2ce9f399cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9a87b9a0de4824a9c4cf9e7d04cfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860308a6b7cf4f718fc560c3150659f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b125efbb0e4553a23124741076594e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[collect input data: behavior_policy]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d96519f111144798e662ac32cab92d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[collect input data: dataset_id]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f1c9583a70466f89c32c88742ed843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[fit FQE model]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-09 00:44.40 [debug    ] RoundIterator is selected.\n",
      "2023-06-09 00:44.40 [info     ] Directory is created at d3rlpy_logs/DiscreteFQE_20230609004440\n",
      "2023-06-09 00:44.40 [debug    ] Fitting scaler...              scaler=min_max\n",
      "2023-06-09 00:44.40 [debug    ] Building models...\n",
      "2023-06-09 00:44.40 [debug    ] Models have been built.\n",
      "2023-06-09 00:44.40 [info     ] Parameters are saved to d3rlpy_logs/DiscreteFQE_20230609004440/params.json params={'action_scaler': None, 'batch_size': 100, 'encoder_factory': {'type': 'vector', 'params': {'hidden_units': [30, 30], 'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None, 'use_dense': False}}, 'gamma': 1.0, 'generated_maxlen': 100000, 'learning_rate': 0.0001, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': {'type': 'min_max', 'params': {'maximum': array([0.99322107, 0.98846274, 0.99608767, 0.9884822 , 0.99590335]), 'minimum': array([-0.97585907, -0.98118773, -0.98361222, -0.99968848, -0.98213745])}}, 'target_update_interval': 100, 'use_gpu': None, 'algorithm': 'DiscreteFQE', 'observation_shape': (5,), 'action_size': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc18088026c4e578d3858b36a968e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-09 00:44.42 [info     ] DiscreteFQE_20230609004440: epoch=1 step=1000 epoch=1 metrics={'time_sample_batch': 0.00019209098815917968, 'time_algorithm_update': 0.002013280391693115, 'loss': 0.29073379988968373, 'time_step': 0.002331352472305298} step=1000\n",
      "2023-06-09 00:44.42 [info     ] Model parameters are saved to d3rlpy_logs/DiscreteFQE_20230609004440/model_1000.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b64773cb5534d548d177550d5f177a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[fit Augmented Lagrangian model]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334ce9b661814a39a269a5606d63e668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[fitting_weight_and_value_functions]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798ebc63981a4e34ab25b987ea5fa7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[epoch:    0]:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=   0, objective_loss=0.810, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82de2842b5324a1181b7456dcb7205b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[fitting_weight_and_value_functions]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e78020710146d1b66ce817407df7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[epoch:    0]:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=   0, objective_loss=0.831, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297cca5bc1cb4e5189a77045d47ca769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[collect input data: eval_policy]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb セル 10\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m train_logged_dataset, test_logged_dataset \u001b[39m=\u001b[39m obtain_logged_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     env_name\u001b[39m=\u001b[39menv_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     env\u001b[39m=\u001b[39menv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     variable_name\u001b[39m=\u001b[39mvariable_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m candidate_policies \u001b[39m=\u001b[39m train_candidate_policies(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     env_name\u001b[39m=\u001b[39menv_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     env\u001b[39m=\u001b[39menv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     variable_name\u001b[39m=\u001b[39mvariable_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m input_dict, policy_value_dict \u001b[39m=\u001b[39m off_policy_evaluation(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     env_name\u001b[39m=\u001b[39;49menv_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     n_trajectories\u001b[39m=\u001b[39;49mn_trajectories,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     test_logged_dataset\u001b[39m=\u001b[39;49mtest_logged_dataset,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     candidate_policies\u001b[39m=\u001b[39;49mcandidate_policies,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     base_random_state\u001b[39m=\u001b[39;49mbase_random_state,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     log_dir\u001b[39m=\u001b[39;49mlog_dir,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     variable\u001b[39m=\u001b[39;49mvariable,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     variable_name\u001b[39m=\u001b[39;49mvariable_name,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m input_dict_ \u001b[39m=\u001b[39m input_dict\u001b[39m.\u001b[39mget(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     behavior_policy_name\u001b[39m=\u001b[39mbehavior_policy_name,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     dataset_id\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ren/dev/scope-rl/tutorial/tutorial.ipynb#W4sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m {i : DataFrame() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m input_dict_\u001b[39m.\u001b[39mkeys()}\n",
      "File \u001b[0;32m~/dev/scope-rl/tutorial/../tutorial/function.py:467\u001b[0m, in \u001b[0;36moff_policy_evaluation\u001b[0;34m(env_name, env, n_trajectories, test_logged_dataset, candidate_policies, device, base_random_state, log_dir, variable, variable_name)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     prep \u001b[39m=\u001b[39m CreateOPEInput(\n\u001b[1;32m    441\u001b[0m         env\u001b[39m=\u001b[39menv,\n\u001b[1;32m    442\u001b[0m         model_args\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m         device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    465\u001b[0m     )\n\u001b[0;32m--> 467\u001b[0m input_dict \u001b[39m=\u001b[39m prep\u001b[39m.\u001b[39;49mobtain_whole_inputs(\n\u001b[1;32m    468\u001b[0m     logged_dataset\u001b[39m=\u001b[39;49mtest_logged_dataset,\n\u001b[1;32m    469\u001b[0m     evaluation_policies\u001b[39m=\u001b[39;49mcandidate_policies,\n\u001b[1;32m    470\u001b[0m     require_value_prediction\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    471\u001b[0m     require_weight_prediction\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    472\u001b[0m     n_trajectories_on_policy_evaluation\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m    473\u001b[0m     path\u001b[39m=\u001b[39;49mlog_dir \u001b[39m+\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/input_dict/multiple/\u001b[39;49m\u001b[39m{\u001b[39;49;00mvariable_name\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mvariable\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    474\u001b[0m     random_state\u001b[39m=\u001b[39;49mbase_random_state,\n\u001b[1;32m    475\u001b[0m )\n\u001b[1;32m    476\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path_input_dict, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    477\u001b[0m     pickle\u001b[39m.\u001b[39mdump(input_dict, f)\n",
      "File \u001b[0;32m~/negocia_ofrl/lib/python3.10/site-packages/scope_rl-0.0.0-py3.10.egg/scope_rl/ope/input.py:2907\u001b[0m, in \u001b[0;36mCreateOPEInput.obtain_whole_inputs\u001b[0;34m(self, logged_dataset, evaluation_policies, behavior_policy_name, dataset_id, require_value_prediction, require_weight_prediction, resample_initial_state, q_function_method, v_function_method, w_function_method, k_fold, n_epochs, n_steps_per_epoch, n_trajectories_on_policy_evaluation, use_stationary_distribution_on_policy_evaluation, minimum_rollout_length, maximum_rollout_length, random_state, path, save_relative_path)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         \u001b[39mfor\u001b[39;00m dataset_id_ \u001b[39min\u001b[39;00m tqdm(\n\u001b[1;32m   2899\u001b[0m             np\u001b[39m.\u001b[39marange(n_datasets),\n\u001b[1;32m   2900\u001b[0m             desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[collect input data: dataset_id]\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2901\u001b[0m             total\u001b[39m=\u001b[39mn_datasets,\n\u001b[1;32m   2902\u001b[0m         ):\n\u001b[1;32m   2903\u001b[0m             logged_dataset_ \u001b[39m=\u001b[39m logged_dataset\u001b[39m.\u001b[39mget(\n\u001b[1;32m   2904\u001b[0m                 behavior_policy_name\u001b[39m=\u001b[39mbehavior_policies[i],\n\u001b[1;32m   2905\u001b[0m                 dataset_id\u001b[39m=\u001b[39mdataset_id_,\n\u001b[1;32m   2906\u001b[0m             )\n\u001b[0;32m-> 2907\u001b[0m             input_dict_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_obtain_whole_inputs(\n\u001b[1;32m   2908\u001b[0m                 logged_dataset\u001b[39m=\u001b[39;49mlogged_dataset_,\n\u001b[1;32m   2909\u001b[0m                 evaluation_policies\u001b[39m=\u001b[39;49mevaluation_policies[\n\u001b[1;32m   2910\u001b[0m                     behavior_policies[i]\n\u001b[1;32m   2911\u001b[0m                 ][dataset_id_],\n\u001b[1;32m   2912\u001b[0m                 require_value_prediction\u001b[39m=\u001b[39;49mrequire_value_prediction,\n\u001b[1;32m   2913\u001b[0m                 require_weight_prediction\u001b[39m=\u001b[39;49mrequire_weight_prediction,\n\u001b[1;32m   2914\u001b[0m                 resample_initial_state\u001b[39m=\u001b[39;49mresample_initial_state,\n\u001b[1;32m   2915\u001b[0m                 q_function_method\u001b[39m=\u001b[39;49mq_function_method,\n\u001b[1;32m   2916\u001b[0m                 v_function_method\u001b[39m=\u001b[39;49mv_function_method,\n\u001b[1;32m   2917\u001b[0m                 w_function_method\u001b[39m=\u001b[39;49mw_function_method,\n\u001b[1;32m   2918\u001b[0m                 k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m   2919\u001b[0m                 n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[1;32m   2920\u001b[0m                 n_steps_per_epoch\u001b[39m=\u001b[39;49mn_steps_per_epoch,\n\u001b[1;32m   2921\u001b[0m                 n_trajectories_on_policy_evaluation\u001b[39m=\u001b[39;49mn_trajectories_on_policy_evaluation,\n\u001b[1;32m   2922\u001b[0m                 use_stationary_distribution_on_policy_evaluation\u001b[39m=\u001b[39;49muse_stationary_distribution_on_policy_evaluation,\n\u001b[1;32m   2923\u001b[0m                 minimum_rollout_length\u001b[39m=\u001b[39;49mminimum_rollout_length,\n\u001b[1;32m   2924\u001b[0m                 maximum_rollout_length\u001b[39m=\u001b[39;49mmaximum_rollout_length,\n\u001b[1;32m   2925\u001b[0m                 random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m   2926\u001b[0m             )\n\u001b[1;32m   2927\u001b[0m             input_dict\u001b[39m.\u001b[39madd(\n\u001b[1;32m   2928\u001b[0m                 input_dict_,\n\u001b[1;32m   2929\u001b[0m                 behavior_policy_name\u001b[39m=\u001b[39mbehavior_policies[i],\n\u001b[1;32m   2930\u001b[0m                 dataset_id\u001b[39m=\u001b[39mdataset_id_,\n\u001b[1;32m   2931\u001b[0m             )\n\u001b[1;32m   2933\u001b[0m \u001b[39melif\u001b[39;00m behavior_policy_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dataset_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/negocia_ofrl/lib/python3.10/site-packages/scope_rl-0.0.0-py3.10.egg/scope_rl/ope/input.py:2481\u001b[0m, in \u001b[0;36mCreateOPEInput._obtain_whole_inputs\u001b[0;34m(self, logged_dataset, evaluation_policies, require_value_prediction, require_weight_prediction, resample_initial_state, q_function_method, v_function_method, w_function_method, k_fold, n_epochs, n_steps_per_epoch, n_trajectories_on_policy_evaluation, use_stationary_distribution_on_policy_evaluation, minimum_rollout_length, maximum_rollout_length, random_state)\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[39m# input for DM, DR\u001b[39;00m\n\u001b[1;32m   2478\u001b[0m \u001b[39mif\u001b[39;00m require_value_prediction:\n\u001b[1;32m   2479\u001b[0m     input_dict[evaluation_policies[i]\u001b[39m.\u001b[39mname][\n\u001b[1;32m   2480\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstate_action_value_prediction\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2481\u001b[0m     ] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobtain_state_action_value_prediction(\n\u001b[1;32m   2482\u001b[0m         method\u001b[39m=\u001b[39;49mq_function_method,\n\u001b[1;32m   2483\u001b[0m         evaluation_policy\u001b[39m=\u001b[39;49mevaluation_policies[i],\n\u001b[1;32m   2484\u001b[0m         k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[1;32m   2486\u001b[0m     input_dict[evaluation_policies[i]\u001b[39m.\u001b[39mname][\n\u001b[1;32m   2487\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minitial_state_value_prediction\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2488\u001b[0m     ] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobtain_initial_state_value_prediction(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2495\u001b[0m         random_state\u001b[39m=\u001b[39mrandom_state,\n\u001b[1;32m   2496\u001b[0m     )\n\u001b[1;32m   2497\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/negocia_ofrl/lib/python3.10/site-packages/scope_rl-0.0.0-py3.10.egg/scope_rl/ope/input.py:1761\u001b[0m, in \u001b[0;36mCreateOPEInput.obtain_state_action_value_prediction\u001b[0;34m(self, method, evaluation_policy, k_fold)\u001b[0m\n\u001b[1;32m   1756\u001b[0m a_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_actions), x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m   1758\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfqe\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1759\u001b[0m     state_action_value_prediction_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfqe[evaluation_policy\u001b[39m.\u001b[39;49mname][\n\u001b[1;32m   1760\u001b[0m         k\n\u001b[0;32m-> 1761\u001b[0m     ]\u001b[39m.\u001b[39;49mpredict_value(x_, a_)\n\u001b[1;32m   1763\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdice_q\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1764\u001b[0m     state_action_value_prediction_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_action_dual_function[\n\u001b[1;32m   1765\u001b[0m         evaluation_policy\u001b[39m.\u001b[39mname\n\u001b[1;32m   1766\u001b[0m     ][k]\u001b[39m.\u001b[39mpredict_value(x_, a_)\n",
      "File \u001b[0;32m~/negocia_ofrl/lib/python3.10/site-packages/d3rlpy/algos/base.py:169\u001b[0m, in \u001b[0;36mAlgoBase.predict_value\u001b[0;34m(self, x, action, with_std)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m\"\"\"Returns predicted action-values.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[39m.. code-block:: python\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, IMPL_NOT_INITIALIZED_ERROR\n\u001b[0;32m--> 169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_impl\u001b[39m.\u001b[39;49mpredict_value(x, action, with_std)\n",
      "File \u001b[0;32m~/negocia_ofrl/lib/python3.10/site-packages/d3rlpy/torch_utility.py:305\u001b[0m, in \u001b[0;36meval_api.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    304\u001b[0m     set_eval_mode(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/negocia_ofrl/lib/python3.10/site-packages/d3rlpy/torch_utility.py:295\u001b[0m, in \u001b[0;36mtorch_api.<locals>._torch_api.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    294\u001b[0m     tensors\u001b[39m.\u001b[39mappend(tensor)\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49mtensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/negocia_ofrl/lib/python3.10/site-packages/d3rlpy/algos/torch/utility.py:46\u001b[0m, in \u001b[0;36mDiscreteQFunctionMixin.predict_value\u001b[0;34m(self, x, action, with_std)\u001b[0m\n\u001b[1;32m     44\u001b[0m ret_stds \u001b[39m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m v, std, a \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(mean_values, stds, action):\n\u001b[0;32m---> 46\u001b[0m     ret_values\u001b[39m.\u001b[39mappend(v[a])\n\u001b[1;32m     47\u001b[0m     ret_stds\u001b[39m.\u001b[39mappend(std[a])\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m with_std:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "# env = gym.make(env_name)\n",
    "#discrete\n",
    "env_name=\"BasicEnv-discrete-v0\"\n",
    "action_type='discrete'\n",
    "behavior_policy_name=\"ddqn_softmax_0.0\"\n",
    "candidate_policy_name=\"cql_b1_eps_0.0\"\n",
    "# behavior_policy_name=\"ddqn_softmax_1.0\"\n",
    "# candidate_policy_name=\"cql_b1_eps_0.1\"\n",
    "\n",
    "#continuous\n",
    "# env_name=\"BasicEnv-continuous-v0\"\n",
    "# action_type='continuous'\n",
    "# behavior_policy_name=\"sac_gauss_1.0\"\n",
    "# candidate_policy_name=\"cql_b1_gauss_0.0\"\n",
    "\n",
    "# behavior_tau=1.0\n",
    "# candidate_epsilons=[0.1]\n",
    "behavior_tau=0.0\n",
    "candidate_epsilons=[0.0]\n",
    "behavior_sigma=0.0\n",
    "candidate_sigmas=[0.0]\n",
    "\n",
    "base_random_state=12345\n",
    "log_dir=\"../tutorial/logs/\"\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "step_per_trajectory_list =  [5, 10, 20, 40, 60, 80, 100]\n",
    "step_per_trajectory = 10\n",
    "n_trajectories_list=[200, 400, 800, 1600, 3200, 4800, 6400, 8000]\n",
    "n_trajectories=10000\n",
    "n_actions_list = [2, 4, 6, 10, 12, 14]\n",
    "n_actions=5\n",
    "# n_actions=5\n",
    "n_random_state=10\n",
    "\n",
    "bias_df_list = []\n",
    "variance_df_list = []\n",
    "mse_df_list = []\n",
    "\n",
    "# variable_name = 'n_trajectories'\n",
    "variable_name = 'n_actions'\n",
    "# variable_name = 'step_per_trajectory'\n",
    "\n",
    "# for n_trajectories in n_trajectories_list:\n",
    "for n_actions in n_actions_list:\n",
    "# for step_per_trajectory in step_per_trajectory_list:\n",
    "\n",
    "    # variable=n_trajectories\n",
    "    variable=n_actions\n",
    "    # variable=step_per_trajectory\n",
    "\n",
    "    env = BasicEnv(\n",
    "        action_type=action_type, \n",
    "        n_actions=n_actions,\n",
    "        random_state=base_random_state, \n",
    "        step_per_episode=step_per_trajectory,\n",
    "    )\n",
    "\n",
    "    behavior_policy = train_behavior_policy(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        behavior_sigma=behavior_sigma,\n",
    "        behavior_tau=behavior_tau,\n",
    "        device=device,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    train_logged_dataset, test_logged_dataset = obtain_logged_dataset(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        behavior_policy=behavior_policy,\n",
    "        n_trajectories=n_trajectories,\n",
    "        n_random_state=n_random_state,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    candidate_policies = train_candidate_policies(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        n_trajectories=n_trajectories,\n",
    "        train_logged_dataset=train_logged_dataset,\n",
    "        candidate_sigmas=candidate_sigmas,\n",
    "        candidate_epsilons=candidate_epsilons,\n",
    "        device=device,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    input_dict, policy_value_dict = off_policy_evaluation(\n",
    "        env_name=env_name,\n",
    "        env=env,\n",
    "        n_trajectories=n_trajectories,\n",
    "        test_logged_dataset=test_logged_dataset,\n",
    "        candidate_policies=candidate_policies,\n",
    "        device=device,\n",
    "        base_random_state=base_random_state,\n",
    "        log_dir=log_dir,\n",
    "        variable=variable,\n",
    "        variable_name=variable_name,\n",
    "    )\n",
    "\n",
    "    input_dict_ = input_dict.get(\n",
    "        behavior_policy_name=behavior_policy_name,\n",
    "        dataset_id=0,\n",
    "    )\n",
    "\n",
    "    dict = {i : DataFrame() for i in input_dict_.keys()}\n",
    "    bias_dict = {i : 0 for i in input_dict_.keys()}\n",
    "    variance_dict = {i : 0 for i in input_dict_.keys()}\n",
    "    mse_dict = {i : 0 for i in input_dict_.keys()}\n",
    "\n",
    "    for dataset_id_ in range(n_random_state):\n",
    "        for eval_policy in input_dict_.keys():\n",
    "            dict[eval_policy] = pd.concat([dict[eval_policy] , DataFrame(policy_value_dict[behavior_policy_name][dataset_id_][eval_policy], index=[dataset_id_])])\n",
    "\n",
    "    for eval_policy in input_dict_.keys():\n",
    "        bias_dict[eval_policy] = abs(dict[eval_policy].mean(axis=0) - dict[eval_policy].mean(axis=0)['on_policy'])\n",
    "        variance_dict[eval_policy] = dict[eval_policy].var(axis=0)\n",
    "        mse_dict[eval_policy] = bias_dict[eval_policy]**2 + variance_dict[eval_policy]\n",
    "\n",
    "    bias_df = DataFrame(DataFrame(bias_dict[candidate_policy_name]).stack())\\\n",
    "    .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"bias\"})\n",
    "    bias_df[variable_name] = variable\n",
    "    bias_df_list.append(bias_df)\n",
    "    variance_df = DataFrame(DataFrame(variance_dict[candidate_policy_name]).stack())\\\n",
    "    .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"variance\"})\n",
    "    variance_df[variable_name] = variable\n",
    "    variance_df_list.append(variance_df)\n",
    "    mse_df = DataFrame(DataFrame(mse_dict[candidate_policy_name]).stack())\\\n",
    "    .reset_index(0).rename(columns={\"level_0\": \"est\", 0: \"mse\"})\n",
    "    mse_df[variable_name] = variable\n",
    "    mse_df_list.append(mse_df)\n",
    "\n",
    "# aggregate all results \n",
    "bias_result_df = pd.concat(bias_df_list).reset_index(level=0)\n",
    "variance_result_df = pd.concat(variance_df_list).reset_index(level=0)\n",
    "mse_result_df = pd.concat(mse_df_list).reset_index(level=0)\n",
    "\n",
    "path_ = Path(\"logs\" + f\"/results/df\")\n",
    "path_.mkdir(exist_ok=True, parents=True)\n",
    "path_bias = Path(path_ / f\"bias_result_df_{variable_name}.pkl\")\n",
    "path_variance = Path(path_ / f\"variance_result_df_{variable_name}.pkl\")\n",
    "path_mse = Path(path_ / f\"mse_result_df_{variable_name}.pkl\")\n",
    "\n",
    "with open(path_bias, \"wb\") as f:\n",
    "    pickle.dump(bias_result_df, f)\n",
    "with open(path_variance, \"wb\") as f:\n",
    "    pickle.dump(variance_result_df, f)\n",
    "with open(path_mse, \"wb\") as f:\n",
    "    pickle.dump(mse_result_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(dict['cql_b1_eps_0.1'].mean(axis=0) - dict['cql_b1_eps_0.1'].mean(axis=0)['on_policy'])\n",
    "# (dict['cql_b1_eps_0.1'].sub(dict['cql_b1_eps_0.1']['on_policy'], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([bias_result_df.drop('bias', axis=1), bias_result_df['bias'], variance_result_df['variance'], mse_result_df['mse']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_estimators = [\"DM\", \"TIS\", \"PDIS\", \"DR\", \"SNTIS\", \"SNPDIS\", \"SNDR\"]\n",
    "state_marginal_estimators = [\"SMIS\", \"SMDR\", \"SMSNIS\", \"SMSNDR\"]\n",
    "state_action_marginal_estimators = [\"SAMIS\", \"SAMDR\", \"SAMSNIS\", \"SAMSNDR\"]\n",
    "drl_estimators = [\"DRL\"]\n",
    "all_estimators = basic_estimators + state_marginal_estimators + state_action_marginal_estimators + drl_estimators\n",
    "\n",
    "basic_estimators_name = [\"dm\", \"tis\", \"pdis\", \"dr\", \"sntis\", \"snpdis\", \"sndr\"]\n",
    "state_marginal_estimators_name = [\"sm_is\", \"sm_dr\", \"sm_snis\", \"sm_sndr\"]\n",
    "state_action_marginal_estimators_name = [\"sam_is\", \"sam_dr\", \"sam_snis\", \"sam_sndr\"]\n",
    "drl_estimators_name = [\"drl\"]\n",
    "all_estimators_name = basic_estimators_name + state_marginal_estimators_name + state_action_marginal_estimators_name + drl_estimators_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_trajectories\n",
    "# x_scales=[200, 400, 800, 1600, 3200,4800, 6400, 8000]\n",
    "# x_label='n_trajectories'\n",
    "\n",
    "# step_per_trajectory\n",
    "# x_scales=[5, 10, 20, 40, 60, 80, 100]\n",
    "# x_label='step_per_trajectory'\n",
    "\n",
    "n_actions\n",
    "x_scales=[4, 6, 8, 10, 12, 14]\n",
    "x_label='n_actions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_fig(\n",
    "    bias_result_df,\n",
    "    ESTIMATORS=basic_estimators,\n",
    "    estimators=basic_estimators_name,\n",
    "    x_scales=x_scales,\n",
    "    x_label=x_label,\n",
    "    # yscale_log=True,\n",
    "    xscale_log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_fig(\n",
    "    variance_result_df,\n",
    "    ESTIMATORS=basic_estimators,\n",
    "    estimators=basic_estimators_name,\n",
    "    x_scales=x_scales,\n",
    "    x_label=x_label,\n",
    "    # yscale_log=True,\n",
    "    # xscale_log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fig(\n",
    "    mse_result_df,\n",
    "    ESTIMATORS=basic_estimators,\n",
    "    estimators=basic_estimators_name,\n",
    "    x_scales=x_scales,\n",
    "    x_label=x_label,\n",
    "    # xscale_log=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "negocia_ofrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
