{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# delete later\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OFRL modules\n",
    "import ofrl\n",
    "from rtbgym import RTBEnv, CustomizedRTBEnv\n",
    "from ofrl.dataset import SyntheticDataset\n",
    "from ofrl.policy import OnlineHead, ContinuousEvalHead\n",
    "from ofrl.policy import ContinuousTruncatedGaussianHead as TruncatedGaussianHead\n",
    "from ofrl.ope.online import (\n",
    "    calc_on_policy_policy_value,\n",
    "    visualize_on_policy_policy_value,\n",
    ")\n",
    "from ofrl.utils import MinMaxScaler, MinMaxActionScaler\n",
    "\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.algos import RandomPolicy\n",
    "# from d3rlpy.preprocessing import MinMaxScaler, MinMaxActionScaler\n",
    "from ofrl.utils import MinMaxScaler, MinMaxActionScaler\n",
    "\n",
    "# import from other libraries\n",
    "import gym\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.0\n"
     ]
    }
   ],
   "source": [
    "# version\n",
    "print(ofrl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "random_state = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log directory\n",
    "from pathlib import Path\n",
    "Path(\"logs/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup, Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized environment for discrete action\n",
    "env = gym.make(\"RTBEnv-continuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for api compatibility to d3rlpy\n",
    "from ofrl.utils import OldGymAPIWrapper\n",
    "env_ = OldGymAPIWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behavior policy\n",
    "from d3rlpy.algos import SAC\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "\n",
    "# model\n",
    "sac = SAC(\n",
    "    actor_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    critic_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    action_scaler=MinMaxActionScaler(\n",
    "        minimum=env_.action_space.low,   # 0.1\n",
    "        maximum=env_.action_space.high,  # 10\n",
    "    ),\n",
    ")\n",
    "# setup replay buffer\n",
    "buffer = ReplayBuffer(\n",
    "    maxlen=10000,\n",
    "    env=env_,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-10 06:52.58 [info     ] Directory is created at d3rlpy_logs/SAC_online_20230310065258\n",
      "2023-03-10 06:52.58 [debug    ] Fitting action scaler...       action_scler=min_max\n",
      "2023-03-10 06:52.58 [debug    ] Building model...\n",
      "2023-03-10 06:52.58 [debug    ] Model has been built.\n",
      "2023-03-10 06:52.58 [info     ] Parameters are saved to d3rlpy_logs/SAC_online_20230310065258/params.json params={'action_scaler': {'type': 'min_max', 'params': {'minimum': array([0.1]), 'maximum': array([10.])}}, 'actor_encoder_factory': {'type': 'vector', 'params': {'hidden_units': [30, 30], 'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None, 'use_dense': False}}, 'actor_learning_rate': 0.0003, 'actor_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'batch_size': 256, 'critic_encoder_factory': {'type': 'vector', 'params': {'hidden_units': [30, 30], 'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None, 'use_dense': False}}, 'critic_learning_rate': 0.0003, 'critic_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'gamma': 0.99, 'generated_maxlen': 100000, 'initial_temperature': 1.0, 'n_critics': 2, 'n_frames': 1, 'n_steps': 1, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'tau': 0.005, 'temp_learning_rate': 0.0003, 'temp_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'use_gpu': None, 'algorithm': 'SAC', 'observation_shape': (7,), 'action_size': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b0c2fefc924289bc957f9bec20c5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-10 06:52.59 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_1000.pt\n",
      "2023-03-10 06:52.59 [info     ] SAC_online_20230310065258: epoch=1 step=1000 epoch=1 metrics={'time_inference': 0.0003314628601074219, 'time_environment_step': 0.0002746443748474121, 'time_step': 0.0006215281486511231, 'rollout_return': 0.0, 'evaluation': 0.0} step=1000\n",
      "2023-03-10 06:53.03 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_2000.pt\n",
      "2023-03-10 06:53.03 [info     ] SAC_online_20230310065258: epoch=2 step=2000 epoch=2 metrics={'time_inference': 0.00029970836639404296, 'time_environment_step': 0.0003407137393951416, 'time_sample_batch': 0.0001199781894683838, 'time_algorithm_update': 0.0031841297149658204, 'temp_loss': -22.49305352857709, 'temp': 1.030346809744835, 'critic_loss': 10942.534064086914, 'actor_loss': 73.95800827026368, 'time_step': 0.00397221040725708, 'rollout_return': 12.154929577464788, 'evaluation': 13.9} step=2000\n",
      "2023-03-10 06:53.06 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_3000.pt\n",
      "2023-03-10 06:53.06 [info     ] SAC_online_20230310065258: epoch=3 step=3000 epoch=3 metrics={'time_inference': 0.00029387784004211427, 'time_environment_step': 0.0003181257247924805, 'time_sample_batch': 0.00011772823333740235, 'time_algorithm_update': 0.0028138649463653564, 'temp_loss': -9.742051307678222, 'temp': 1.0948299300670623, 'critic_loss': 993.2232944335938, 'actor_loss': 45.926547889709475, 'time_step': 0.0035689537525177, 'rollout_return': 13.43661971830986, 'evaluation': 12.4} step=3000\n",
      "2023-03-10 06:53.10 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_4000.pt\n",
      "2023-03-10 06:53.10 [info     ] SAC_online_20230310065258: epoch=4 step=4000 epoch=4 metrics={'time_inference': 0.00029726171493530274, 'time_environment_step': 0.0003109269142150879, 'time_sample_batch': 0.00011525392532348633, 'time_algorithm_update': 0.002769160509109497, 'temp_loss': -9.885384543418883, 'temp': 1.240791330933571, 'critic_loss': 602.0406118774414, 'actor_loss': 40.00851431465149, 'time_step': 0.0035173392295837404, 'rollout_return': 13.0, 'evaluation': 12.0} step=4000\n",
      "2023-03-10 06:53.13 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_5000.pt\n",
      "2023-03-10 06:53.13 [info     ] SAC_online_20230310065258: epoch=5 step=5000 epoch=5 metrics={'time_inference': 0.0002973711490631104, 'time_environment_step': 0.0003129260540008545, 'time_sample_batch': 0.0001197042465209961, 'time_algorithm_update': 0.002766900062561035, 'temp_loss': -7.309468232154846, 'temp': 1.432212491750717, 'critic_loss': 446.00763012695313, 'actor_loss': 28.554597820281984, 'time_step': 0.003521531820297241, 'rollout_return': 12.80281690140845, 'evaluation': 13.6} step=5000\n",
      "2023-03-10 06:53.17 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_6000.pt\n",
      "2023-03-10 06:53.17 [info     ] SAC_online_20230310065258: epoch=6 step=6000 epoch=6 metrics={'time_inference': 0.0002886404991149902, 'time_environment_step': 0.000312298059463501, 'time_sample_batch': 0.00012303280830383302, 'time_algorithm_update': 0.0027642781734466553, 'temp_loss': -4.138130526840687, 'temp': 1.6806858825683593, 'critic_loss': 299.75438766479493, 'actor_loss': 20.501698967933656, 'time_step': 0.0035128145217895508, 'rollout_return': 13.753521126760564, 'evaluation': 15.4} step=6000\n",
      "2023-03-10 06:53.21 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_7000.pt\n",
      "2023-03-10 06:53.21 [info     ] SAC_online_20230310065258: epoch=7 step=7000 epoch=7 metrics={'time_inference': 0.0002955367565155029, 'time_environment_step': 0.00031015825271606443, 'time_sample_batch': 0.00012448573112487792, 'time_algorithm_update': 0.0027831099033355715, 'temp_loss': -3.335990376830101, 'temp': 1.899927432537079, 'critic_loss': 287.04607096862793, 'actor_loss': 17.006226643562318, 'time_step': 0.0035378692150115965, 'rollout_return': 14.570422535211268, 'evaluation': 14.2} step=7000\n",
      "2023-03-10 06:53.24 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_8000.pt\n",
      "2023-03-10 06:53.24 [info     ] SAC_online_20230310065258: epoch=8 step=8000 epoch=8 metrics={'time_inference': 0.00029731082916259764, 'time_environment_step': 0.00031176161766052247, 'time_sample_batch': 0.0001248190402984619, 'time_algorithm_update': 0.0027785592079162596, 'temp_loss': -4.49487966144085, 'temp': 2.3452356345653533, 'critic_loss': 445.56865505981443, 'actor_loss': 21.127923411369324, 'time_step': 0.0035369629859924316, 'rollout_return': 14.345070422535212, 'evaluation': 13.9} step=8000\n",
      "2023-03-10 06:53.28 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_9000.pt\n",
      "2023-03-10 06:53.28 [info     ] SAC_online_20230310065258: epoch=9 step=9000 epoch=9 metrics={'time_inference': 0.0002986171245574951, 'time_environment_step': 0.00030958032608032225, 'time_sample_batch': 0.0001301906108856201, 'time_algorithm_update': 0.0027881767749786375, 'temp_loss': -6.465956196188927, 'temp': 3.2037981967926026, 'critic_loss': 854.7328710327148, 'actor_loss': 30.363180376052856, 'time_step': 0.003551034212112427, 'rollout_return': 14.316901408450704, 'evaluation': 16.0} step=9000\n",
      "2023-03-10 06:53.31 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_10000.pt\n",
      "2023-03-10 06:53.31 [info     ] SAC_online_20230310065258: epoch=10 step=10000 epoch=10 metrics={'time_inference': 0.0002948112487792969, 'time_environment_step': 0.0003186783790588379, 'time_sample_batch': 0.00013288640975952148, 'time_algorithm_update': 0.002849411487579346, 'temp_loss': -9.696904814720154, 'temp': 4.6430880661010745, 'critic_loss': 1790.7367311401367, 'actor_loss': 45.85458785438538, 'time_step': 0.003620815277099609, 'rollout_return': 14.647887323943662, 'evaluation': 16.0} step=10000\n",
      "2023-03-10 06:53.35 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_11000.pt\n",
      "2023-03-10 06:53.35 [info     ] SAC_online_20230310065258: epoch=11 step=11000 epoch=11 metrics={'time_inference': 0.00029965686798095705, 'time_environment_step': 0.00031229448318481444, 'time_sample_batch': 0.00013162398338317872, 'time_algorithm_update': 0.002799773693084717, 'temp_loss': -16.38695771455765, 'temp': 6.927037857055664, 'critic_loss': 4048.268749633789, 'actor_loss': 73.10376891708374, 'time_step': 0.0035698392391204834, 'rollout_return': 14.401408450704226, 'evaluation': 15.5} step=11000\n",
      "2023-03-10 06:53.39 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_12000.pt\n",
      "2023-03-10 06:53.39 [info     ] SAC_online_20230310065258: epoch=12 step=12000 epoch=12 metrics={'time_inference': 0.00029651522636413573, 'time_environment_step': 0.00031137704849243164, 'time_sample_batch': 0.00013066840171813965, 'time_algorithm_update': 0.002929250717163086, 'temp_loss': -26.812985964775084, 'temp': 10.512661540985107, 'critic_loss': 8942.261765136718, 'actor_loss': 118.13223110198975, 'time_step': 0.0036949474811553955, 'rollout_return': 14.338028169014084, 'evaluation': 14.6} step=12000\n",
      "2023-03-10 06:53.43 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_13000.pt\n",
      "2023-03-10 06:53.43 [info     ] SAC_online_20230310065258: epoch=13 step=13000 epoch=13 metrics={'time_inference': 0.00033577752113342287, 'time_environment_step': 0.0004369220733642578, 'time_sample_batch': 0.00015679979324340822, 'time_algorithm_update': 0.0031930370330810545, 'temp_loss': -39.669445880889896, 'temp': 15.680523023605346, 'critic_loss': 19500.03734667969, 'actor_loss': 177.82742126464845, 'time_step': 0.004155910730361938, 'rollout_return': 14.345070422535212, 'evaluation': 14.3} step=13000\n",
      "2023-03-10 06:53.47 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_14000.pt\n",
      "2023-03-10 06:53.47 [info     ] SAC_online_20230310065258: epoch=14 step=14000 epoch=14 metrics={'time_inference': 0.0003180539608001709, 'time_environment_step': 0.00035834026336669923, 'time_sample_batch': 0.00014813995361328126, 'time_algorithm_update': 0.003049897909164429, 'temp_loss': -58.870242670059206, 'temp': 23.217665508270265, 'critic_loss': 42829.1410859375, 'actor_loss': 265.23877265930173, 'time_step': 0.003911482334136963, 'rollout_return': 14.816901408450704, 'evaluation': 13.6} step=14000\n",
      "2023-03-10 06:53.51 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_15000.pt\n",
      "2023-03-10 06:53.51 [info     ] SAC_online_20230310065258: epoch=15 step=15000 epoch=15 metrics={'time_inference': 0.0002975442409515381, 'time_environment_step': 0.0003236832618713379, 'time_sample_batch': 0.00013846492767333985, 'time_algorithm_update': 0.002867827415466309, 'temp_loss': -88.83229047775268, 'temp': 34.4441443195343, 'critic_loss': 94511.765109375, 'actor_loss': 394.87029629516604, 'time_step': 0.0036549625396728514, 'rollout_return': 15.049295774647888, 'evaluation': 16.0} step=15000\n",
      "2023-03-10 06:53.55 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_16000.pt\n",
      "2023-03-10 06:53.55 [info     ] SAC_online_20230310065258: epoch=16 step=16000 epoch=16 metrics={'time_inference': 0.00030383634567260744, 'time_environment_step': 0.00032827258110046387, 'time_sample_batch': 0.0001384146213531494, 'time_algorithm_update': 0.002916250467300415, 'temp_loss': -133.41934462738038, 'temp': 51.16326089477539, 'critic_loss': 200846.52640625, 'actor_loss': 582.5164303283691, 'time_step': 0.0037153348922729494, 'rollout_return': 14.387323943661972, 'evaluation': 13.7} step=16000\n",
      "2023-03-10 06:53.58 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_17000.pt\n",
      "2023-03-10 06:53.58 [info     ] SAC_online_20230310065258: epoch=17 step=17000 epoch=17 metrics={'time_inference': 0.00029139375686645507, 'time_environment_step': 0.0003046929836273193, 'time_sample_batch': 0.00012910890579223634, 'time_algorithm_update': 0.0027672502994537354, 'temp_loss': -204.17595680999756, 'temp': 76.21806909942627, 'critic_loss': 463342.570859375, 'actor_loss': 892.5247603759766, 'time_step': 0.003518632173538208, 'rollout_return': 15.126760563380282, 'evaluation': 14.0} step=17000\n",
      "2023-03-10 06:54.02 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_18000.pt\n",
      "2023-03-10 06:54.02 [info     ] SAC_online_20230310065258: epoch=18 step=18000 epoch=18 metrics={'time_inference': 0.0003186302185058594, 'time_environment_step': 0.00034470009803771974, 'time_sample_batch': 0.00014322137832641603, 'time_algorithm_update': 0.00301090407371521, 'temp_loss': -295.4838069076538, 'temp': 112.67058390808106, 'critic_loss': 981887.1763125, 'actor_loss': 1305.7388844604493, 'time_step': 0.003847530126571655, 'rollout_return': 14.866197183098592, 'evaluation': 15.3} step=18000\n",
      "2023-03-10 06:54.06 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_19000.pt\n",
      "2023-03-10 06:54.06 [info     ] SAC_online_20230310065258: epoch=19 step=19000 epoch=19 metrics={'time_inference': 0.0003040883541107178, 'time_environment_step': 0.0003408303260803223, 'time_sample_batch': 0.00014203405380249024, 'time_algorithm_update': 0.002938584089279175, 'temp_loss': -430.4068143310547, 'temp': 166.46172315979004, 'critic_loss': 2133487.5599375, 'actor_loss': 1917.6420993652343, 'time_step': 0.0037548701763153074, 'rollout_return': 14.330985915492958, 'evaluation': 17.3} step=19000\n",
      "2023-03-10 06:54.10 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_20000.pt\n",
      "2023-03-10 06:54.10 [info     ] SAC_online_20230310065258: epoch=20 step=20000 epoch=20 metrics={'time_inference': 0.00029272890090942385, 'time_environment_step': 0.00030980753898620603, 'time_sample_batch': 0.00013012003898620606, 'time_algorithm_update': 0.00277363920211792, 'temp_loss': -634.8739742431641, 'temp': 246.10330563354492, 'critic_loss': 4660648.98275, 'actor_loss': 2837.337378417969, 'time_step': 0.0035328366756439207, 'rollout_return': 14.535211267605634, 'evaluation': 16.9} step=20000\n",
      "2023-03-10 06:54.13 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_21000.pt\n",
      "2023-03-10 06:54.13 [info     ] SAC_online_20230310065258: epoch=21 step=21000 epoch=21 metrics={'time_inference': 0.00029085516929626464, 'time_environment_step': 0.0003037843704223633, 'time_sample_batch': 0.00012871599197387696, 'time_algorithm_update': 0.0027790753841400147, 'temp_loss': -925.3301770019531, 'temp': 363.1417247924805, 'critic_loss': 10161071.1875, 'actor_loss': 4167.644470703125, 'time_step': 0.0035292885303497314, 'rollout_return': 14.908450704225352, 'evaluation': 12.6} step=21000\n",
      "2023-03-10 06:54.17 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_22000.pt\n",
      "2023-03-10 06:54.17 [info     ] SAC_online_20230310065258: epoch=22 step=22000 epoch=22 metrics={'time_inference': 0.000289010763168335, 'time_environment_step': 0.0003018069267272949, 'time_sample_batch': 0.00012787365913391113, 'time_algorithm_update': 0.0027593796253204345, 'temp_loss': -1394.7882617797852, 'temp': 538.4804001159667, 'critic_loss': 22564996.141, 'actor_loss': 6187.519628417968, 'time_step': 0.003504573345184326, 'rollout_return': 14.964788732394366, 'evaluation': 15.0} step=22000\n",
      "2023-03-10 06:54.20 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_23000.pt\n",
      "2023-03-10 06:54.20 [info     ] SAC_online_20230310065258: epoch=23 step=23000 epoch=23 metrics={'time_inference': 0.0002875363826751709, 'time_environment_step': 0.00030308151245117185, 'time_sample_batch': 0.00013225317001342774, 'time_algorithm_update': 0.0027584540843963623, 'temp_loss': -2081.162901123047, 'temp': 798.0844482421875, 'critic_loss': 49186763.918, 'actor_loss': 9143.532176757812, 'time_step': 0.003507431983947754, 'rollout_return': 14.19718309859155, 'evaluation': 15.3} step=23000\n",
      "2023-03-10 06:54.24 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_24000.pt\n",
      "2023-03-10 06:54.24 [info     ] SAC_online_20230310065258: epoch=24 step=24000 epoch=24 metrics={'time_inference': 0.000288820743560791, 'time_environment_step': 0.0003059995174407959, 'time_sample_batch': 0.00013027501106262207, 'time_algorithm_update': 0.0027869288921356203, 'temp_loss': -2988.9806732177735, 'temp': 1179.462026184082, 'critic_loss': 105318118.452, 'actor_loss': 13376.969969726562, 'time_step': 0.00353847074508667, 'rollout_return': 15.274647887323944, 'evaluation': 14.9} step=24000\n",
      "2023-03-10 06:54.27 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_25000.pt\n",
      "2023-03-10 06:54.27 [info     ] SAC_online_20230310065258: epoch=25 step=25000 epoch=25 metrics={'time_inference': 0.00028829431533813476, 'time_environment_step': 0.00030361032485961913, 'time_sample_batch': 0.00012772464752197266, 'time_algorithm_update': 0.002757117509841919, 'temp_loss': -4444.158658203125, 'temp': 1741.8351716308593, 'critic_loss': 230538835.688, 'actor_loss': 19746.896267578126, 'time_step': 0.0035028293132781984, 'rollout_return': 14.669014084507042, 'evaluation': 14.3} step=25000\n",
      "2023-03-10 06:54.31 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_26000.pt\n",
      "2023-03-10 06:54.31 [info     ] SAC_online_20230310065258: epoch=26 step=26000 epoch=26 metrics={'time_inference': 0.00029236435890197756, 'time_environment_step': 0.00030743026733398435, 'time_sample_batch': 0.00012909626960754396, 'time_algorithm_update': 0.0027649216651916505, 'temp_loss': -6584.816345703125, 'temp': 2577.67274609375, 'critic_loss': 505682442.432, 'actor_loss': 29187.6671875, 'time_step': 0.003520280122756958, 'rollout_return': 14.774647887323944, 'evaluation': 13.8} step=26000\n",
      "2023-03-10 06:54.35 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_27000.pt\n",
      "2023-03-10 06:54.35 [info     ] SAC_online_20230310065258: epoch=27 step=27000 epoch=27 metrics={'time_inference': 0.0002930891513824463, 'time_environment_step': 0.0003111758232116699, 'time_sample_batch': 0.00013142037391662597, 'time_algorithm_update': 0.0027903931140899657, 'temp_loss': -9484.486408935547, 'temp': 3805.8643076171875, 'critic_loss': 1092693034.272, 'actor_loss': 42536.75041210937, 'time_step': 0.0035530362129211424, 'rollout_return': 14.73943661971831, 'evaluation': 13.9} step=27000\n",
      "2023-03-10 06:54.38 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_28000.pt\n",
      "2023-03-10 06:54.38 [info     ] SAC_online_20230310065258: epoch=28 step=28000 epoch=28 metrics={'time_inference': 0.0003076682090759277, 'time_environment_step': 0.00034070730209350585, 'time_sample_batch': 0.0001423966884613037, 'time_algorithm_update': 0.0029531290531158447, 'temp_loss': -14026.9383671875, 'temp': 5625.326420410156, 'critic_loss': 2429448812.288, 'actor_loss': 63393.784015625, 'time_step': 0.003773743629455566, 'rollout_return': 14.830985915492958, 'evaluation': 14.9} step=28000\n",
      "2023-03-10 06:54.42 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_29000.pt\n",
      "2023-03-10 06:54.42 [info     ] SAC_online_20230310065258: epoch=29 step=29000 epoch=29 metrics={'time_inference': 0.00030590248107910156, 'time_environment_step': 0.0003289375305175781, 'time_sample_batch': 0.00013821339607238768, 'time_algorithm_update': 0.0028869943618774416, 'temp_loss': -20976.862704101564, 'temp': 8329.220178710937, 'critic_loss': 5368710087.168, 'actor_loss': 93688.57345703125, 'time_step': 0.0036892848014831543, 'rollout_return': 14.507042253521126, 'evaluation': 14.4} step=29000\n",
      "2023-03-10 06:54.46 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_30000.pt\n",
      "2023-03-10 06:54.46 [info     ] SAC_online_20230310065258: epoch=30 step=30000 epoch=30 metrics={'time_inference': 0.0003077173233032227, 'time_environment_step': 0.0003376924991607666, 'time_sample_batch': 0.00014386343955993653, 'time_algorithm_update': 0.0029344279766082763, 'temp_loss': -31047.37746582031, 'temp': 12341.63592578125, 'critic_loss': 11763466622.976, 'actor_loss': 138745.973828125, 'time_step': 0.0037531957626342773, 'rollout_return': 14.394366197183098, 'evaluation': 15.6} step=30000\n",
      "2023-03-10 06:54.50 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_31000.pt\n",
      "2023-03-10 06:54.50 [info     ] SAC_online_20230310065258: epoch=31 step=31000 epoch=31 metrics={'time_inference': 0.00030261874198913576, 'time_environment_step': 0.0003312931060791016, 'time_sample_batch': 0.0001396653652191162, 'time_algorithm_update': 0.002897857904434204, 'temp_loss': -46133.52043847656, 'temp': 18281.153008789064, 'critic_loss': 25510369342.464, 'actor_loss': 204372.9509375, 'time_step': 0.0037007393836975097, 'rollout_return': 14.507042253521126, 'evaluation': 16.3} step=31000\n",
      "2023-03-10 06:54.54 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_32000.pt\n",
      "2023-03-10 06:54.54 [info     ] SAC_online_20230310065258: epoch=32 step=32000 epoch=32 metrics={'time_inference': 0.0003102884292602539, 'time_environment_step': 0.0003471317291259766, 'time_sample_batch': 0.0001469573974609375, 'time_algorithm_update': 0.002970798492431641, 'temp_loss': -68127.31468359375, 'temp': 27053.59565234375, 'critic_loss': 55729307746.304, 'actor_loss': 301395.88340625, 'time_step': 0.0038056087493896482, 'rollout_return': 14.485915492957746, 'evaluation': 14.2} step=32000\n",
      "2023-03-10 06:54.57 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_33000.pt\n",
      "2023-03-10 06:54.57 [info     ] SAC_online_20230310065258: epoch=33 step=33000 epoch=33 metrics={'time_inference': 0.0002984795570373535, 'time_environment_step': 0.0003265066146850586, 'time_sample_batch': 0.00013486862182617186, 'time_algorithm_update': 0.002855757236480713, 'temp_loss': -101092.11438671875, 'temp': 40097.0529921875, 'critic_loss': 123370274000.896, 'actor_loss': 445914.72515625, 'time_step': 0.0036439418792724608, 'rollout_return': 14.788732394366198, 'evaluation': 14.4} step=33000\n",
      "2023-03-10 06:55.01 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_34000.pt\n",
      "2023-03-10 06:55.01 [info     ] SAC_online_20230310065258: epoch=34 step=34000 epoch=34 metrics={'time_inference': 0.00029268646240234375, 'time_environment_step': 0.0003090987205505371, 'time_sample_batch': 0.00013170218467712402, 'time_algorithm_update': 0.0027815940380096437, 'temp_loss': -150602.57725, 'temp': 59496.3225625, 'critic_loss': 271985220534.272, 'actor_loss': 665993.8996875, 'time_step': 0.0035417966842651366, 'rollout_return': 14.52112676056338, 'evaluation': 13.8} step=34000\n",
      "2023-03-10 06:55.05 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_35000.pt\n",
      "2023-03-10 06:55.05 [info     ] SAC_online_20230310065258: epoch=35 step=35000 epoch=35 metrics={'time_inference': 0.00030774855613708495, 'time_environment_step': 0.00033319854736328126, 'time_sample_batch': 0.00013780903816223145, 'time_algorithm_update': 0.0029093129634857177, 'temp_loss': -224440.708234375, 'temp': 88070.310125, 'critic_loss': 599092667629.568, 'actor_loss': 981941.049375, 'time_step': 0.0037181882858276366, 'rollout_return': 14.69718309859155, 'evaluation': 14.7} step=35000\n",
      "2023-03-10 06:55.08 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_36000.pt\n",
      "2023-03-10 06:55.08 [info     ] SAC_online_20230310065258: epoch=36 step=36000 epoch=36 metrics={'time_inference': 0.0002969906330108643, 'time_environment_step': 0.0003101832866668701, 'time_sample_batch': 0.0001309361457824707, 'time_algorithm_update': 0.002813173770904541, 'temp_loss': -331270.682953125, 'temp': 130633.885171875, 'critic_loss': 1329266102435.84, 'actor_loss': 1454659.9225, 'time_step': 0.0035781431198120116, 'rollout_return': 15.112676056338028, 'evaluation': 15.3} step=36000\n",
      "2023-03-10 06:55.12 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_37000.pt\n",
      "2023-03-10 06:55.12 [info     ] SAC_online_20230310065258: epoch=37 step=37000 epoch=37 metrics={'time_inference': 0.0002968072891235352, 'time_environment_step': 0.0003069536685943604, 'time_sample_batch': 0.00013001394271850586, 'time_algorithm_update': 0.0027739057540893553, 'temp_loss': -492661.76028125, 'temp': 193328.398875, 'critic_loss': 2882534965510.144, 'actor_loss': 2136062.58725, 'time_step': 0.0035343661308288575, 'rollout_return': 14.76056338028169, 'evaluation': 16.6} step=37000\n",
      "2023-03-10 06:55.16 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_38000.pt\n",
      "2023-03-10 06:55.16 [info     ] SAC_online_20230310065258: epoch=38 step=38000 epoch=38 metrics={'time_inference': 0.00029114603996276855, 'time_environment_step': 0.00030516862869262696, 'time_sample_batch': 0.00012953448295593262, 'time_algorithm_update': 0.00275494122505188, 'temp_loss': -711036.2854375, 'temp': 286281.550734375, 'critic_loss': 6313463641276.416, 'actor_loss': 3125187.9985, 'time_step': 0.0035070884227752686, 'rollout_return': 15.126760563380282, 'evaluation': 13.2} step=38000\n",
      "2023-03-10 06:55.19 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_39000.pt\n",
      "2023-03-10 06:55.19 [info     ] SAC_online_20230310065258: epoch=39 step=39000 epoch=39 metrics={'time_inference': 0.0002928164005279541, 'time_environment_step': 0.00030449461936950684, 'time_sample_batch': 0.00012857580184936522, 'time_algorithm_update': 0.0027544353008270264, 'temp_loss': -1042509.52471875, 'temp': 421801.509125, 'critic_loss': 13805162257186.816, 'actor_loss': 4572980.64925, 'time_step': 0.0035066680908203123, 'rollout_return': 14.788732394366198, 'evaluation': 13.0} step=39000\n",
      "2023-03-10 06:55.23 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_40000.pt\n",
      "2023-03-10 06:55.23 [info     ] SAC_online_20230310065258: epoch=40 step=40000 epoch=40 metrics={'time_inference': 0.00028902339935302733, 'time_environment_step': 0.0003042459487915039, 'time_sample_batch': 0.00012793707847595214, 'time_algorithm_update': 0.0027617061138153075, 'temp_loss': -1576015.13534375, 'temp': 625765.3816875, 'critic_loss': 30004577428307.97, 'actor_loss': 6675935.42, 'time_step': 0.0035094540119171144, 'rollout_return': 15.147887323943662, 'evaluation': 13.3} step=40000\n",
      "2023-03-10 06:55.26 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_41000.pt\n",
      "2023-03-10 06:55.26 [info     ] SAC_online_20230310065258: epoch=41 step=41000 epoch=41 metrics={'time_inference': 0.0002890074253082275, 'time_environment_step': 0.00031061482429504397, 'time_sample_batch': 0.00012934470176696777, 'time_algorithm_update': 0.002780974626541138, 'temp_loss': -2315136.4104375, 'temp': 927546.2639375, 'critic_loss': 65505433313345.54, 'actor_loss': 9753097.8285, 'time_step': 0.0035363993644714356, 'rollout_return': 14.908450704225352, 'evaluation': 13.9} step=41000\n",
      "2023-03-10 06:55.30 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_42000.pt\n",
      "2023-03-10 06:55.30 [info     ] SAC_online_20230310065258: epoch=42 step=42000 epoch=42 metrics={'time_inference': 0.00029204487800598143, 'time_environment_step': 0.00030396246910095216, 'time_sample_batch': 0.00012749552726745605, 'time_algorithm_update': 0.0027398526668548583, 'temp_loss': -3380691.3, 'temp': 1371013.46875, 'critic_loss': 146750181663047.7, 'actor_loss': 14090606.025, 'time_step': 0.003489680290222168, 'rollout_return': 15.056338028169014, 'evaluation': 13.9} step=42000\n",
      "2023-03-10 06:55.34 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_43000.pt\n",
      "2023-03-10 06:55.34 [info     ] SAC_online_20230310065258: epoch=43 step=43000 epoch=43 metrics={'time_inference': 0.0002927863597869873, 'time_environment_step': 0.00030408978462219237, 'time_sample_batch': 0.00012855768203735352, 'time_algorithm_update': 0.002753014802932739, 'temp_loss': -5010426.892875, 'temp': 2026015.742375, 'critic_loss': 328321712204546.06, 'actor_loss': 20292183.583, 'time_step': 0.00350508189201355, 'rollout_return': 14.901408450704226, 'evaluation': 14.7} step=43000\n",
      "2023-03-10 06:55.37 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_44000.pt\n",
      "2023-03-10 06:55.37 [info     ] SAC_online_20230310065258: epoch=44 step=44000 epoch=44 metrics={'time_inference': 0.0002918670177459717, 'time_environment_step': 0.0003035469055175781, 'time_sample_batch': 0.00012734198570251465, 'time_algorithm_update': 0.0027590932846069337, 'temp_loss': -7372652.45675, 'temp': 2999040.87125, 'critic_loss': 741878026597302.2, 'actor_loss': 28969960.768, 'time_step': 0.0035079710483551024, 'rollout_return': 14.78169014084507, 'evaluation': 15.7} step=44000\n",
      "2023-03-10 06:55.41 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_45000.pt\n",
      "2023-03-10 06:55.41 [info     ] SAC_online_20230310065258: epoch=45 step=45000 epoch=45 metrics={'time_inference': 0.0002903027534484863, 'time_environment_step': 0.0003114347457885742, 'time_sample_batch': 0.0001311042308807373, 'time_algorithm_update': 0.0027779397964477538, 'temp_loss': -11094720.344, 'temp': 4452450.16025, 'critic_loss': 1687941163564138.5, 'actor_loss': 41113212.304, 'time_step': 0.003539701700210571, 'rollout_return': 14.204225352112676, 'evaluation': 16.2} step=45000\n",
      "2023-03-10 06:55.45 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_46000.pt\n",
      "2023-03-10 06:55.45 [info     ] SAC_online_20230310065258: epoch=46 step=46000 epoch=46 metrics={'time_inference': 0.00030810904502868655, 'time_environment_step': 0.0003465063571929932, 'time_sample_batch': 0.0001426706314086914, 'time_algorithm_update': 0.0029826171398162843, 'temp_loss': -16141053.074, 'temp': 6586223.9965, 'critic_loss': 3882808532256424.0, 'actor_loss': 57757508.672, 'time_step': 0.0038095824718475343, 'rollout_return': 14.894366197183098, 'evaluation': 14.8} step=46000\n",
      "2023-03-10 06:55.48 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_47000.pt\n",
      "2023-03-10 06:55.48 [info     ] SAC_online_20230310065258: epoch=47 step=47000 epoch=47 metrics={'time_inference': 0.0002946038246154785, 'time_environment_step': 0.0003157789707183838, 'time_sample_batch': 0.00013515329360961915, 'time_algorithm_update': 0.002812058448791504, 'temp_loss': -23622999.6, 'temp': 9726840.247, 'critic_loss': 8898899464207466.0, 'actor_loss': 79464316.184, 'time_step': 0.0035850176811218263, 'rollout_return': 15.19718309859155, 'evaluation': 15.1} step=47000\n",
      "2023-03-10 06:55.52 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_48000.pt\n",
      "2023-03-10 06:55.52 [info     ] SAC_online_20230310065258: epoch=48 step=48000 epoch=48 metrics={'time_inference': 0.00030029630661010745, 'time_environment_step': 0.00033063435554504393, 'time_sample_batch': 0.00013538360595703126, 'time_algorithm_update': 0.002866470813751221, 'temp_loss': -35177342.563, 'temp': 14413609.224, 'critic_loss': 2.028315545856849e+16, 'actor_loss': 109889611.456, 'time_step': 0.0036612670421600343, 'rollout_return': 14.69718309859155, 'evaluation': 14.6} step=48000\n",
      "2023-03-10 06:55.56 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_49000.pt\n",
      "2023-03-10 06:55.56 [info     ] SAC_online_20230310065258: epoch=49 step=49000 epoch=49 metrics={'time_inference': 0.00029858899116516114, 'time_environment_step': 0.0003298342227935791, 'time_sample_batch': 0.0001388235092163086, 'time_algorithm_update': 0.0028586890697479246, 'temp_loss': -51651390.344, 'temp': 21328233.082, 'critic_loss': 4.677752307140946e+16, 'actor_loss': 151991594.744, 'time_step': 0.003654571533203125, 'rollout_return': 14.80281690140845, 'evaluation': 14.4} step=49000\n",
      "2023-03-10 06:55.59 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_50000.pt\n",
      "2023-03-10 06:55.59 [info     ] SAC_online_20230310065258: epoch=50 step=50000 epoch=50 metrics={'time_inference': 0.00029563212394714356, 'time_environment_step': 0.00032322239875793457, 'time_sample_batch': 0.000137404203414917, 'time_algorithm_update': 0.002821437120437622, 'temp_loss': 9175685.709, 'temp': 26987602.784, 'critic_loss': 2.6717885755666988e+16, 'actor_loss': 104719793.52, 'time_step': 0.0036051619052886964, 'rollout_return': 15.204225352112676, 'evaluation': 14.4} step=50000\n",
      "2023-03-10 06:56.03 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_51000.pt\n",
      "2023-03-10 06:56.03 [info     ] SAC_online_20230310065258: epoch=51 step=51000 epoch=51 metrics={'time_inference': 0.0003063540458679199, 'time_environment_step': 0.0003087794780731201, 'time_sample_batch': 0.00013084864616394042, 'time_algorithm_update': 0.0027541937828063964, 'temp_loss': 35916025.816, 'temp': 21714404.038, 'critic_loss': 2745479030110159.0, 'actor_loss': 48427065.906, 'time_step': 0.00352660870552063, 'rollout_return': 14.23943661971831, 'evaluation': 13.7} step=51000\n",
      "2023-03-10 06:56.07 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_52000.pt\n",
      "2023-03-10 06:56.07 [info     ] SAC_online_20230310065258: epoch=52 step=52000 epoch=52 metrics={'time_inference': 0.0002894859313964844, 'time_environment_step': 0.0003059558868408203, 'time_sample_batch': 0.00012864017486572267, 'time_algorithm_update': 0.0027525987625122072, 'temp_loss': 28382774.624, 'temp': 17135306.812, 'critic_loss': 2796242982137757.5, 'actor_loss': 30347792.422, 'time_step': 0.0035029189586639406, 'rollout_return': 14.225352112676056, 'evaluation': 13.5} step=52000\n",
      "2023-03-10 06:56.10 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_53000.pt\n",
      "2023-03-10 06:56.10 [info     ] SAC_online_20230310065258: epoch=53 step=53000 epoch=53 metrics={'time_inference': 0.00029360055923461915, 'time_environment_step': 0.00030591297149658205, 'time_sample_batch': 0.000128948450088501, 'time_algorithm_update': 0.00276242995262146, 'temp_loss': 22581578.176, 'temp': 13621731.098, 'critic_loss': 2658942573170655.0, 'actor_loss': 18392154.821, 'time_step': 0.0035170588493347167, 'rollout_return': 13.908450704225352, 'evaluation': 12.9} step=53000\n",
      "2023-03-10 06:56.14 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_54000.pt\n",
      "2023-03-10 06:56.14 [info     ] SAC_online_20230310065258: epoch=54 step=54000 epoch=54 metrics={'time_inference': 0.00028411555290222166, 'time_environment_step': 0.0003061578273773193, 'time_sample_batch': 0.00012905573844909667, 'time_algorithm_update': 0.0028568413257598875, 'temp_loss': 18042605.784, 'temp': 10859640.749, 'critic_loss': 2032765687180558.2, 'actor_loss': 10392323.764, 'time_step': 0.003602728605270386, 'rollout_return': 14.767605633802816, 'evaluation': 14.4} step=54000\n",
      "2023-03-10 06:56.17 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_55000.pt\n",
      "2023-03-10 06:56.17 [info     ] SAC_online_20230310065258: epoch=55 step=55000 epoch=55 metrics={'time_inference': 0.0002929036617279053, 'time_environment_step': 0.0003170278072357178, 'time_sample_batch': 0.00013329672813415527, 'time_algorithm_update': 0.0028123621940612793, 'temp_loss': 14391145.76, 'temp': 8671889.667, 'critic_loss': 1302038742457909.2, 'actor_loss': 4963924.07290625, 'time_step': 0.003582873821258545, 'rollout_return': 14.767605633802816, 'evaluation': 14.3} step=55000\n",
      "2023-03-10 06:56.21 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_56000.pt\n",
      "2023-03-10 06:56.21 [info     ] SAC_online_20230310065258: epoch=56 step=56000 epoch=56 metrics={'time_inference': 0.0002992992401123047, 'time_environment_step': 0.0003268554210662842, 'time_sample_batch': 0.00014028692245483397, 'time_algorithm_update': 0.0028748319149017334, 'temp_loss': 11538031.533, 'temp': 6927682.1325, 'critic_loss': 740376518043107.4, 'actor_loss': 477970.5435039062, 'time_step': 0.0036702828407287596, 'rollout_return': 14.56338028169014, 'evaluation': 14.6} step=56000\n",
      "2023-03-10 06:56.25 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_57000.pt\n",
      "2023-03-10 06:56.25 [info     ] SAC_online_20230310065258: epoch=57 step=57000 epoch=57 metrics={'time_inference': 0.00030102992057800295, 'time_environment_step': 0.0003257763385772705, 'time_sample_batch': 0.00013599705696105957, 'time_algorithm_update': 0.0028397676944732666, 'temp_loss': 9199570.472, 'temp': 5538099.386, 'critic_loss': 389600791389798.4, 'actor_loss': -2798352.253828125, 'time_step': 0.003631197452545166, 'rollout_return': 14.788732394366198, 'evaluation': 15.2} step=57000\n",
      "2023-03-10 06:56.28 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_58000.pt\n",
      "2023-03-10 06:56.28 [info     ] SAC_online_20230310065258: epoch=58 step=58000 epoch=58 metrics={'time_inference': 0.0002878825664520264, 'time_environment_step': 0.00030609416961669924, 'time_sample_batch': 0.00012768149375915527, 'time_algorithm_update': 0.002734165668487549, 'temp_loss': 7372965.3715, 'temp': 4427542.91925, 'critic_loss': 194876846456700.94, 'actor_loss': -5023125.2435, 'time_step': 0.003482053995132446, 'rollout_return': 14.78169014084507, 'evaluation': 14.5} step=58000\n",
      "2023-03-10 06:56.32 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_59000.pt\n",
      "2023-03-10 06:56.32 [info     ] SAC_online_20230310065258: epoch=59 step=59000 epoch=59 metrics={'time_inference': 0.00028976988792419435, 'time_environment_step': 0.0003034656047821045, 'time_sample_batch': 0.00012976884841918944, 'time_algorithm_update': 0.002745455503463745, 'temp_loss': 5891590.471, 'temp': 3539732.115, 'critic_loss': 88496664247009.28, 'actor_loss': -5524745.788, 'time_step': 0.0034948348999023437, 'rollout_return': 14.52112676056338, 'evaluation': 15.5} step=59000\n",
      "2023-03-10 06:56.36 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_60000.pt\n",
      "2023-03-10 06:56.36 [info     ] SAC_online_20230310065258: epoch=60 step=60000 epoch=60 metrics={'time_inference': 0.000293337345123291, 'time_environment_step': 0.00030597805976867676, 'time_sample_batch': 0.00012977123260498046, 'time_algorithm_update': 0.002759899377822876, 'temp_loss': 4710666.372, 'temp': 2830631.811, 'critic_loss': 36289609027551.234, 'actor_loss': -4698807.26425, 'time_step': 0.0035152959823608397, 'rollout_return': 14.267605633802816, 'evaluation': 13.7} step=60000\n",
      "2023-03-10 06:56.39 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_61000.pt\n",
      "2023-03-10 06:56.39 [info     ] SAC_online_20230310065258: epoch=61 step=61000 epoch=61 metrics={'time_inference': 0.0003107931613922119, 'time_environment_step': 0.0003401658535003662, 'time_sample_batch': 0.00014051127433776855, 'time_algorithm_update': 0.0029127349853515627, 'temp_loss': 3771689.894, 'temp': 2263207.993, 'critic_loss': 15537565063970.816, 'actor_loss': -3732859.04675, 'time_step': 0.003734286069869995, 'rollout_return': 14.366197183098592, 'evaluation': 14.3} step=61000\n",
      "2023-03-10 06:56.43 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_62000.pt\n",
      "2023-03-10 06:56.43 [info     ] SAC_online_20230310065258: epoch=62 step=62000 epoch=62 metrics={'time_inference': 0.00031363463401794435, 'time_environment_step': 0.0003452117443084717, 'time_sample_batch': 0.00015092921257019043, 'time_algorithm_update': 0.0029667210578918457, 'temp_loss': 3015952.323, 'temp': 1809459.336375, 'critic_loss': 7205805750747.136, 'actor_loss': -2947485.21625, 'time_step': 0.003808185577392578, 'rollout_return': 14.112676056338028, 'evaluation': 13.2} step=62000\n",
      "2023-03-10 06:56.47 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_63000.pt\n",
      "2023-03-10 06:56.47 [info     ] SAC_online_20230310065258: epoch=63 step=63000 epoch=63 metrics={'time_inference': 0.00032532191276550294, 'time_environment_step': 0.00038768625259399416, 'time_sample_batch': 0.000160170316696167, 'time_algorithm_update': 0.0032121741771697996, 'temp_loss': 2413164.14925, 'temp': 1446656.7705, 'critic_loss': 2595387434729.472, 'actor_loss': -2282207.582, 'time_step': 0.004120254755020141, 'rollout_return': 14.295774647887324, 'evaluation': 14.2} step=63000\n",
      "2023-03-10 06:56.51 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_64000.pt\n",
      "2023-03-10 06:56.51 [info     ] SAC_online_20230310065258: epoch=64 step=64000 epoch=64 metrics={'time_inference': 0.0002980356216430664, 'time_environment_step': 0.00031285500526428224, 'time_sample_batch': 0.00013330411911010743, 'time_algorithm_update': 0.002789733648300171, 'temp_loss': 1929069.742125, 'temp': 1156842.7681875, 'critic_loss': 940533154873.344, 'actor_loss': -1748966.71475, 'time_step': 0.0035618371963500976, 'rollout_return': 14.47887323943662, 'evaluation': 14.6} step=64000\n",
      "2023-03-10 06:56.55 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_65000.pt\n",
      "2023-03-10 06:56.55 [info     ] SAC_online_20230310065258: epoch=65 step=65000 epoch=65 metrics={'time_inference': 0.00031079649925231935, 'time_environment_step': 0.0003356690406799316, 'time_sample_batch': 0.00014322066307067871, 'time_algorithm_update': 0.0029118316173553467, 'temp_loss': 1539887.295875, 'temp': 925093.5389375, 'critic_loss': 449293170655.232, 'actor_loss': -1389703.11825, 'time_step': 0.003730468034744263, 'rollout_return': 14.352112676056338, 'evaluation': 15.0} step=65000\n",
      "2023-03-10 06:56.59 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_66000.pt\n",
      "2023-03-10 06:56.59 [info     ] SAC_online_20230310065258: epoch=66 step=66000 epoch=66 metrics={'time_inference': 0.0002982745170593262, 'time_environment_step': 0.0003199646472930908, 'time_sample_batch': 0.00013536429405212403, 'time_algorithm_update': 0.002825265169143677, 'temp_loss': 1234726.518125, 'temp': 739863.0154375, 'critic_loss': 282935794876.416, 'actor_loss': -1124646.797125, 'time_step': 0.003606522560119629, 'rollout_return': 14.535211267605634, 'evaluation': 14.0} step=66000\n",
      "2023-03-10 06:57.02 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_67000.pt\n",
      "2023-03-10 06:57.02 [info     ] SAC_online_20230310065258: epoch=67 step=67000 epoch=67 metrics={'time_inference': 0.0002971956729888916, 'time_environment_step': 0.0003205287456512451, 'time_sample_batch': 0.0001355767250061035, 'time_algorithm_update': 0.0028744657039642334, 'temp_loss': 984951.526875, 'temp': 591566.2605625, 'critic_loss': 196666691764.224, 'actor_loss': -905202.5334375, 'time_step': 0.0036558613777160644, 'rollout_return': 14.78169014084507, 'evaluation': 14.2} step=67000\n",
      "2023-03-10 06:57.07 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_68000.pt\n",
      "2023-03-10 06:57.07 [info     ] SAC_online_20230310065258: epoch=68 step=68000 epoch=68 metrics={'time_inference': 0.0003475260734558106, 'time_environment_step': 0.0003778085708618164, 'time_sample_batch': 0.0001601133346557617, 'time_algorithm_update': 0.0032530920505523682, 'temp_loss': 789103.469625, 'temp': 473097.9626875, 'critic_loss': 127360702021.632, 'actor_loss': -742635.8794375, 'time_step': 0.00417614197731018, 'rollout_return': 13.669014084507042, 'evaluation': 13.8} step=68000\n",
      "2023-03-10 06:57.10 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_69000.pt\n",
      "2023-03-10 06:57.10 [info     ] SAC_online_20230310065258: epoch=69 step=69000 epoch=69 metrics={'time_inference': 0.0003057286739349365, 'time_environment_step': 0.0003254830837249756, 'time_sample_batch': 0.0001354234218597412, 'time_algorithm_update': 0.0028836543560028078, 'temp_loss': 630397.379125, 'temp': 378256.9665625, 'critic_loss': 78479433408.512, 'actor_loss': -612335.623375, 'time_step': 0.0036783907413482668, 'rollout_return': 14.612676056338028, 'evaluation': 14.9} step=69000\n",
      "2023-03-10 06:57.15 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_70000.pt\n",
      "2023-03-10 06:57.15 [info     ] SAC_online_20230310065258: epoch=70 step=70000 epoch=70 metrics={'time_inference': 0.00034170842170715333, 'time_environment_step': 0.0003925981521606445, 'time_sample_batch': 0.00016616368293762208, 'time_algorithm_update': 0.0032614526748657225, 'temp_loss': 504115.6651875, 'temp': 302515.10559375, 'critic_loss': 46482527870.976, 'actor_loss': -504550.6968125, 'time_step': 0.004198939085006714, 'rollout_return': 14.67605633802817, 'evaluation': 16.4} step=70000\n",
      "2023-03-10 06:57.19 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_71000.pt\n",
      "2023-03-10 06:57.19 [info     ] SAC_online_20230310065258: epoch=71 step=71000 epoch=71 metrics={'time_inference': 0.00031378602981567384, 'time_environment_step': 0.0003603212833404541, 'time_sample_batch': 0.00014844083786010742, 'time_algorithm_update': 0.003013553857803345, 'temp_loss': 403433.96590625, 'temp': 241896.347453125, 'critic_loss': 25625398736.896, 'actor_loss': -424009.64753125, 'time_step': 0.003867818832397461, 'rollout_return': 14.880281690140846, 'evaluation': 14.5} step=71000\n",
      "2023-03-10 06:57.22 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_72000.pt\n",
      "2023-03-10 06:57.22 [info     ] SAC_online_20230310065258: epoch=72 step=72000 epoch=72 metrics={'time_inference': 0.0002931983470916748, 'time_environment_step': 0.0003164215087890625, 'time_sample_batch': 0.00013428831100463867, 'time_algorithm_update': 0.002815293788909912, 'temp_loss': 322400.9938125, 'temp': 193432.981640625, 'critic_loss': 15026710712.32, 'actor_loss': -352003.66984375, 'time_step': 0.003586563587188721, 'rollout_return': 14.549295774647888, 'evaluation': 14.7} step=72000\n",
      "2023-03-10 06:57.26 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_73000.pt\n",
      "2023-03-10 06:57.26 [info     ] SAC_online_20230310065258: epoch=73 step=73000 epoch=73 metrics={'time_inference': 0.0003044369220733643, 'time_environment_step': 0.00033081769943237304, 'time_sample_batch': 0.00014045286178588867, 'time_algorithm_update': 0.002895427703857422, 'temp_loss': 257556.881234375, 'temp': 154712.83809375, 'critic_loss': 9698168224.768, 'actor_loss': -284983.248296875, 'time_step': 0.0037003726959228514, 'rollout_return': 14.535211267605634, 'evaluation': 14.4} step=73000\n",
      "2023-03-10 06:57.30 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_74000.pt\n",
      "2023-03-10 06:57.30 [info     ] SAC_online_20230310065258: epoch=74 step=74000 epoch=74 metrics={'time_inference': 0.0003004868030548096, 'time_environment_step': 0.00032397246360778807, 'time_sample_batch': 0.00013623881340026856, 'time_algorithm_update': 0.0029062225818634034, 'temp_loss': 205826.329953125, 'temp': 123726.3656640625, 'critic_loss': 5933227837.696, 'actor_loss': -230843.804109375, 'time_step': 0.0036951537132263183, 'rollout_return': 14.098591549295774, 'evaluation': 14.6} step=74000\n",
      "2023-03-10 06:57.33 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_75000.pt\n",
      "2023-03-10 06:57.33 [info     ] SAC_online_20230310065258: epoch=75 step=75000 epoch=75 metrics={'time_inference': 0.00030266618728637694, 'time_environment_step': 0.0003216557502746582, 'time_sample_batch': 0.00013540887832641602, 'time_algorithm_update': 0.00288047456741333, 'temp_loss': 164716.005703125, 'temp': 98949.80259375, 'critic_loss': 3503385625.088, 'actor_loss': -186267.860890625, 'time_step': 0.003668193817138672, 'rollout_return': 14.485915492957746, 'evaluation': 13.1} step=75000\n",
      "2023-03-10 06:57.37 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_76000.pt\n",
      "2023-03-10 06:57.37 [info     ] SAC_online_20230310065258: epoch=76 step=76000 epoch=76 metrics={'time_inference': 0.0002910146713256836, 'time_environment_step': 0.00031605744361877443, 'time_sample_batch': 0.00013385796546936036, 'time_algorithm_update': 0.0028060717582702637, 'temp_loss': 131708.7090234375, 'temp': 79132.975328125, 'critic_loss': 2131618646.272, 'actor_loss': -150418.0322578125, 'time_step': 0.0035744848251342775, 'rollout_return': 14.077464788732394, 'evaluation': 14.0} step=76000\n",
      "2023-03-10 06:57.41 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_77000.pt\n",
      "2023-03-10 06:57.41 [info     ] SAC_online_20230310065258: epoch=77 step=77000 epoch=77 metrics={'time_inference': 0.0002995262145996094, 'time_environment_step': 0.000319005012512207, 'time_sample_batch': 0.00013572120666503906, 'time_algorithm_update': 0.002856559753417969, 'temp_loss': 105150.41175, 'temp': 63290.32838671875, 'critic_loss': 1355922410.496, 'actor_loss': -120703.0520703125, 'time_step': 0.003638846158981323, 'rollout_return': 14.387323943661972, 'evaluation': 13.6} step=77000\n",
      "2023-03-10 06:57.45 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_78000.pt\n",
      "2023-03-10 06:57.45 [info     ] SAC_online_20230310065258: epoch=78 step=78000 epoch=78 metrics={'time_inference': 0.0003063168525695801, 'time_environment_step': 0.00033015918731689453, 'time_sample_batch': 0.00013706493377685548, 'time_algorithm_update': 0.0029294815063476564, 'temp_loss': 84059.4531484375, 'temp': 50619.12513671875, 'critic_loss': 899025086.688, 'actor_loss': -96477.2226953125, 'time_step': 0.0037319204807281492, 'rollout_return': 14.514084507042254, 'evaluation': 15.8} step=78000\n",
      "2023-03-10 06:57.48 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_79000.pt\n",
      "2023-03-10 06:57.48 [info     ] SAC_online_20230310065258: epoch=79 step=79000 epoch=79 metrics={'time_inference': 0.00030855154991149905, 'time_environment_step': 0.0003265423774719238, 'time_sample_batch': 0.0001346728801727295, 'time_algorithm_update': 0.002844135522842407, 'temp_loss': 67190.87157421875, 'temp': 40488.82363671875, 'critic_loss': 590139645.248, 'actor_loss': -76961.84973828125, 'time_step': 0.0036446495056152344, 'rollout_return': 14.133802816901408, 'evaluation': 14.8} step=79000\n",
      "2023-03-10 06:57.52 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_80000.pt\n",
      "2023-03-10 06:57.52 [info     ] SAC_online_20230310065258: epoch=80 step=80000 epoch=80 metrics={'time_inference': 0.0002988889217376709, 'time_environment_step': 0.00031436777114868166, 'time_sample_batch': 0.00013611292839050293, 'time_algorithm_update': 0.0028107149600982667, 'temp_loss': 53670.5698671875, 'temp': 32387.299625, 'critic_loss': 402185627.504, 'actor_loss': -61314.90683203125, 'time_step': 0.0035874361991882326, 'rollout_return': 14.584507042253522, 'evaluation': 14.9} step=80000\n",
      "2023-03-10 06:57.56 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_81000.pt\n",
      "2023-03-10 06:57.56 [info     ] SAC_online_20230310065258: epoch=81 step=81000 epoch=81 metrics={'time_inference': 0.00029849672317504885, 'time_environment_step': 0.00032052206993103026, 'time_sample_batch': 0.00013379955291748048, 'time_algorithm_update': 0.0028326070308685304, 'temp_loss': 42831.39853125, 'temp': 25906.32051171875, 'critic_loss': 298333700.88, 'actor_loss': -48623.89000390625, 'time_step': 0.0036129238605499265, 'rollout_return': 14.584507042253522, 'evaluation': 16.4} step=81000\n",
      "2023-03-10 06:57.59 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_82000.pt\n",
      "2023-03-10 06:57.59 [info     ] SAC_online_20230310065258: epoch=82 step=82000 epoch=82 metrics={'time_inference': 0.0003014097213745117, 'time_environment_step': 0.0003244025707244873, 'time_sample_batch': 0.00013316035270690918, 'time_algorithm_update': 0.002849641561508179, 'temp_loss': 34240.62713476562, 'temp': 20726.766140625, 'critic_loss': 210997767.136, 'actor_loss': -38465.076587890624, 'time_step': 0.003636526107788086, 'rollout_return': 14.408450704225352, 'evaluation': 15.2} step=82000\n",
      "2023-03-10 06:58.03 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_83000.pt\n",
      "2023-03-10 06:58.03 [info     ] SAC_online_20230310065258: epoch=83 step=83000 epoch=83 metrics={'time_inference': 0.0003055558204650879, 'time_environment_step': 0.00032445049285888673, 'time_sample_batch': 0.00013562607765197753, 'time_algorithm_update': 0.002853827953338623, 'temp_loss': 27322.424716796875, 'temp': 16582.101083984377, 'critic_loss': 153713775.168, 'actor_loss': -30299.95976953125, 'time_step': 0.003647723913192749, 'rollout_return': 15.056338028169014, 'evaluation': 15.5} step=83000\n",
      "2023-03-10 06:58.07 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_84000.pt\n",
      "2023-03-10 06:58.07 [info     ] SAC_online_20230310065258: epoch=84 step=84000 epoch=84 metrics={'time_inference': 0.00029808783531188964, 'time_environment_step': 0.0003203911781311035, 'time_sample_batch': 0.00013399600982666015, 'time_algorithm_update': 0.002823108434677124, 'temp_loss': 21804.943064453124, 'temp': 13269.545671875, 'critic_loss': 116915199.256, 'actor_loss': -23899.6969921875, 'time_step': 0.0036050357818603516, 'rollout_return': 14.570422535211268, 'evaluation': 16.9} step=84000\n",
      "2023-03-10 06:58.10 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_85000.pt\n",
      "2023-03-10 06:58.10 [info     ] SAC_online_20230310065258: epoch=85 step=85000 epoch=85 metrics={'time_inference': 0.0003034965991973877, 'time_environment_step': 0.00031487369537353514, 'time_sample_batch': 0.00013298320770263673, 'time_algorithm_update': 0.002789708375930786, 'temp_loss': 17389.717163085938, 'temp': 10619.62562890625, 'critic_loss': 88967025.676, 'actor_loss': -18834.67968261719, 'time_step': 0.0035683434009552003, 'rollout_return': 14.049295774647888, 'evaluation': 13.9} step=85000\n",
      "2023-03-10 06:58.14 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_86000.pt\n",
      "2023-03-10 06:58.14 [info     ] SAC_online_20230310065258: epoch=86 step=86000 epoch=86 metrics={'time_inference': 0.00029228925704956056, 'time_environment_step': 0.0003164718151092529, 'time_sample_batch': 0.00013555526733398436, 'time_algorithm_update': 0.002793487310409546, 'temp_loss': 13843.935018554688, 'temp': 8500.705178222655, 'critic_loss': 70029213.484, 'actor_loss': -14860.464592773438, 'time_step': 0.003564897537231445, 'rollout_return': 14.401408450704226, 'evaluation': 14.7} step=86000\n",
      "2023-03-10 06:58.18 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_87000.pt\n",
      "2023-03-10 06:58.18 [info     ] SAC_online_20230310065258: epoch=87 step=87000 epoch=87 metrics={'time_inference': 0.0002983825206756592, 'time_environment_step': 0.0003145022392272949, 'time_sample_batch': 0.0001337125301361084, 'time_algorithm_update': 0.002800828456878662, 'temp_loss': 11018.103053710938, 'temp': 6807.208560546875, 'critic_loss': 54713159.324, 'actor_loss': -11690.034709960937, 'time_step': 0.0035745339393615724, 'rollout_return': 14.443661971830986, 'evaluation': 14.1} step=87000\n",
      "2023-03-10 06:58.21 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_88000.pt\n",
      "2023-03-10 06:58.21 [info     ] SAC_online_20230310065258: epoch=88 step=88000 epoch=88 metrics={'time_inference': 0.0002969827651977539, 'time_environment_step': 0.00031700658798217775, 'time_sample_batch': 0.00013339090347290038, 'time_algorithm_update': 0.0028113844394683837, 'temp_loss': 8768.543510742187, 'temp': 5453.2619477539065, 'critic_loss': 44138839.66, 'actor_loss': -9154.125376953125, 'time_step': 0.0035862343311309814, 'rollout_return': 13.640845070422536, 'evaluation': 12.8} step=88000\n",
      "2023-03-10 06:58.25 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_89000.pt\n",
      "2023-03-10 06:58.25 [info     ] SAC_online_20230310065258: epoch=89 step=89000 epoch=89 metrics={'time_inference': 0.00029258012771606447, 'time_environment_step': 0.00031382155418395994, 'time_sample_batch': 0.0001331205368041992, 'time_algorithm_update': 0.0027934701442718507, 'temp_loss': 6981.937700683594, 'temp': 4368.383672363281, 'critic_loss': 35638633.834, 'actor_loss': -7113.145541503906, 'time_step': 0.003560652017593384, 'rollout_return': 14.52112676056338, 'evaluation': 14.1} step=89000\n",
      "2023-03-10 06:58.29 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_90000.pt\n",
      "2023-03-10 06:58.29 [info     ] SAC_online_20230310065258: epoch=90 step=90000 epoch=90 metrics={'time_inference': 0.00029096198081970216, 'time_environment_step': 0.0003152091503143311, 'time_sample_batch': 0.00013262653350830077, 'time_algorithm_update': 0.0028166050910949708, 'temp_loss': 5559.721577148437, 'temp': 3498.9683259277344, 'critic_loss': 28083792.288, 'actor_loss': -5588.441112060547, 'time_step': 0.0035829434394836425, 'rollout_return': 14.035211267605634, 'evaluation': 14.0} step=90000\n",
      "2023-03-10 06:58.33 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_91000.pt\n",
      "2023-03-10 06:58.33 [info     ] SAC_online_20230310065258: epoch=91 step=91000 epoch=91 metrics={'time_inference': 0.00031222081184387207, 'time_environment_step': 0.0003449740409851074, 'time_sample_batch': 0.00014476251602172851, 'time_algorithm_update': 0.0030168509483337403, 'temp_loss': 4421.832099365234, 'temp': 2803.5767438964845, 'critic_loss': 23182712.545, 'actor_loss': -4359.2605413818355, 'time_step': 0.0038492565155029295, 'rollout_return': 14.633802816901408, 'evaluation': 14.7} step=91000\n",
      "2023-03-10 06:58.36 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_92000.pt\n",
      "2023-03-10 06:58.36 [info     ] SAC_online_20230310065258: epoch=92 step=92000 epoch=92 metrics={'time_inference': 0.00030793166160583495, 'time_environment_step': 0.00033472895622253416, 'time_sample_batch': 0.00014436650276184083, 'time_algorithm_update': 0.0029075345993041994, 'temp_loss': 3512.283310546875, 'temp': 2246.9421179199217, 'critic_loss': 18822117.611, 'actor_loss': -3498.1634099121093, 'time_step': 0.003724625587463379, 'rollout_return': 14.169014084507042, 'evaluation': 14.3} step=92000\n",
      "2023-03-10 06:58.41 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_93000.pt\n",
      "2023-03-10 06:58.41 [info     ] SAC_online_20230310065258: epoch=93 step=93000 epoch=93 metrics={'time_inference': 0.0003797538280487061, 'time_environment_step': 0.0004564063549041748, 'time_sample_batch': 0.0001786673069000244, 'time_algorithm_update': 0.0035957350730895996, 'temp_loss': 2792.6977119140624, 'temp': 1801.006644165039, 'critic_loss': 15980126.324, 'actor_loss': -2631.7277842407225, 'time_step': 0.0046526029109954835, 'rollout_return': 14.894366197183098, 'evaluation': 14.6} step=93000\n",
      "2023-03-10 06:58.45 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_94000.pt\n",
      "2023-03-10 06:58.45 [info     ] SAC_online_20230310065258: epoch=94 step=94000 epoch=94 metrics={'time_inference': 0.00031996655464172364, 'time_environment_step': 0.0003437302112579346, 'time_sample_batch': 0.00014071106910705565, 'time_algorithm_update': 0.003032740116119385, 'temp_loss': 2226.613403930664, 'temp': 1443.3073120117188, 'critic_loss': 13912568.38, 'actor_loss': -1983.9359201831817, 'time_step': 0.003867150068283081, 'rollout_return': 14.394366197183098, 'evaluation': 14.3} step=94000\n",
      "2023-03-10 06:58.49 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_95000.pt\n",
      "2023-03-10 06:58.49 [info     ] SAC_online_20230310065258: epoch=95 step=95000 epoch=95 metrics={'time_inference': 0.00030498027801513673, 'time_environment_step': 0.0003288576602935791, 'time_sample_batch': 0.0001379685401916504, 'time_algorithm_update': 0.002845012903213501, 'temp_loss': 1765.869202270508, 'temp': 1156.653301513672, 'critic_loss': 11919860.2155, 'actor_loss': -1473.3482677783966, 'time_step': 0.0036460564136505127, 'rollout_return': 13.830985915492958, 'evaluation': 16.1} step=95000\n",
      "2023-03-10 06:58.52 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_96000.pt\n",
      "2023-03-10 06:58.52 [info     ] SAC_online_20230310065258: epoch=96 step=96000 epoch=96 metrics={'time_inference': 0.0002980492115020752, 'time_environment_step': 0.00031954050064086914, 'time_sample_batch': 0.0001334049701690674, 'time_algorithm_update': 0.0028250279426574707, 'temp_loss': 1395.016018798828, 'temp': 927.8588006591797, 'critic_loss': 9924230.704, 'actor_loss': -1111.035787199974, 'time_step': 0.003603824853897095, 'rollout_return': 14.816901408450704, 'evaluation': 14.5} step=96000\n",
      "2023-03-10 06:58.56 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_97000.pt\n",
      "2023-03-10 06:58.56 [info     ] SAC_online_20230310065258: epoch=97 step=97000 epoch=97 metrics={'time_inference': 0.00029921245574951173, 'time_environment_step': 0.00031537818908691404, 'time_sample_batch': 0.00013438129425048827, 'time_algorithm_update': 0.0028156468868255615, 'temp_loss': 1105.9509074707032, 'temp': 744.7050050659179, 'critic_loss': 9192897.1295, 'actor_loss': -946.8833868122101, 'time_step': 0.0035922529697418213, 'rollout_return': 14.23943661971831, 'evaluation': 14.3} step=97000\n",
      "2023-03-10 06:59.00 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_98000.pt\n",
      "2023-03-10 06:59.00 [info     ] SAC_online_20230310065258: epoch=98 step=98000 epoch=98 metrics={'time_inference': 0.00029673361778259275, 'time_environment_step': 0.00031627631187438964, 'time_sample_batch': 0.00013402700424194335, 'time_algorithm_update': 0.0028157761096954345, 'temp_loss': 869.8549802246093, 'temp': 598.1469752197265, 'critic_loss': 7849474.9705, 'actor_loss': -764.1324606986046, 'time_step': 0.0035906410217285156, 'rollout_return': 14.373239436619718, 'evaluation': 14.5} step=98000\n",
      "2023-03-10 06:59.03 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_99000.pt\n",
      "2023-03-10 06:59.03 [info     ] SAC_online_20230310065258: epoch=99 step=99000 epoch=99 metrics={'time_inference': 0.00029259371757507326, 'time_environment_step': 0.0003167881965637207, 'time_sample_batch': 0.0001367034912109375, 'time_algorithm_update': 0.0028034904003143312, 'temp_loss': 689.4610739746093, 'temp': 480.68476599121095, 'critic_loss': 7091381.09925, 'actor_loss': -554.5331451749802, 'time_step': 0.0035771205425262453, 'rollout_return': 14.035211267605634, 'evaluation': 15.2} step=99000\n",
      "2023-03-10 06:59.07 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230310065258/model_100000.pt\n",
      "2023-03-10 06:59.07 [info     ] SAC_online_20230310065258: epoch=100 step=100000 epoch=100 metrics={'time_inference': 0.0002979753017425537, 'time_environment_step': 0.00031586408615112303, 'time_sample_batch': 0.00013291597366333008, 'time_algorithm_update': 0.0027975826263427735, 'temp_loss': 544.7908496704101, 'temp': 386.1639324951172, 'critic_loss': 6002463.638, 'actor_loss': -511.5920666465759, 'time_step': 0.0035716347694396975, 'rollout_return': 14.838028169014084, 'evaluation': 14.8} step=100000\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "# skip if there is a pre-trained model\n",
    "sac.fit_online(\n",
    "    env_,\n",
    "    buffer,\n",
    "    eval_env=env_,\n",
    "    n_steps=100000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    update_start_step=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "sac.save_model(\"d3rlpy_logs/sac.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-10 06:59.07 [warning  ] Parameters will be reinitialized.\n"
     ]
    }
   ],
   "source": [
    "# reload model\n",
    "sac.build_with_env(env_)\n",
    "sac.load_model(\"d3rlpy_logs/sac.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy = TruncatedGaussianHead(\n",
    "    sac, \n",
    "    minimum=env.action_space.low,\n",
    "    maximum=env.action_space.high,\n",
    "    sigma=np.array([1.0]),\n",
    "    name=\"sac_sigma_1.0\",\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset class\n",
    "dataset = SyntheticDataset(\n",
    "    env=env,\n",
    "    state_keys=env.obs_keys,\n",
    "    max_episode_steps=env.step_per_episode,\n",
    "    info_keys={\n",
    "        \"search_volume\": int,\n",
    "        \"impression\": int,\n",
    "        \"click\": int,\n",
    "        \"conversion\": int,\n",
    "        \"average_bid_price\": float,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c70595d7de044d4adb1fc4ca67436cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4646975668d248499efd57442598ea01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[obtain_trajectories]:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collect logged data by a behavior policy\n",
    "# skip if there is a preserved logged dataset\n",
    "train_logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000, \n",
    "    obtain_info=True,\n",
    "    random_state=random_state,\n",
    ")\n",
    "test_logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000, \n",
    "    obtain_info=True,\n",
    "    random_state=random_state + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/train_dataset_continuous_sac.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_logged_dataset, f)\n",
    "with open(\"logs/test_dataset_continuous_sac.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_logged_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/train_dataset_continuous_sac.pkl\", \"rb\") as f:\n",
    "    train_logged_dataset = pickle.load(f)\n",
    "with open(\"logs/test_dataset_continuous_sac.pkl\", \"rb\") as f:\n",
    "    test_logged_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline Policy Learning\n",
    "Here, we learn several \"candidate\" policies to be evaluated and selected using [d3rlpy](https://github.com/takuseno/d3rlpy)'s algorithm implementation.\n",
    "\n",
    "For more about the offline RL procedure, please refer to [examples/quickstart/rtb_synthetic_discrete_basic.ipynb](https://github.com/negocia-inc/rtb_reinforcement_learing/blob/ope/examples/quickstart/rtb_synthetic_discrete_basic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules from d3rlpy\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from d3rlpy.algos import CQL\n",
    "# transform offline dataset for d3rlpy\n",
    "offlinerl_dataset = MDPDataset(\n",
    "    observations=train_logged_dataset[\"state\"],\n",
    "    actions=train_logged_dataset[\"action\"],\n",
    "    rewards=train_logged_dataset[\"reward\"],\n",
    "    terminals=train_logged_dataset[\"done\"],\n",
    "    episode_terminals=train_logged_dataset[\"done\"],\n",
    "    discrete_action=True,\n",
    ")\n",
    "train_episodes, test_episodes = train_test_split(offlinerl_dataset, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Conservative Q-Learning policy\n",
    "cql = CQL(\n",
    "    actor_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    critic_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    action_scaler=MinMaxActionScaler(\n",
    "        minimum=env_.action_space.low,  # minimum value that policy can take\n",
    "        maximum=env_.action_space.high,  # maximum value that policy can take\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The action-space of the given dataset is not compatible with the algorithm. Please use discrete action-space algorithms. The algorithms list is available below.\nhttps://d3rlpy.readthedocs.io/en/v1.1.1/references/algos.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cql\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     train_episodes,\n\u001b[1;32m      3\u001b[0m     eval_episodes\u001b[39m=\u001b[39;49mtest_episodes,\n\u001b[1;32m      4\u001b[0m     n_steps\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     scorers\u001b[39m=\u001b[39;49m{},\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/d3rlpy/base.py:406\u001b[0m, in \u001b[0;36mLearnableBase.fit\u001b[0;34m(self, dataset, n_epochs, n_steps, n_steps_per_epoch, save_metrics, experiment_name, with_timestamp, logdir, verbose, show_progress, tensorboard_dir, eval_episodes, save_interval, scorers, shuffle, callback)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[1;32m    350\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    351\u001b[0m     dataset: Union[List[Episode], List[Transition], MDPDataset],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     callback: Optional[Callable[[\u001b[39m\"\u001b[39m\u001b[39mLearnableBase\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m], \u001b[39mNone\u001b[39;00m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[\u001b[39mint\u001b[39m, Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]]]:\n\u001b[1;32m    370\u001b[0m     \u001b[39m\"\"\"Trains with the given dataset.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \n\u001b[1;32m    372\u001b[0m \u001b[39m    .. code-block:: python\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m \n\u001b[1;32m    405\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    407\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitter(\n\u001b[1;32m    408\u001b[0m             dataset,\n\u001b[1;32m    409\u001b[0m             n_epochs,\n\u001b[1;32m    410\u001b[0m             n_steps,\n\u001b[1;32m    411\u001b[0m             n_steps_per_epoch,\n\u001b[1;32m    412\u001b[0m             save_metrics,\n\u001b[1;32m    413\u001b[0m             experiment_name,\n\u001b[1;32m    414\u001b[0m             with_timestamp,\n\u001b[1;32m    415\u001b[0m             logdir,\n\u001b[1;32m    416\u001b[0m             verbose,\n\u001b[1;32m    417\u001b[0m             show_progress,\n\u001b[1;32m    418\u001b[0m             tensorboard_dir,\n\u001b[1;32m    419\u001b[0m             eval_episodes,\n\u001b[1;32m    420\u001b[0m             save_interval,\n\u001b[1;32m    421\u001b[0m             scorers,\n\u001b[1;32m    422\u001b[0m             shuffle,\n\u001b[1;32m    423\u001b[0m             callback,\n\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/d3rlpy/base.py:507\u001b[0m, in \u001b[0;36mLearnableBase.fitter\u001b[0;34m(self, dataset, n_epochs, n_steps, n_steps_per_epoch, save_metrics, experiment_name, with_timestamp, logdir, verbose, show_progress, tensorboard_dir, eval_episodes, save_interval, scorers, shuffle, callback)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39melif\u001b[39;00m transitions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mis_discrete:\n\u001b[0;32m--> 507\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    508\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_action_type() \u001b[39m==\u001b[39m ActionSpace\u001b[39m.\u001b[39mDISCRETE\n\u001b[1;32m    509\u001b[0m     ), DISCRETE_ACTION_SPACE_MISMATCH_ERROR\n\u001b[1;32m    510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    512\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_action_type() \u001b[39m==\u001b[39m ActionSpace\u001b[39m.\u001b[39mCONTINUOUS\n\u001b[1;32m    513\u001b[0m     ), CONTINUOUS_ACTION_SPACE_MISMATCH_ERROR\n",
      "\u001b[0;31mAssertionError\u001b[0m: The action-space of the given dataset is not compatible with the algorithm. Please use discrete action-space algorithms. The algorithms list is available below.\nhttps://d3rlpy.readthedocs.io/en/v1.1.1/references/algos.html"
     ]
    }
   ],
   "source": [
    "cql.fit(\n",
    "    train_episodes,\n",
    "    eval_episodes=test_episodes,\n",
    "    n_steps=10000,\n",
    "    scorers={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "cql.save_model(\"d3rlpy_logs/cql_continuous.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "cql.build_with_env(env)\n",
    "cql.load_model(\"d3rlpy_logs/cql_continuous.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Off-Policy Evaluation\n",
    "In this section, we aim at evaluating the various performance statistics of policies in an offline manner using (basic) model-based and importance sampling-based estimators.\n",
    "\n",
    "#### Estimation Target\n",
    "\n",
    "The goal to estimate the estimators' average policy performance, the *policy value*.\n",
    "\n",
    "$$ V(\\pi) := \\mathbb{E}\\left[\\sum_{t=1}^T \\gamma^{t-1} r_t \\mid \\pi \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ope modules from OFRL\n",
    "from ofrl.ope import CreateOPEInput\n",
    "from ofrl.ope import OffPolicyEvaluation as OPE\n",
    "# basic estimators\n",
    "from ofrl.ope import ContinuousDirectMethod as DM\n",
    "from ofrl.ope import ContinuousTrajectoryWiseImportanceSampling as TIS\n",
    "from ofrl.ope import ContinuousPerDecisionImportanceSampling as PDIS\n",
    "from ofrl.ope import ContinuousDoublyRobust as DR\n",
    "# self normalized estimators\n",
    "from ofrl.ope import ContinuousSelfNormalizedTrajectoryWiseImportanceSampling as SNTIS\n",
    "from ofrl.ope import ContinuousSelfNormalizedPerDecisionImportanceSampling as SNPDIS\n",
    "from ofrl.ope import ContinuousSelfNormalizedDoublyRobust as SNDR\n",
    "# marginal estimators\n",
    "from ofrl.ope import ContinuousStateActionMarginalImportanceSampling as SAMIS\n",
    "from ofrl.ope import ContinuousStateActionMarginalDoublyRobust as SAMDR\n",
    "from ofrl.ope import ContinuousStateMarginalImportanceSampling as SMIS\n",
    "from ofrl.ope import ContinuousStateMarginalDoublyRobust as SMDR\n",
    "from ofrl.ope import ContinuousStateActionMarginalSelfNormalizedImportanceSampling as SAMSNIS\n",
    "from ofrl.ope import ContinuousStateActionMarginalSelfNormalizedDoublyRobust as SAMSNDR\n",
    "from ofrl.ope import ContinuousStateMarginalSelfNormalizedImportanceSampling as SMSNIS\n",
    "from ofrl.ope import ContinuousStateMarginalSelfNormalizedDoublyRobust as SMSNDR\n",
    "# double reinforcement learning estimators\n",
    "from ofrl.ope import ContinuousDoubleReinforcementLearning as DRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation policies (should be deterministic policy)\n",
    "cql_ = ContinuousEvalHead(\n",
    "    base_policy=cql,\n",
    "    name=\"cql\",\n",
    ")\n",
    "sac_ = ContinuousEvalHead(\n",
    "    base_policy=sac, \n",
    "    name=\"sac\", \n",
    ")\n",
    "evaluation_policies = [cql_, sac_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, prepare OPE inputs\n",
    "prep = CreateOPEInput(\n",
    "    env=env,\n",
    "    model_args={\n",
    "        \"fqe\": {\n",
    "            \"encoder_factory\": VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "            \"q_func_factory\": MeanQFunctionFactory(),\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"use_gpu\": torch.cuda.is_available(),\n",
    "        },\n",
    "    },\n",
    "    state_scaler=MinMaxScaler(\n",
    "        minimum=test_logged_dataset[\"state\"].min(axis=0),\n",
    "        maximum=test_logged_dataset[\"state\"].max(axis=0),\n",
    "    ),\n",
    "    action_scaler=MinMaxActionScaler(\n",
    "        minimum=env.action_space.low,  # minimum value that policy can take\n",
    "        maximum=env.action_space.high,  # maximum value that policy can take\n",
    "    ),\n",
    "    gamma=0.95,\n",
    "    sigma=0.1,\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes time\n",
    "input_dict = prep.obtain_whole_inputs(\n",
    "    logged_dataset=test_logged_dataset,\n",
    "    evaluation_policies=evaluation_policies,\n",
    "    require_value_prediction=True,\n",
    "    require_weight_prediction=True,\n",
    "    n_trajectories_on_policy_evaluation=100,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/ope_input_dict_continuous_zoo.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/ope_input_dict_continuous_zoo.pkl\", \"rb\") as f:\n",
    "    input_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_estimators = [DM(), TIS(), PDIS(), DR(), SNTIS(), SNPDIS(), SNDR()]\n",
    "state_marginal_estimators = [SMIS(), SMDR(), SMSNIS(), SMSNDR()]\n",
    "state_action_marginal_estimators = [SAMIS(), SAMDR(), SAMSNIS(), SAMSNDR()]\n",
    "drl_estimators = [DRL()]\n",
    "all_estimators = basic_estimators + state_marginal_estimators + state_action_marginal_estimators + drl_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_estimators_name = [\"dm\", \"tis\", \"pdis\", \"dr\", \"sntis\", \"snpdis\", \"sndr\"]\n",
    "state_marginal_estimators_name = [\"sm_is\", \"sm_dr\", \"sm_snis\", \"sm_sndr\"]\n",
    "state_action_marginal_estimators_name = [\"sam_is\", \"sam_dr\", \"sam_snis\", \"sam_sndr\"]\n",
    "drl_estimators_name = [\"drl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope = OPE(\n",
    "    logged_dataset=test_logged_dataset,\n",
    "    ope_estimators=all_estimators,\n",
    "    action_scaler=MinMaxActionScaler(\n",
    "        minimum=env.action_space.low,  # minimum value that policy can take\n",
    "        maximum=env.action_space.high,  # maximum value that policy can take\n",
    "    ),\n",
    "    sigma=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cql':            policy_value  relative_policy_value\n",
       " on_policy  1.325953e+01           1.000000e+00\n",
       " dm         3.124717e+00           2.356582e-01\n",
       " tis        8.444623e-13           6.368722e-14\n",
       " pdis       9.176629e+00           6.920782e-01\n",
       " dr         1.180140e+01           8.900321e-01\n",
       " sntis      8.438487e-03           6.364094e-04\n",
       " snpdis     1.795548e+01           1.354157e+00\n",
       " sndr       2.023309e+01           1.525929e+00\n",
       " sm_is      0.000000e+00           0.000000e+00\n",
       " sm_dr      3.124717e+00           2.356582e-01\n",
       " sm_snis    0.000000e+00           0.000000e+00\n",
       " sm_sndr    3.124717e+00           2.356582e-01\n",
       " sam_is     1.391036e-03           1.049084e-04\n",
       " sam_dr     3.126076e+00           2.357608e-01\n",
       " sam_snis   7.931812e+00           5.981972e-01\n",
       " sam_sndr   1.087879e+01           8.204511e-01\n",
       " drl        3.127077e+00           2.358363e-01,\n",
       " 'sac':            policy_value  relative_policy_value\n",
       " on_policy     13.849778               1.000000\n",
       " dm             3.909876               0.282306\n",
       " tis           21.615777               1.560731\n",
       " pdis          10.720657               0.774067\n",
       " dr             9.806448               0.708058\n",
       " sntis         14.754259               1.065306\n",
       " snpdis        13.964634               1.008293\n",
       " sndr          13.940724               1.006567\n",
       " sm_is          0.000000               0.000000\n",
       " sm_dr          3.909876               0.282306\n",
       " sm_snis        0.000000               0.000000\n",
       " sm_sndr        3.909876               0.282306\n",
       " sam_is         0.000383               0.000028\n",
       " sam_dr         3.910206               0.282330\n",
       " sam_snis       3.185051               0.229971\n",
       " sam_sndr       6.436976               0.464771\n",
       " drl            3.911633               0.282433}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy value estimation by ope\n",
    "policy_value_df_dict, policy_value_interval_df_dict = ope.summarize_off_policy_estimates(\n",
    "    input_dict, \n",
    "    random_state=random_state,\n",
    ")\n",
    "# dictionary of the estimation\n",
    "policy_value_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize and compare the result\n",
    "# ope.visualize_off_policy_estimates(\n",
    "#     input_dict, \n",
    "#     compared_estimators=basic_estimators_name,\n",
    "#     random_state=random_state, \n",
    "#     sharey=False,\n",
    "# )\n",
    "# # relative policy value to the behavior policy\n",
    "# # ope.visualize_off_policy_estimates(input_dict, random_state=random_state, is_relative=True, sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize and compare the result\n",
    "# ope.visualize_off_policy_estimates(\n",
    "#     input_dict, \n",
    "#     compared_estimators=state_marginal_estimators_name,\n",
    "#     random_state=random_state, \n",
    "#     sharey=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize and compare the result\n",
    "# ope.visualize_off_policy_estimates(\n",
    "#     input_dict, \n",
    "#     compared_estimators=state_action_marginal_estimators_name,\n",
    "#     random_state=random_state, \n",
    "#     sharey=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize and compare the result\n",
    "# ope.visualize_off_policy_estimates(\n",
    "#     input_dict, \n",
    "#     compared_estimators=[\"dm\", \"sm_is\", \"sm_dr\", \"sam_is\", \"sam_dr\", \"drl\"],\n",
    "#     random_state=random_state, \n",
    "#     sharey=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ope.visualize_off_policy_estimates(\n",
    "#     input_dict, \n",
    "#     compared_estimators=[\"dm\", \"snpdis\", \"sndr\", \"sm_snis\", \"sm_sndr\", \"sam_snis\", \"sam_sndr\", \"drl\"],\n",
    "#     random_state=random_state, \n",
    "#     sharey=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced topics in OPE (e.g., cumulative distribution function estimation) and OPS, please refer to [examples/quickstart/rtb_synthetic_continuous_advanced.ipynb](https://github.com/negocia-inc/rtb_reinforcement_learing/blob/ope/examples/quickstart/rtb_synthetic_continuous_advanced.ipynb). \n",
    "\n",
    "For the examples in the discrete action space, please refer to [examples/quickstart/rtb_synthetic_discrete_zoo.ipynb](https://github.com/negocia-inc/rtb_reinforcement_learing/blob/ope/examples/quickstart/rtb_synthetic_discrete_zoo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Haanvid Lee, Jongmin Lee, Yunseon Choi, Wonseok Jeon, Byung-Jun Lee, Yung-Kyun Noh, and Kee-Eung Kim. \\\n",
    "\"Local Metric Learning for Off-Policy Evaluation in Contextual Bandits with Continuous Actions.\", 2022.\n",
    "\n",
    "- Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. \\\n",
    "\"Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation.\", 2021.\n",
    "\n",
    "- Takuma Seno and Michita Imai. \\\n",
    "\"d3rlpy: An Offline Deep Reinforcement Library.\", 2021.\n",
    "\n",
    "- Christina J. Yuan, Yash Chandak, Stephen Giguere, Philip S. Thomas, and Scott Niekum. \\\n",
    "\"SOPE: Spectrum of Off-Policy Estimators.\", 2021.\n",
    "\n",
    "- Nathan Kallus and Masatoshi Uehara. \\\n",
    "\"Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes.\", 2020.\n",
    "\n",
    "- Masatoshi Uehara, Jiawei Huang, and Nan Jiang. \\\n",
    "\"Minimax Weight and Q-Function Learning for Off-Policy Evaluation.\", 2020.\n",
    "\n",
    "- Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. \\\n",
    "\"Off-Policy Evaluation via the Regularized Lagrangian.\", 2020.\n",
    "\n",
    "- Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. \\\n",
    "\"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.\" 2020.\n",
    "\n",
    "- Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. \\\n",
    "\"Conservative Q-Learning for Offline Reinforcement Learning.\", 2020.\n",
    "\n",
    "- Nathan Kallus and Angela Zhou. \\\n",
    "\"Policy Evaluation and Optimization with Continuous Treatments.\", 2019.\n",
    "\n",
    "- Nathan Kallus and Masatoshi Uehara. \\\n",
    "\"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning.\", 2019.\n",
    "\n",
    "- Hoang Le, Cameron Voloshin, and Yisong Yue. \\\n",
    "\"Batch Policy Learning under Constraints.\", 2019.\n",
    "\n",
    "- Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. \\\n",
    "\"Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation.\", 2018\n",
    "\n",
    "- Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and Kun Gai. \\\n",
    "\"Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising.\", 2018.\n",
    "\n",
    "- Jun Zhao, Guang Qiu, Ziyu Guan, Wei Zhao, and Xiaofei He. \\\n",
    "\"Deep Reinforcement Learning for Sponsored Search Real-time Bidding.\", 2018.\n",
    "\n",
    "- Josiah P. Hanna, Peter Stone, and Scott Niekum. \\\n",
    "\"Bootstrapping with Models: Confidence Intervals for Off-Policy Evaluation.\", 2017.\n",
    "\n",
    "- Nan Jiang and Lihong Li. \\\n",
    "\"Doubly Robust Off-policy Value Evaluation for Reinforcement Learning.\", 2016.\n",
    "\n",
    "- Philip S. Thomas and Emma Brunskill. \\\n",
    "\"Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning.\", 2016.\n",
    "\n",
    "- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. \\\n",
    "\"OpenAI Gym.\", 2016.\n",
    "\n",
    "- Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. \\\n",
    "\"High Confidence Policy Improvement.\", 2015.\n",
    "\n",
    "- Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. \\\n",
    "\"High Confidence Off-Policy Evaluation.\", 2015.\n",
    "\n",
    "- Adith Swaminathan and Thorsten Joachims. \\\n",
    "\"The Self-Normalized Estimator for Counterfactual Learning.\", 2015.\n",
    "\n",
    "- Hado van Hasselt, Arthur Guez, and David Silver. \\\n",
    "\"Deep Reinforcement Learning with Double Q-learning.\", 2015.\n",
    "\n",
    "- Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li. \\\n",
    "\"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n",
    "\n",
    "- Alex Strehl, John Langford, Sham Kakade, and Lihong Li. \\\n",
    "\"Learning from Logged Implicit Exploration Data.\", 2010.\n",
    "\n",
    "- Alina Beygelzimer and John Langford. \\\n",
    "\"The Offset Tree for Learning with Partial Labels.\", 2009.\n",
    "\n",
    "- Doina Precup, Richard S. Sutton, and Satinder P. Singh. \\\n",
    "\"Eligibility Traces for Off-Policy Policy Evaluation.\", 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "70404ee114725fce8ed9e697d67827f8546c678889944e6d695790702cbfe1f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
