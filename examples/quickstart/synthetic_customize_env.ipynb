{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Example with Synthetic Simulation (Customization)\n",
    "This notebook provides an example of customizing the Recommender environment.\n",
    "\n",
    "This example on the Synthetic Recommender Simulation consists of the following 3 steps:\n",
    "1. Setup Synthetic Recommender Simulation Environment and Interacting Online RL Agent\n",
    "2. Standardized Environment\n",
    "3. Customize Environmental Configuration\n",
    "\n",
    "\\* This library uses [d3rlpy](https://github.com/takuseno/d3rlpy)'s algorithm implementations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# delete later\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OFRL modules\n",
    "import ofrl\n",
    "from ofrl.policy import OnlineHead\n",
    "from ofrl.policy import DiscreteEpsilonGreedyHead\n",
    "\n",
    "# import synthetic modules\n",
    "# import synthetic\n",
    "from syntheticgym import SyntheticEnv\n",
    "from syntheticgym import RewardFunction\n",
    "from syntheticgym import StateTransition\n",
    "from syntheticgym import BaseRewardFunction\n",
    "from syntheticgym import BaseStateTransition\n",
    "\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.algos import DiscreteRandomPolicy\n",
    "from d3rlpy.algos import RandomPolicy as ContinuousRandomPolicy\n",
    "from d3rlpy.preprocessing import MinMaxActionScaler\n",
    "\n",
    "# import from other libraries\n",
    "import gym\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.0\n"
     ]
    }
   ],
   "source": [
    "# version\n",
    "print(ofrl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "random_state = 12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Synthetic Recommender Simulation Environment and Interacting Online RL Agent\n",
    "To begin with, we briefly describe the basic usage of the environment.\n",
    "\n",
    "#### RL setup for Recommendation\n",
    "In recommendation, the objective of the RL agent is to maximize reward\n",
    "\n",
    "We often formulate this recommendation problem as the following (Partially Observable) Markov Decision Process ((PO)MDP):\n",
    "- `state`: \n",
    "   - A vector representing user preference.  The preference changes over time in an episode by the actions presented by the RL agent.\n",
    "   - When the true state is unobservable, you can gain observation instead of state.\n",
    "- `action`:  Index of an item to present to the user.\n",
    "- `reward`: User engagement signal. Either binary or continuous.\n",
    "\n",
    "Let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "env = SyntheticEnv(action_type='continuous', random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random agent\n",
    "agent = OnlineHead(\n",
    "    ContinuousRandomPolicy(\n",
    "        action_scaler=MinMaxActionScaler(\n",
    "            minimum=0.1,  # minimum value that policy can take\n",
    "            maximum=10,  # maximum value that policy can take\n",
    "        )\n",
    "    ),\n",
    "    name=\"random\",\n",
    ")\n",
    "agent.build_with_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnlineHead(base_policy=d3rlpy.algos.random_policy.RandomPolicy(action_scaler=d3rlpy.preprocessing.action_scalers.MinMaxActionScaler(minimum=0.1, maximum=10), action_size=3, batch_size=1, distribution='uniform', gamma=0.0, generated_maxlen=100000, impl=None, n_frames=1, n_steps=1, normal_std=1.0, real_ratio=1.0, reward_scaler=None, scaler=None), name='random')\n",
      "Box(-0.1, 10.0, (3,), float64)\n",
      "Box(-inf, inf, (5,), float64)\n"
     ]
    }
   ],
   "source": [
    "print(agent)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743fd405e4f34b9bb04c24711105163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[calculate on-policy policy value]:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ofrl\n",
    "from ofrl.ope.online import calc_on_policy_policy_value\n",
    "# calculate on-policy policy value\n",
    "on_policy_performance = calc_on_policy_policy_value(\n",
    "  env,\n",
    "  agent,\n",
    "  n_trajectories=100,\n",
    "  random_state=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.74249729211489\n"
     ]
    }
   ],
   "source": [
    "print(on_policy_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09942776  0.24849768 -0.85626477 -0.01309953 -0.44159038]\n",
      "[ 0.00743469  0.0436235  -0.91310436 -0.2106408  -0.34628397]\n",
      "[ 0.14515309  0.36207813 -0.83050387  0.12027032 -0.37898336]\n",
      "[-0.08094221 -0.22962    -0.82441706 -0.41244637 -0.30157508]\n",
      "[ 0.1919433   0.41881823 -0.84215904  0.0364326  -0.27783065]\n",
      "[-0.14548493 -0.06312537 -0.82987187 -0.33383096 -0.4179939 ]\n",
      "[ 0.09630338  0.25807178 -0.90114567 -0.09857046 -0.31991399]\n",
      "[-0.10781579 -0.20386854 -0.80988352 -0.44406587 -0.30611689]\n",
      "[ 0.10047396  0.15484876 -0.92097913 -0.22751759 -0.25682687]\n",
      "[-0.63980048 -0.08198772 -0.41483501 -0.06847088 -0.63808856]\n",
      "[ 0.08136232  0.28788647 -0.91451925  0.03389718 -0.2701982 ]\n",
      "[ 0.01686929  0.12411115 -0.90164585 -0.15443479 -0.38405273]\n",
      "[-0.02177701 -0.09914732 -0.88393893 -0.3263774  -0.31910081]\n",
      "[ 0.06410097  0.09123478 -0.91274587 -0.20587645 -0.33477924]\n",
      "[ 0.10773348  0.34412105 -0.81246893  0.10823938 -0.44514344]\n",
      "[-0.1350462  -0.24277364 -0.78262197 -0.47405631 -0.29256956]\n",
      "[-0.03689883 -0.29486818 -0.76285099 -0.55169419 -0.15932084]\n",
      "[-0.12084583 -0.3333972  -0.68851123 -0.58406254 -0.24303463]\n",
      "[ 0.12421605  0.23137957 -0.90035773 -0.20471914 -0.28014265]\n",
      "[ 0.22725557 -0.60624805  0.54536     0.26485149 -0.46179473]\n",
      "[ 0.52667004  0.45383815 -0.66881757  0.0832118   0.24981684]\n",
      "[-0.16786586 -0.20808786 -0.75687793 -0.46115249 -0.37814636]\n",
      "[ 0.18153305  0.27814028 -0.90116188  0.00314289 -0.27853388]\n",
      "[-0.10884599 -0.12764196 -0.83318413 -0.38305639 -0.3618454 ]\n",
      "[-0.02639899 -0.10328484 -0.85120339 -0.44502752 -0.25697983]\n",
      "[-0.07702389 -0.2414925  -0.7906652  -0.5008664  -0.24439739]\n",
      "[ 0.03688035 -0.07798001 -0.88416115 -0.38387375 -0.25191061]\n",
      "[-0.06500047 -0.03364919 -0.85249418 -0.36069204 -0.37121098]\n",
      "[-0.02775303 -0.22076046 -0.83954366 -0.43621621 -0.23532203]\n",
      "[-0.13918208 -0.39852272 -0.88046558 -0.1583024  -0.14672661]\n",
      "[ 0.35020497  0.30153857 -0.8465774  -0.26298198  0.02404497]\n",
      "[-0.03670037 -0.33730817 -0.75540637 -0.51940694 -0.21084101]\n",
      "[-0.0159129  -0.19639413 -0.82666552 -0.47350771 -0.23149663]\n",
      "[-0.02811986 -0.15483693 -0.84715705 -0.4429324  -0.24773093]\n",
      "[ 0.04867949  0.10420343 -0.9057422  -0.24846518 -0.32352446]\n",
      "[-0.13170514 -0.36975003 -0.68854154 -0.53154311 -0.29884969]\n",
      "[ 0.19426849  0.15058217 -0.9380696  -0.12814077 -0.20782237]\n",
      "[-0.09936419 -0.09992075 -0.82952248 -0.36742009 -0.39627962]\n",
      "[-0.11048481 -0.32907275 -0.70837574 -0.58267059 -0.19545594]\n",
      "[-0.00790108  0.80669143  0.25691889 -0.20779123  0.48989999]\n",
      "[ 2.56880518e-01  2.02736826e-04 -8.40110974e-01 -3.12102028e-01\n",
      " -3.61688033e-01]\n",
      "[-0.15083731 -0.24728249 -0.77592183 -0.43091737 -0.35826666]\n",
      "[ 0.02749345 -0.14382581 -0.86299102 -0.44908579 -0.17923924]\n",
      "[ 0.04819665 -0.05703123 -0.89946681 -0.33441791 -0.27119851]\n",
      "[ 0.04833784  0.04534076 -0.91588144 -0.23643495 -0.31759624]\n",
      "[-0.06547275 -0.01750217 -0.86712121 -0.33770887 -0.35980622]\n",
      "[ 0.05857129  0.01046323 -0.92636664 -0.2468736  -0.27813343]\n",
      "[ 0.16459037  0.33645017 -0.86094164  0.07150901 -0.33671537]\n",
      "[-0.19112717 -0.30801232 -0.6936663  -0.53790565 -0.31318268]\n",
      "[ 0.28450722  0.74400779 -0.58279271 -0.12478717  0.10143407]\n",
      "[ 0.18544587  0.42261399 -0.81492085  0.19543388 -0.2910616 ]\n",
      "[-0.17784397 -0.31499822 -0.69480209 -0.55504396 -0.27986409]\n",
      "[-0.02235946 -0.35806422 -0.73062594 -0.56980555 -0.11312579]\n",
      "[ 0.0607108  -0.20634812 -0.8512772  -0.43979018 -0.18880246]\n",
      "[-0.11719026 -0.18988711 -0.76546771 -0.47085765 -0.37757329]\n",
      "[-0.04956127 -0.22894919 -0.80638523 -0.48542731 -0.24337037]\n",
      "[ 0.10223579 -0.0376193  -0.92015841 -0.28938544 -0.24020243]\n",
      "[-0.0699846  -0.25897038 -0.79943676 -0.46757224 -0.26516704]\n",
      "[ 0.08166111 -0.04432034 -0.8886875  -0.27732113 -0.35312135]\n",
      "[ 6.21133023e-01 -1.93794125e-01  7.08675405e-01  2.72794380e-01\n",
      "  4.43944635e-05]\n",
      "[-0.184577   -0.20884887 -0.81168943 -0.42719881 -0.28456095]\n",
      "[ 0.14751465  0.15299726 -0.91279449 -0.25785627 -0.23483535]\n",
      "[-0.08340469 -0.13428023 -0.84757858 -0.38889052 -0.32463393]\n",
      "[-0.10771223 -0.35266533 -0.69416511 -0.5710668  -0.23673348]\n",
      "[ 0.1735073   0.20236113 -0.91623134 -0.15453413 -0.25609475]\n",
      "[-0.04661462 -0.20459962 -0.84140199 -0.40411831 -0.29102774]\n",
      "[ 0.21802433  0.30432071 -0.88296886  0.04730603 -0.27925334]\n",
      "[-0.16143716 -0.04510135 -0.81652968 -0.3426064  -0.43336365]\n",
      "[ 0.00771273 -0.12733594 -0.87936314 -0.38634978 -0.2473467 ]\n",
      "[ 0.0743091  -0.73632855 -0.43282795  0.08948232 -0.50690364]\n",
      "[ 0.13995844 -0.12947011 -0.83943672 -0.5026116  -0.07985426]\n",
      "[-0.17185143 -0.23860866 -0.6764426  -0.47977976 -0.47515238]\n",
      "[ 0.0042499  -0.09156608 -0.883525   -0.37382662 -0.26689853]\n",
      "[ 0.01276574  0.11263787 -0.89969219 -0.25227287 -0.33773081]\n",
      "[-0.0937944  -0.16778067 -0.82336788 -0.43998066 -0.30254687]\n",
      "[ 0.11220065  0.26406243 -0.90025849 -0.12509038 -0.30260386]\n",
      "[-0.14658966 -0.19549679 -0.78491861 -0.45000081 -0.34884743]\n",
      "[ 0.01135215 -0.27446703 -0.82761592 -0.45388678 -0.18324208]\n",
      "[ 0.01818563 -0.08781673 -0.87860428 -0.39740308 -0.24916422]\n",
      "[-0.38457489 -0.23348559  0.76974791  0.09823776  0.44206801]\n",
      "[ 0.39967635  0.28521636 -0.85625027 -0.15347752  0.04680347]\n",
      "[-0.02997409  0.01722769 -0.87969972 -0.26402218 -0.39398662]\n",
      "[-0.02118464  0.03154509 -0.88913531 -0.27976981 -0.36017132]\n",
      "[ 0.05637786  0.10197944 -0.89901595 -0.13561491 -0.39975074]\n",
      "[-0.09713713 -0.15832056 -0.8184386  -0.38617358 -0.38278871]\n",
      "[ 0.02575135 -0.06888397 -0.89901594 -0.35048711 -0.25203371]\n",
      "[ 0.18400438  0.42174198 -0.81329137  0.12380384 -0.33392491]\n",
      "[-0.11500203 -0.11540282 -0.84433258 -0.35946457 -0.36241474]\n",
      "[ 0.156026    0.31089953 -0.87180183  0.02318846 -0.34412387]\n",
      "[-0.220255    0.69502066  0.59196985 -0.24034431  0.24543903]\n",
      "[ 0.61057031  0.57089044 -0.33271812  0.2561781   0.35349602]\n",
      "[-0.03129355 -0.19617425 -0.82157407 -0.47128314 -0.25188217]\n",
      "[ 0.21043977  0.39352596 -0.83138119  0.09529302 -0.31713873]\n",
      "[-0.11746184 -0.0465875  -0.85295987 -0.30554335 -0.40389979]\n",
      "[-0.02148525 -0.12547814 -0.857153   -0.43758633 -0.24000117]\n",
      "[-0.0798503  -0.26463445 -0.77410553 -0.51384619 -0.24559166]\n",
      "[ 0.03680822 -0.14505295 -0.87227105 -0.41239404 -0.2160536 ]\n",
      "[-0.22289033 -0.30726095 -0.63403284 -0.58257554 -0.33840613]\n",
      "[ 0.07979645 -0.13356188 -0.88636709 -0.40653811 -0.15771461]\n",
      "[ 0.23272124  0.36088736  0.8626536  -0.26577384  0.02818099]\n"
     ]
    }
   ],
   "source": [
    "# interact agent with the environment\n",
    "# only 6 lines are needed for RL interaction\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.sample_action_online(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[ 0.00987027 -0.44329176 -0.36035872 -0.77317723  0.27519731]\n"
     ]
    }
   ],
   "source": [
    "# state \n",
    "print(obs.shape)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's visualize the transition of reward\n",
    "def visualize_interaction_on_environment(env, agent):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    reward_list = []\n",
    "\n",
    "    while not done:\n",
    "        action = agent.sample_action_online(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(reward_list[:-1], label='reward', color='tab:orange')\n",
    "    ax1.set_xlabel('timestep')\n",
    "    ax1.set_ylabel('reward')\n",
    "    ax1.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_interaction_on_environment(env, agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standardized Environment\n",
    "\n",
    "We provide standardize recommender environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SyntheticEnv(random_state=12345)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Customize Environmental Configuration\n",
    "Now, we customize the recommender simulation environment.\n",
    "\n",
    "We have the following environmental configurations:\n",
    "- `StateTransition`: State transition of the synthetic simulation.\n",
    "- `RewardFunction`: Reward function of the synthetic simulation.\n",
    "- `state_dim`: Dimensions of state.\n",
    "- `action_type`: action type (i.e., continuous / discrete).\n",
    "- `n_actions`: Number of actions. Applicable only when reward_type is \"discrete\".\n",
    "- `action_context_dim`: Dimensions of the action context.\n",
    "- `action_context`: Feature vectors that characterizes each action.\n",
    "- `reward_type`: Reward type (i.e., continuous / binary).\n",
    "- `reward_std`: Standard deviation of the reward distribution. Applicable only when reward_type is \"continuous\".\n",
    "- `obs_std`: Standard deviation of the observation distribution.\n",
    "- `step_per_episode`: Number of timesteps in an episode.\n",
    "- `random_state` : Random state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1. Customizing the Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m SyntheticEnv(\n\u001b[1;32m      2\u001b[0m         StateTransition \u001b[39m=\u001b[39m StateTransition,\n\u001b[1;32m      3\u001b[0m         RewardFunction \u001b[39m=\u001b[39m RewardFunction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         random_state \u001b[39m=\u001b[39m \u001b[39m12345\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m visualize_interaction_on_environment(env, agent)\n",
      "Cell \u001b[0;32mIn [68], line 9\u001b[0m, in \u001b[0;36mvisualize_interaction_on_environment\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m      8\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39msample_action_online(obs)\n\u001b[0;32m----> 9\u001b[0m     obs, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     10\u001b[0m     reward_list\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     12\u001b[0m \u001b[39m# plot\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ofrl/examples/quickstart/../../syntheticgym/envs/synthetic.py:184\u001b[0m, in \u001b[0;36mSyntheticEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"Simulate a recommender interaction with a user.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39mNote\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# 1. sample reward for the given item.\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_function\u001b[39m.\u001b[39;49msample(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, action)\n\u001b[1;32m    186\u001b[0m \u001b[39m# 2. update user state with state_transition\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_transition\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, action)\n",
      "File \u001b[0;32m~/dev/ofrl/examples/quickstart/../../syntheticgym/envs/simulator/function.py:172\u001b[0m, in \u001b[0;36mRewardFunction.sample\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39m\"\"\"Reward function. inner product of state and recommended item_feature\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcontinuous\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[39m# print('action=', action)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m     \u001b[39m# reward = self.action_coef.T @ action\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_coef\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m state \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_coef\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m action \u001b[39m+\u001b[39m state\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_action_coef \u001b[39m@\u001b[39m action)\n\u001b[1;32m    174\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscrete\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    175\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_coef\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m state \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_coef\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_context[action] \u001b[39m+\u001b[39m state\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_action_coef \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_context[action])\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 10)"
     ]
    }
   ],
   "source": [
    "env = SyntheticEnv(\n",
    "        StateTransition = StateTransition,\n",
    "        RewardFunction = RewardFunction,\n",
    "        state_dim = 5,\n",
    "        n_actions = 10,\n",
    "        action_context_dim = 10,\n",
    "        action_context = None,\n",
    "        reward_type = \"continuous\",  # \"binary\"\n",
    "        reward_std = 0.0,\n",
    "        obs_std = 0.0,\n",
    "        step_per_episode = 10,\n",
    "        random_state = 12345,\n",
    ")\n",
    "visualize_interaction_on_environment(env, agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2. Defining the action_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (729911845.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [18], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    action_context =\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# we use the following items for recommendation\n",
    "action_context = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user96</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user97</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user98</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user99</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user100</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature1  feature2  feature3  feature4  feature5\n",
       "user1          -1        -1        -1         1         1\n",
       "user2          -1        -1         1         1        -1\n",
       "user3          -1        -1         1         1        -1\n",
       "user4           1         1         1         1         1\n",
       "user5          -1        -1         1         1         1\n",
       "...           ...       ...       ...       ...       ...\n",
       "user96         -1        -1        -1        -1         1\n",
       "user97          1        -1         1         1        -1\n",
       "user98          1        -1         1        -1        -1\n",
       "user99         -1         1        -1         1        -1\n",
       "user100         1        -1        -1         1         1\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we assume that the following 100 users \n",
    "user_features = np.sign(random_.normal(size=(100, 5))).astype(int)\n",
    "user_names, feature_names = [f\"user{i+1}\" for i in range(100)], [f\"feature{i+1}\" for i in range(5)]\n",
    "user_df = pd.DataFrame(user_features, columns=feature_names, index=user_names)\n",
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UserModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# using the above data, we can simulate an recommendation as follows\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env \u001b[39m=\u001b[39m SyntheticEnv(\n\u001b[0;32m----> 3\u001b[0m         UserModel \u001b[39m=\u001b[39m UserModel,\n\u001b[1;32m      4\u001b[0m         n_items \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,  \u001b[39m# we use 100 items\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         n_users \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,  \u001b[39m# 100 users exists\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         item_feature_dim \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,  \u001b[39m#each item has 5 dimensional features\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         user_feature_dim \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,  \u001b[39m#each user has 5 dimensional features\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         item_feature_vector \u001b[39m=\u001b[39m item_features, \u001b[39m# use item features defined above\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         user_feature_vector \u001b[39m=\u001b[39m user_features, \u001b[39m# use user features defined above\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         reward_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcontinuous\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m#we use continuous reward\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         reward_std \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m,\n\u001b[1;32m     12\u001b[0m         obs_std \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m#not add noise to the observation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         step_per_episode \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[1;32m     14\u001b[0m         random_state \u001b[39m=\u001b[39m \u001b[39m12345\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m visualize_interaction_on_environment(env, agent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UserModel' is not defined"
     ]
    }
   ],
   "source": [
    "# using the above data, we can simulate an recommendation as follows\n",
    "env = SyntheticEnv(\n",
    "        UserModel = UserModel,\n",
    "        n_items = 100,  # we use 100 items\n",
    "        n_users = 100,  # 100 users exists\n",
    "        item_feature_dim = 5,  #each item has 5 dimensional features\n",
    "        user_feature_dim = 5,  #each user has 5 dimensional features\n",
    "        item_feature_vector = item_features, # use item features defined above\n",
    "        user_feature_vector = user_features, # use user features defined above\n",
    "        reward_type = \"continuous\", #we use continuous reward\n",
    "        reward_std = 0.0,\n",
    "        obs_std = 0.0, #not add noise to the observation\n",
    "        step_per_episode = 10,\n",
    "        random_state = 12345,\n",
    ")\n",
    "visualize_interaction_on_environment(env, agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3. Using Customized UserModel(reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic import BaseUserModel\n",
    "from synthetic import Action\n",
    "\n",
    "@dataclass\n",
    "class UserModel(BaseUserModel):\n",
    "    reward_type: str = \"continuous\"  # \"binary\"\n",
    "    reward_std: float = 0.0\n",
    "    item_feature_vector: Optional[np.ndarray] = None,\n",
    "    random_state: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        check_scalar(\n",
    "            self.reward_std,\n",
    "            name=\"reward_std\",\n",
    "            target_type=float,\n",
    "        )\n",
    "\n",
    "        if self.reward_type not in [\"continuous\", \"binary\"]:\n",
    "            raise ValueError(\n",
    "                f'reward_type must be either \"continuous\" or \"binary\", but {self.reward_type} is given'\n",
    "            )\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "\n",
    "    def user_preference_dynamics(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: Action,\n",
    "        alpha: float = 1.0,\n",
    "    )-> np.ndarray:\n",
    "        \n",
    "        check_scalar(\n",
    "            state,\n",
    "            name=\"state\",\n",
    "            target_type=np.ndarray,\n",
    "        )\n",
    "        check_scalar(\n",
    "            action,\n",
    "            name=\"action\",\n",
    "            target_type=Action,\n",
    "        )\n",
    "\n",
    "        state = (state + alpha * state @ self.item_feature_vector[action] * self.item_feature_vector[action])\n",
    "        state = state / np.linalg.norm(state, ord=2)\n",
    "        return state\n",
    "\n",
    "    def reward_function(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: Action,\n",
    "    )-> float:\n",
    "        reward = self.cos_similar_function(state, action)\n",
    "\n",
    "        if self.reward_type is \"continuous\":\n",
    "            reward = reward + self.random_.normal(loc=0.0, scale=self.reward_std)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def cos_similar_function(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: Action,\n",
    "    )-> float:\n",
    "        inner = state @ self.item_feature_vector[action]\n",
    "        reward = inner / (np.linalg.norm(state, ord=2) * np.linalg.norm(self.item_feature_vector[action], ord=2))\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "env = SyntheticEnv(\n",
    "    UserModel = UserModel,\n",
    "    reward_type = \"continuous\",\n",
    "    random_state=12345\n",
    ")\n",
    "visualize_interaction_on_environment(env, agent)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4. Using Customized UserModel(user_preference_dynamics and reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic import BaseUserModel\n",
    "from synthetic import Action\n",
    "\n",
    "@dataclass\n",
    "class UserModel(BaseUserModel):\n",
    "    reward_type: str = \"continuous\"  # \"binary\"\n",
    "    reward_std: float = 0.0\n",
    "    item_feature_vector: Optional[np.ndarray] = None,\n",
    "    random_state: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        check_scalar(\n",
    "            self.reward_std,\n",
    "            name=\"reward_std\",\n",
    "            target_type=float,\n",
    "        )\n",
    "\n",
    "        if self.reward_type not in [\"continuous\", \"binary\"]:\n",
    "            raise ValueError(\n",
    "                f'reward_type must be either \"continuous\" or \"binary\", but {self.reward_type} is given'\n",
    "            )\n",
    "\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "\n",
    "    def user_preference_dynamics(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: Action,\n",
    "        alpha: float = 1.0,\n",
    "    )-> np.ndarray:\n",
    "        \n",
    "        check_scalar(\n",
    "            state,\n",
    "            name=\"state\",\n",
    "            target_type=np.ndarray,\n",
    "        )\n",
    "        check_scalar(\n",
    "            action,\n",
    "            name=\"action\",\n",
    "            target_type=Action,\n",
    "        )\n",
    "\n",
    "        if self.reward == 1:\n",
    "            state = (state + alpha * state @ self.item_feature_vector[action] * self.item_feature_vector[action])\n",
    "            state = state / np.linalg.norm(state, ord=2)\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def reward_function(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: Action,\n",
    "    )-> float:\n",
    "        self.reward = self.random_.binomial(n=1, p=self.sigmoid(self.cos_similar_function(state, action)))\n",
    "\n",
    "        if self.reward_type is \"continuous\":\n",
    "            reward = reward + self.random_.normal(loc=0.0, scale=self.reward_std)\n",
    "\n",
    "        return self.reward\n",
    "\n",
    "    def cos_similar_function(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: Action,\n",
    "    )-> float:\n",
    "        inner = state @ self.item_feature_vector[action]\n",
    "        reward = inner / (np.linalg.norm(state, ord=2) * np.linalg.norm(self.item_feature_vector[action], ord=2))\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def sigmoid(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    # Sigmoid function\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "env = SyntheticEnv(\n",
    "    UserModel = UserModel,\n",
    "    reward_type = \"binary\",\n",
    "    random_state=12345\n",
    ")\n",
    "visualize_interaction_on_environment(env, agent)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customize Bidding Setup in RTB Env\n",
    "Here, we describe how the decision makers can customize their own RTB environment.  \n",
    "Specifically, they can setup their own action space and bid price calculation rules by defining the following modules.\n",
    "- `reward_predictor` in Bidder class  \n",
    "    We use predicted rewards to calculate the bid price.  \n",
    "        bid price = adjust rate * predicted reward ( * constant)\n",
    "    If None, we use the ground-truth reward instead of the predicted reward.  \n",
    "\n",
    "- `scaler` in Bidder class\n",
    "    Scaler defines constant in the bid price calculation.  \n",
    "        bid price = adjust rate * predicted reward ( * constant)\n",
    "        constant = scaler * standard_bid_price\n",
    "    where standard_bid_price indicates the average of standard_bid_price  \n",
    "    (bid price which has approximately 50% impression probability) over all ads.\n",
    "\n",
    "- `action_space` for agent  \n",
    "    We transform continual adjust rate space $[0, \\infty)$ into agent action space $[0.1, 10]$.  \n",
    "    Both discrete and continuous actions are acceptable.  \n",
    "    (We can tune multiplication of adjust rate using scaler.)\n",
    "    \n",
    "The arguments are given as follows:\n",
    "- `original_env`: Original RTB Environment.\n",
    "- `reward_predictor`: A machine learning model to predict the reward to determine the bidding price.\n",
    "- `scaler`: Scaling factor (constant value) used for bid price determination. (`None` for the auto-fitting)\n",
    "- `action_min`: Minimum value of adjust rate.\n",
    "- `action_max`: Maximum value of adjust rate.\n",
    "- `action_type`: Action type of the RL agent, which should either be \"discrete\" or \"continuous\".\n",
    "- `n_actions`: Number of \"discrete\" actions.\n",
    "- `action_meaning`: Mapping function of agent action index to the actual \"discrete\" action to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "env = SyntheticEnv(random_state=random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1. Defining Continuous Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's customize the continuous action space\n",
    "env = SyntheticEnv(\n",
    "    original_env=env,\n",
    "    action_type=\"continuous\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random agent\n",
    "agent = DiscreteEpsilonGreedyHead(\n",
    "      base_policy = DiscreteRandomPolicy(),\n",
    "      name = 'random',\n",
    "      n_actions = env.n_actions,\n",
    "      epsilon = 1. ,\n",
    "      random_state = random_state, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example\n",
    "custom_env = SyntheticEnv(\n",
    "    original_env=env,\n",
    "    action_min=0.1,\n",
    "    action_max=0.5,\n",
    "    action_type=\"continuous\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_env.action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2. Defining Discrete Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's customize the environment and discretize the action space\n",
    "custom_env = SyntheticEnv(\n",
    "    random_state=random_state,\n",
    "    action_type=\"discrete\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(100)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SyntheticEnv' object has no attribute 'action_meaning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [168], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(custom_env\u001b[39m.\u001b[39maction_space)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(custom_env\u001b[39m.\u001b[39;49maction_meaning)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SyntheticEnv' object has no attribute 'action_meaning'"
     ]
    }
   ],
   "source": [
    "print(custom_env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example\n",
    "custom_env = SyntheticEnv(\n",
    "    original_env=env,\n",
    "    action_type=\"discrete\",\n",
    "    n_actions=5,\n",
    "    action_meaning=np.arange(1, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_env.action_space)\n",
    "print(custom_env.action_meaning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Sarah Dean, Jamie Morgenstern. \\\n",
    "\"Preference Dynamics Under Personalized Recommendations.\", 2022.\n",
    "\n",
    "- Takuma Seno and Michita Imai. \\\n",
    "\"d3rlpy: An Offline Deep Reinforcement Library.\", 2021.\n",
    "\n",
    "- David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, Alexandros Karatzoglou. \\\n",
    "\"RecoGym: A Reinforcement Learning Environment for the Problem of Product Recommendation in Online Advertising.\" 2018.\n",
    "\n",
    "- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. \\\n",
    "\"OpenAI Gym.\", 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('3.10.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "70404ee114725fce8ed9e697d67827f8546c678889944e6d695790702cbfe1f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
