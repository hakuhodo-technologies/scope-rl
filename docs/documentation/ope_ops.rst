==========
Overview
==========

We describe the problem setup of Off-Policy Evaluation (OPE) and Selection (OPS).

.. _overview_ope:

Off-Policy Evaluation
~~~~~~~~~~
We consider a general reinforcement learning setup, which is formalized by Markov Decision Process (MDP) as :math:`\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, P_r, \gamma \rangle`.
:math:`\mathcal{S}` is the state space and :math:`\mathcal{A}` is the action space, which is either discrete or continuous.
Let :math:`\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{S})` is the state transition probability where :math:`\mathcal{T}(s' | s,a)` is the probability of observing state :math:`s'` after taking action :math:`a` given state :math:`s`.
:math:`P_r: \mathcal{S} \times \mathcal{A} \times \mathbb{R} \rightarrow [0,1]` is the probability distribution of the immediate reward.
Given :math:`P_r`, :math:`R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}` is the expected reward function where :math:`R(s,a) := \mathbb{E}_{r \sim P_r (r | s, a)}[r]` is the expected reward when taking action :math:`a` for state :math:`s`.
We also let :math:`\gamma \in (0,1]` be a discount factor. Finally, :math:`\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})` denotes a *policy* where :math:`\pi(a| s)` is the probability of taking action :math:`a` at a given state :math:`s`.
Note that we also denote :math:`d_0` as the initial state distribution.

.. raw:: html

    <div class="white-space-20px"></div>

.. card:: Off-Policy Evaluation and Selection (OPE/OPS) in the process of offline RL
    :img-top: ../_static/images/scope_workflow.png
    :text-align: center

.. raw:: html

    <div class="white-space-20px"></div>

In OPE/OPS, we are given a logged dataset :math:`\mathcal{D}` consisting of :math:`n` trajectories, each of which is generated by a behavior policy :math:`\pi_b` as follows.

.. math::

    \tau := \{ (s_t, a_t, s_{t+1}, r_t) \}_{t=0}^{T} \sim p(s_0) \prod_{t=0}^{T} \pi_b(a_t | s_t) \mathcal{T}(s_{t+1} | s_t, a_t) P_r (r_t | s_t, a_t)

Our goal is to leverage this the logged dataset to accurately evaluate the performance of evaluation policies (OPE) and to select the best candidate policies based on OPE result (i.e., Off-Policy Selection; OPS).


.. _overview_basic_ope:

Policy Value Estimation
----------

In the basic OPE, we aim at evaluating the *policy value* or the expected trajectory-wise reward of the given evaluation policy :math:`\pi`:

.. math::

    J(\pi) := \mathbb{E}_{\tau} \left [ \sum_{t=0}^{T-1} \gamma^t r_{t} \mid \pi \right ],

Estimating the policy value before deploying policy in online environment is beneficial, as we can reduce the deployment cost and risks in online evaluation.
However, the challenge is that we need to answer a counterfactual question, *''What if a new policy chooses a different action from that of behavior policy?''*
by dealing with the distribution shift between :math:`\pi_b` and :math:`\pi`.

We discuss the properties of various OPE estimators together with their implementaion details in :doc:`Supported OPE estimators <evaluation_implementation>`.

.. seealso::

    * :ref:`Supported OPE estimators <implementation_basic_ope>` and :doc:`their API reference <_autosummary/scope_rl.ope.basic_estimators_discrete>` 
    * (advanced) :ref:`Marginal OPE estimators <implementation_marginal_ope>` and :doc:`their API reference <_autosummary/scope_rl.ope.marginal_estimators_discrete>`
    * :doc:`Quickstart <quickstart>` and :doc:`related example codes </documentation/examples/basic_ope>`

.. _overview_cumulative_distribution_ope:

Cumulative Distribution and Risk Function Estimation
----------

In practical situation, we are sometimes more interested in risk functions such as conditional value at risk and quartile range rather than the expectation of the trajectory-wise reward.
To derive these risk functions, we first estimate the following cumulative distribution function.

.. math::

    F(m, \pi) := \mathbb{E} \left[ \mathbb{I} \left \{ \sum_{t=0}^{T-1} \gamma^t r_t \leq m \right \} \mid \pi \right]

Then, we can derive various risk functions based on :math:`F(\cdot)` as follows.

* Mean: :math:`\mu(F) := \int_{G} G \, \mathrm{d}F(G)`
* Variance: :math:`\sigma^2(F) := \int_{G} (G - \mu(F))^2 \, \mathrm{d}F(G)`
* :math:`\alpha`-quartile: :math:`Q^{\alpha}(F) := \min \{ G \mid F(G) \leq \alpha \}`
* Conditional Value at Risk (CVaR): :math:`\int_{G} G \, \mathbb{I}\{ G \leq Q^{\alpha}(F) \} \, \mathrm{d}F(G)`

where we let :math:`G := \sum_{t=0}^{T-1} \gamma^t r_t` to represent the random variable of trajectory wise reward
and :math:`dF(G) := \mathrm{lim}_{\Delta \rightarrow 0} F(G) - F(G- \Delta)`.

We also discuss the properties of various cumulative distribution OPE estimators together with their implementaion details in :doc:`Supported OPE estimators <evaluation_implementation>`.

.. seealso::

    * :ref:`Supported OPE estimators <implementation_cumulative_distribution_ope>` and :doc:`their API reference <_autosummary/scope_rl.ope.cumulative_distribution_estimators_discrete>` 
    * :doc:`Quickstart <quickstart>` and :doc:`related example codes </documentation/examples/cumulative_dist_ope>`

.. _overview_ops:

Off-Policy Selection
~~~~~~~~~~

Finally, OPS aims to select the best policy among several candidates as follows.

.. math::

    \hat{\pi} := {\arg \max}_{\pi \in \Pi} \hat{J}(\pi)

where the :math:`\hat{J}(\cdot)` is the OPE estimate of the policy value, which can be substituted by some other metrics including CVaR.

In OPS, how well the ranking of the candidate policy preserves and the safety of the chosen policy matters as well as the accuracy of OPE.
In the next page, we provide a review of conventional evaluation metrics of OPE/OPS and describe the risk-return tradeoff metrics of top-:math:`k` policy selection.
We also feature SharpRatio@k, which is the main contribution of our research paper "Risk-Return Assessments of Off-Policy Evaluation in Offline" in :doc:`this page <sharpe_ratio>`. 

.. seealso::

    * :doc:`Conventional OPS metrics and SharpRatio@k <sharpe_ratio>`
    * :ref:`OPS evaluation protocols <implementation_eval_ope_ops>` and :doc:`their API reference <_autosummary/scope_rl.ope.ops>` 
    * :doc:`Quickstart <quickstart>` and :doc:`related example codes </documentation/examples/assessments>`

.. seealso::

    For further theoretical properties of OPE estimators, we refer readers to a survey paper :cite:`uehara2022review`.
    `awesome-offline-rl <https://github.com/hanjuku-kaso/awesome-offline-rl>`_ also provides a comprehensive list of literature.

.. seealso::

    :doc:`Overview (online/offline RL) <online_offline_rl>` describes the problem setting of the policy learning (offline RL) part.

.. raw:: html

    <div class="white-space-5px"></div>

.. grid::
    :margin: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: online_offline_rl
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Offline RL**

    .. grid-item::
        :columns: 6
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 3
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: sharpe_ratio
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **SharpRatio metrics**

            .. grid-item-card::
                :link: evaluation_implementation
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Supported Implementation**
