Tutorial
==========

.. _overview_ope:

Off-Policy Evaluation
~~~~~~~~~~
We consider a general reinforcement learning setup, which is formalized by Markov Decision Process (MDP) as :math:`\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, P_r, \gamma \rangle`.
:math:`\mathcal{S}` is the state space and :math:`\mathcal{A}` is the action space, which is either discrete or continuous.
Let :math:`\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{S})` is the state transition probability where :math:`\mathcal{T}(s' | s,a)` is the probability of observing state :math:`s'` after taking action :math:`a` given state :math:`s`.
:math:`P_r: \mathcal{S} \times \mathcal{A} \times \mathbb{R} \rightarrow [0,1]` is the probability distribution of the immediate reward.
Given :math:`P_r`, :math:`R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}` is the expected reward function where :math:`R(s,a) := \mathbb{E}_{r \sim P_r (r | s, a)}[r]` is the expected reward when taking action :math:`a` for state :math:`s`.
We also let :math:`\gamma \in (0,1]` be a discount factor. Finally, :math:`\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})` denotes a *policy* where :math:`\pi(a| s)` is the probability of taking action :math:`a` at a given state :math:`s`.
Note that we also denote :math:`d_0` as the initial state distribution.

In OPE, we are given a logged dataset :math:`\mathcal{D}` consisting of :math:`n` trajectories, each of which is generated by a behavior policy :math:`\pi_0` as follows.

.. math::

    \tau := \{ (s_t, a_t, s_{t+1}, r_t) \}_{t=0}^{T} \sim p(s_0) \prod_{t=0}^{T} \pi_0(a_t | s_t) \mathcal{T}(s_{t+1} | s_t, a_t) P_r (r_t | s_t, a_t)

Our goal is to leverage this the logged dataset to accurately evaluate the performance of evaluation policies (OPE)

.. _overview_basic_ope:

Policy Value Estimation
----------

In the basic OPE, we aim at evaluating the *policy value* or the expected trajectory-wise reward of the given evaluation policy :math:`\pi`:

.. math::

    J(\pi) := \mathbb{E}_{\tau} \left [ \sum_{t=0}^{T-1} \gamma^t r_{t} \mid \pi \right ],

Estimating the policy value before deploying policy in online environment is beneficial, as we can reduce the deployment cost and risks in online evaluation.
However, the challenge is that we need to answer a counterfactual question, *''What if a new policy chooses a different action from that of behavior policy?''*
by dealing with the distribution shift between :math:`\pi_0` and :math:`\pi`.

.. seealso::

    * :doc:`Supported OPE estimators <evaluation_implementation>` and :doc:`their API reference <_autosummary/ofrl.ope.basic_estimators_discrete>` 
    * (advanced) :doc:`Marginal OPE estimators <evaluation_implementation>`, and their :doc:`API reference <_autosummary/ofrl.ope.marginal_ope_discrete>`
    * :doc:`Quickstart <quickstart>` and :doc:`related tutorials <_autogallery/basic_ope/index>`


Standard OPE estimators
----------

.. _implementation_dm:

Direct Method (DM)
----------
DM :cite:`beygelzimer2009offset` is a model-based approach which uses the initial state value (estimated by e.g., Fitted Q Evaluation (FQE) :cite:`le2019batch`).
It first learns the Q-function and then leverages the learned Q-function as follows.

.. math::

    \hat{J}_{\mathrm{DM}} (\pi; \mathcal{D}) := \mathbb{E}_n [ \mathbb{E}_{a_0 \sim \pi(a_0 | s_0)} [\hat{Q}(s_0, a_0)] ] = \mathbb{E}_n [\hat{V}(s_0)],

where :math:`\mathcal{D}=\{\{(s_t, a_t, r_t)\}_{t=0}^T\}_{i=1}^n` is the logged dataset with :math:`n` trajectories of data.
:math:`T` indicates step per episode. :math:`\hat{Q}(s_t, a_t)` is the estimated state-action value and :math:`\hat{V}(s_t)` is the estimated state value.
DM has low variance, but can incur bias due to approximation errors.

    * :class:`DiscreteDirectMethod`
    * :class:`ContinuousDirectMethod`


.. _implementation_tis:

Trajectory-wise Importance Sampling (TIS)
----------

TIS :cite:`precup2000eligibility` uses importance sampling technique to correct the distribution shift between :math:`\pi` and :math:`\pi_0` as follows.

.. math::

    \hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right],

where :math:`w_{0:T-1} := \prod_{t=0}^{T-1} (\pi(a_t | s_t) / \pi_0(a_t | s_t))` is the trajectory-wise importance weight.

TIS enables an unbiased estimation of the policy value. 

Unbiased Estimator

.. math::

    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})] = J

.. dropdown:: proof

    .. math::

        &\mathbb{E}_{\tau}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]\\
        &=\mathbb{E}_{\tau \sim q} \left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right] \\
        &= \mathbb{E}_{\tau \sim q}\left[\frac{\pi(a_1|s_1)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{T-1}|s_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau \sim q}\left[\frac{p(s_0)\pi(a_1|s_1)P_r(r_1|s_t, a_t)\mathcal{T}(s_{t+1}|s_t, a_t)\cdots \pi(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})}
        {p(s_0)\pi_0(a_1|s_1)P_r(r_1|s_t, a_t)\mathcal{T}(s_{t+1}|s_t, a_t)\cdots \pi_0(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim q}\left[\frac{p(\tau)}{q(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim p}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=J

However, when the trajectory length :math:`T` is large, TIS suffers from high variance
due to the product of importance weights.


Variance Analysis

.. math::

    \mathbb{V}_{\tau}[\hat{J}_{\mathrm{TIS}}(\pi; \mathcal{D})] = \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]+\mathbb{V}_{s}\left[ w_{1:T-1} V(s)\right]

where :math:`w_{t} := \pi_e(a_{t'} | s_{t'}) / \pi_b(a_{t'} | s_{t'})`

.. dropdown:: proof

    .. math::

        &\mathbb{V}_{\tau}[\hat{J}_{\mathrm{TIS}}(\pi; \mathcal{D})]\\
        &=\mathbb{V}_{\tau} \left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right]\\
        &=\mathbb{E}_{s, a}\left[\mathbb{V}_{r}\left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right] \right ]+ \mathbb{V}_{s, a}\left[\mathbb{E}_{r}\left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right]\right]\\
        &= \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{V}_{s, a}\left[ w_{1:T-1} Q(s, a)\right]\\
        &= \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]+\mathbb{V}_{s}\left[\mathbb{E}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]\\
        &= \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]+\mathbb{V}_{s}\left[ w_{1:T-1} V(s)\right]\\

.

    * :class:`DiscreteTrajectoryWiseImportanceSampling`
    * :class:`ContinuousTrajectoryWiseImportanceSampling`


DM vs TIS Bias-Variance Trade-off 
^^^^^

.. .. image:: ./images/bias_tis.png
..     :scale: 45%
..     :align: center

.. |

.. .. image:: ./images/variance_tis.png
..     :scale: 45%
..     :align: center

.. |
.. .. card:: 
..     :width: 50%
..     :margin: auto
..     :img-top: ./images/bias_tis.png
..     :text-align: left
    
..     Variance with varying number of trajectories

.. .. card:: 
..     :width: 50%
..     :margin: auto
..     :img-top: ./images/variance_tis.png
..     :text-align: right
    
..     Variance with varying number of trajectories

.. grid:: 1 1 2 2

    .. grid-item-card:: 
        :img-top: ./images/bias_tis.png

        Bias with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/variance_tis.png

        Variance with varying number of trajectories


    .. .. grid-item-card:: 
    ..     :img-top: ./images/mse_tis.png

        MSE with varying number of trajectories


DM works well with small n_trajectries, TIS is getting better with large n_trajectries. 
DM(high bias, low variance) and TIS(low bias, high variance) are a trade-off between bias and variance.


DM vs TIS Curse of Horizon
^^^^^^
.. grid:: 1 1 2 2

    .. grid-item-card:: 
        :img-top: ./images/variance_step_per_trajectory.png

        Variance with varying number of step_per_trajectory

    .. grid-item-card:: 
        :img-top: ./images/mse_step_per_trajectory.png

        MSE with varying number of step_per_trajectory


TIS tends to have less bias than DM, and the bias decreases as the number of trajectories :math:`n` increases. On the other hand, variance of TIS tends to be larger than that of DM, and the larger the trajectory length :math:`T`, the larger the variance.

.. _implementation_pdis:

Per-Decision Importance Sampling (PDIS)
----------
PDIS :cite:`precup2000eligibility` leverages the sequential nature of the MDP to reduce the variance of TIS.
Specifically, since :math:`s_t` only depends on :math:`s_0, \ldots, s_{t-1}` and :math:`a_0, \ldots, a_{t-1}` and is independent of :math:`s_{t+1}, \ldots, s_{T}` and :math:`a_{t+1}, \ldots, a_{T}`,
PDIS only considers the importance weight of the past interactions when estimating :math:`r_t` as follows.

.. math::

    \hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[ \sum_{t=0}^{T-1} \gamma^t w_{0:t} r_t \right],

where :math:`w_{0:t} := \prod_{t'=0}^t (\pi_e(a_{t'} | s_{t'}) / \pi_b(a_{t'} | s_{t'}))` is the importance weight of past interactions.

Unbiased Estimator

.. math::

    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})] = J

.. dropdown:: proof

    .. math::

        \mathbb{E}_{\tau}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]
        &= \mathbb{E}_{\tau \sim q}\left[\sum_{t=0}^{T-1}\frac{\pi(a_1|s_1)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]\\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim q} \left[ \frac{\pi(a_1|s_1)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t  \right] \\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim q}\left[\frac{\pi(a_1|s_1)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]
        \mathbb{E}_{\tau \sim q}\left[\frac{\pi(a_{t+1}|s_{t+1})\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_{t+1}|s_{t+1})\cdots \pi_0(a_{T-1}|s_{T-1})}\right]\\
        &= \mathbb{E}_{\tau \sim q}\left[\sum_{t=0}^{T-1}\frac{\pi(a_1|s_1)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{T-1}|s_{T-1})} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})] \\
        &= J


Variance Analysis

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{H+1-t}(\pi; \mathcal{D})] = \mathbb{V}[J(s_t)] + \mathbb{E}_t[{w_t}^2\mathbb{V}_{t+1}[r_t]] + \mathbb{E}_t[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{PDIS}}^{H-t}(\pi; \mathcal{D})]] + \mathbb{E}_t[\mathbb{V}_t[w_tQ(s_t, a_t)]]

where :math:`w_{t} := \pi_e(a_{t'} | s_{t'}) / \pi_b(a_{t'} | s_{t'})`, 
:math:`\mathbb{E}_{t}:= \mathbb{E}[\cdot \mid s_0, a_0, r_0, ..., s_{t-1}, a_{t-1}, r_{t-1}]`

.. dropdown:: proof

    .. math::

        &\mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{H+1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{PDIS}}^{H+1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(w_t\left(r_t+\gamma J_{\mathrm{PDIS}}^{H-t} \right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t+\gamma J_{\mathrm{PDIS}}^{H-t}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(J_{\mathrm{PDIS}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(J_{\mathrm{PDIS}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(w_tQ(s_t, a_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(J_{\mathrm{PDIS}}^{H-t}(s_t, a_t)-\mathbb{E}_{t+1}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{V}_{a_t, r_t}\left[w_tQ(s_t, a_t) \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[{w_t}^2\mathbb{V}_{r_{t+1}}[r_t]\right] + \mathbb{E}_{s_t, a_t}\left[\gamma^2{w_t}^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{PDIS}}^{H-t}]\right] + \mathbb{V}_t[V(s_t)] 




:math:`\mathbb{V}[J(s_t)]`: state transition randomness

:math:`\mathbb{E}_t[{w_t}^2\mathbb{V}_{t+1}[r_t]]` : action stochasticity in :math:`\pi_b` randomness

:math:`\mathbb{E}_t[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{PDIS}}^{H-t}(\pi; \mathcal{D})]]`: variance from future steps

:math:`\mathbb{E}_t[\mathbb{V}_t[w_t]Q(s_t, a_t)]` : reward randomness

PDIS remains unbiased while reducing the variance of TIS. However, when trajectory length :math:`T` is large, PDIS still suffers from high variance.

    * :class:`DiscretePerDecisionImportanceSampling`
    * :class:`ContinuousPerDecisionWiseImportanceSampling`


TIS vs PDIS
^^^^^^
.. card:: 
    :width: 50%
    :margin: auto
    :img-top: 
    :text-align: center
    


.. card:: 
    :width: 50%
    :margin: auto
    :img-top: 
    :text-align: center
    


.. grid:: 1 1 2 2

    .. grid-item-card:: 
        :img-top: ./images/variance_pdis.png

        Variance with varying number of step_per_trajectory

    .. grid-item-card:: 
        :img-top: ./images/mse_pdis.png

        MSE with varying number of step_per_trajectory


The PDIS has less variance than the TIS. When the number of trajectories:math:`n` is large, it still suffers from variance.


.. _implementation_dr:

Doubly Robust (DR)
----------
DR :cite:`jiang2016doubly` :cite:`thomas2016data` is a hybrid of model-based estimation and importance sampling.
It introduces :math:`\hat{Q}` as a baseline estimation in the recursive form of PDIS and applies importance weighting only on its residual.

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)])\right],

Unbiased Estimator

.. math::

    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})] = J

.. dropdown:: proof

    .. math::

        &\mathbb{E}_{\tau \sim p}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})]\\
        &= \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t \left (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right)\right]\\
        &= \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t} r_t \right ] - \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t)) \right] + \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right]\\
        &= \mathbb{E}_{\tau \sim p}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t)) \right] + \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi_0(a | s_t)}\left[\frac{\pi(a \mid s_t)}{\pi_0(a \mid s_t)}\hat{Q}(s_t, a)\right]\right]\\
        &= \mathbb{E}_{\tau \sim p}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t)) \right] + \mathbb{E}_{\tau \sim p} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t)) \right] \\
        &= J


Variance Analysis

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{H+1-t}(\pi; \mathcal{D})] = \mathbb{V}[J(s_t)] + \mathbb{E}_t\left[{w_t}^2\mathbb{V}_{t+1}[r_t]\right] + \mathbb{E}_t\left[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{DR}}^{H-t}(\pi; \mathcal{D})]\right] + \mathbb{E}_t\left[\mathbb{V}_t[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t))]\right]

.. dropdown:: proof

    .. math::

        &\mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{H+1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{DR}}^{H+1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(\hat{V}(s_t)+w_t\left(r_t+\gamma J_{\mathrm{DR}}^{H-t} - \hat{Q}(s_t, a_t)\right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)-w_t\hat{Q}(s_t, a_t)+\hat{V}(s_t)+w_t\left(r_t+\gamma J_{\mathrm{DR}}^{H-t}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(J_{\mathrm{DR}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(J_{\mathrm{DR}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(-w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(J_{\mathrm{DR}}^{H-t}(s_t, a_t)-\mathbb{E}_{t+1}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ -w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{H-t}]\right]+ \mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{V}_{a_t, r_t}\left[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t)) \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[{w_t}^2\mathbb{V}_{r_{t+1}}[r_t]\right] + \mathbb{E}_{s_t, a_t}\left[\gamma^2{w_t}^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{H-t}]\right] + \mathbb{V}_t[V(s_t)] 


The first 3 terms are he same as PDIS, but 4th term differs from PDIS.

DR is unbiased and DR reduces the variance of PDIS when :math:`\hat{Q}(\cdot)` is reasonably accurate to satisfy :math:`0 < \hat{Q}(\cdot) < 2 Q(\cdot)`. 

However, when the importance weight is quite large, it may still suffer from a high variance.

    * :class:`DiscreteDoublyRobust`
    * :class:`ContinuousDoublyRobust`


DR vs PDIS
^^^^^^


.. image:: ./images/variance_dr.png
    :scale: 45%
    :align: center

|

The DR has less variance than the PDIS. When the number of trajectories:math:`n` is large, it still suffers from variance. 


Self-Normalized estimators
----------
Self-normalized estimators :cite:`kallus2019intrinsically` aim to reduce the scale of importance weight for the variance reduction purpose.
Specifically, it substitute importance weight :math:`w_{\ast}` as follows.

.. math::

    \tilde{w}_{\ast} := w_{\ast} / \mathbb{E}_{n}[w_{\ast}]

where :math:`\tilde{w}_{\ast}` is the self-normalized importance weight.

Self-normalized estimators are no longer unbiased, but has variance bounded by :math:`r_{max}^2` while also being consistent.


.. _implementation_sntis:

Self-normalized Trajectory-wise Importance Sampling (SNTIS)
----------
.. math::

    \hat{J}_{\mathrm{SNTIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t \frac{w_{1:T-1}}{\mathbb{E}_n[w_{1:T-1}]} r_t \right]   
.

    * :class:`DiscreteSelfNormalizedTrajectoryWiseImportanceSampling`
    * :class:`ContinuousSelfNormalizedTrajectoryWiseImportanceSampling`


.. _implementation_snpdis:

Self-normalized Per-Decision Importance Sampling (SNPDIS)
----------

.. math::

    \hat{J}_{\mathrm{SNPDIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[ \sum_{t=0}^{T-1} \gamma^t \frac{w_{0:t}}{\mathbb{E}_n[w_{0:t}]} r_t \right]
.

    * :class:`DiscreteSelfNormalizedPerDecisionImportanceSampling`
    * :class:`ContinuousSelfNormalizedPerDecisionImportanceSampling`



.. _implementation_sndr:

Self-normalized Doubly Robust (SNDR)
----------

.. math::

    \hat{J}_{\mathrm{SNDR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t \left(\frac{w_{0:t}}{\mathbb{E}_n[w_{0:t}]} (r_t - \hat{Q}(s_t, a_t)) + \frac{w_{0:t-1}}{\mathbb{E}_n[w_{0:t-1}]} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right)\right]
.

    * :class:`DiscreteSelfNormalizedDoublyRobust`
    * :class:`ContinuousSelfNormalizedDoublyRobust`


.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_sntis.png

        Bias with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/variance_sntis.png

        Variance with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/mse_sntis.png

        MSE with varying number of trajectories


.. _implementation_marginal_ope:

Marginalized Importance Sampling Estimators
----------
When the length of trajectory (:math:`T`) is large, even per-decision importance weights can exponentially large in the latter part of the trajectory.
To alleviate this, state marginal or state-action marginal importance weights can be used instead of the per-decision importance weight as follows :cite:`liu2018breaking` :cite:`uehara2020minimax`.

.. math::

    w_{s, a}(s, a) &:= d^{\pi}(s, a) / d^{\pi_0}(s, a) \\
    w_s(s) &:= d^{\pi}(s) / d^{\pi_0}(s)

Then, the importance weight is replaced as follows.

.. math::

    w(s_t, a_t) &= w_{s, a}(s_t, a_t) \\
    w(s_t, a_t) &= w_{s}(s_t) w_{t}(s_t, a_t)
    
    
where :math:`w_t(s_t, a_t) = \pi(a_t | s_t) / \pi_0(a_t | s_t)` is the immediate importance weight.

This estimator is particularly useful when policy visits the same or similar states among different trajectories or different timestep.
(e.g., when the state transition is something like :math:`\cdots \rightarrow s_1 \rightarrow s_2 \rightarrow s_1 \rightarrow s_2 \rightarrow \cdots` or when the trajectories always visits some particular state as :math:`\cdots \rightarrow s_{*} \rightarrow s_{1} \rightarrow s_{*} \rightarrow \cdots`)

We implement state marginal and state-action marginal OPE estimators in the following classes (both for :class:`Discrete-` and :class:`Continuous-` action spaces).

(State Marginal Estimators)

    * :class:`StateMarginalDirectMethod`
    * :class:`StateMarginalImportanceSampling`
    * :class:`StateMarginalDoublyRobust`
    * :class:`StateMarginalSelfNormalizedImportanceSampling`
    * :class:`StateMarginalSelfNormalizedDoublyRobust`

(State-Action Marginal Estimators)

    * :class:`StateActionMarginalImportanceSampling`
    * :class:`StateActionMarginalDoublyRobust`
    * :class:`StateActionMarginalSelfNormalizedImportanceSampling`
    * :class:`StateActionMarginalSelfNormalizedDoublyRobust`

.. _implementation_drl:

Double Reinforcement Learning (DRL)
----------
Comparing DR in the standard and marginal OPE, we notice that their formulation is slightly different as follows.

(DR in standard OPE)

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]) \right],

(DR in marginal OPE)

.. math::

    \hat{J}_{\mathrm{SAM-DR}} (\pi; \mathcal{D})
    &:= \mathbb{E}_{n} [\mathbb{E}_{a_0 \sim \pi(a_0 | s_0)} \hat{Q}(s_0, a_0)] \\
    & \quad \quad + \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{s, a}(s_t, a_t) (r_t + \gamma \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_{t+1}, a)] - \hat{Q}(s_t, a_t)) \right],

Then, a natural question arises, would it be possible to use marginal importance weight in DR in the standard formulation?

DRL :cite:`kallus2020double` leverages the marginal importance sampling in the standard OPE formulation as follows.

.. math::

    \hat{J}_{\mathrm{DRL}} (\pi; \mathcal{D})
    & := \frac{1}{n} \sum_{k=1}^K \sum_{i=1}^{n_k} \sum_{t=0}^{T-1} (w_s^j(s_{i,t}, a_{i, t}) (r_{i, t} - Q^j(s_{i, t}, a_{i, t})) \\
    & \quad \quad + w_s^j(s_{i, t-1}, a_{i, t-1}) \mathbb{E}_{a \sim \pi(a | s_t)}[Q^j(s_{i, t}, a)] )

DRL achieves the semiparametric efficiency with a consistent value predictor :math:`Q`. 
Therefore, to alleviate the potential bias introduced in :math:`Q`, DRL uses the "cross-fitting" technique to estimate the value function.
Specifically, let :math:`K` is the number of folds and :math:`\mathcal{D}_j` is the :math:`j`-th split of logged data consisting of :math:`n_k` samples.
Cross-fitting trains :math:`w^j` and :math:`Q^j` on the subset of data used for OPE, i.e., :math:`\mathcal{D} \setminus \mathcal{D}_j`.

    * :class:`DiscreteDoubleReinforcementLearning`
    * :class:`ContinuousDoubleReinforcementLearning`

.. _implementation_sope:

Spectrum of Off-Policy Estimators (SOPE)
----------
While state marginal or state-action marginal importance weight effectively alleviates the variance of per-decision importance weight, the estimation error of marginal importance weights
may introduce some bias in estimation. To alleviate this and control the bias-variance tradeoff more flexibly, SOPE uses the following interpolated importance weights :cite:`yuan2021sope`.

.. math::

    w(s_t, a_t) &= 
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        w_{s, a}(s_{t-k}, a_{t-k}) \prod_{t'=t-k+1}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases} \\
    w(s_t, a_t) &= 
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        w_{s}(s_{t-k}) \prod_{t'=t-k}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases}
    
where SOPE uses per-decision importance weight :math:`w_t(s_t, a_t) := \pi(a_t | s_t) / \pi_0(a_t | s_t)` for the :math:`k` most recent timesteps.
    
.. grid:: 1 1 2 2

    .. grid-item-card:: 
        :img-top: ./images/bias_samdr.png

        Bias with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/variance_samdr.png

        Variance with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/mse_samdr.png

        MSE with varying number of trajectories
    MSE with varying number of trajectories
More tutorials with a variety of environments and OPE estimators are available in the next page!

.. raw:: html

    <div class="white-space-5px"></div>

.. grid::

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: installation
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Quickstart**

    .. grid-item::
        :columns: 8
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: _autogallery/index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Tutorial**

            .. grid-item-card::
                :link: index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Documentation**

