Tutorial
==========

.. _overview_ope:

Off-Policy Evaluation
~~~~~~~~~~
Off-policy evaluation is a technique used in reinforcement learning to estimate the policy value based on data collected from a different policy. It is particularly useful when you want to evaluate the performance of candidate policies without actually executing it in the online environment.

Setup
----------
We consider a general reinforcement learning setup, which is formalized by Markov Decision Process (MDP) as :math:`\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, P_r, \gamma \rangle`.
:math:`\mathcal{S}` is the state space and :math:`\mathcal{A}` is the action space, which is either discrete or continuous.
Let :math:`\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{S})` is the state transition probability where :math:`\mathcal{T}(s' | s,a)` is the probability of observing state :math:`s'` after taking action :math:`a` given state :math:`s`.
:math:`P_r: \mathcal{S} \times \mathcal{A} \times \mathbb{R} \rightarrow [0,1]` is the probability distribution of the immediate reward.
Given :math:`P_r`, :math:`R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}` is the expected reward function where :math:`R(s,a) := \mathbb{E}_{r \sim P_r (r | s, a)}[r]` is the expected reward when taking action :math:`a` for state :math:`s`.
We also let :math:`\gamma \in (0,1]` be a discount factor. Finally, :math:`\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})` denotes a *policy* where :math:`\pi(a| s)` is the probability of taking action :math:`a` at a given state :math:`s`.
Note that we also denote :math:`d_0` as the initial state distribution.

In OPE, we are given a logged dataset :math:`\mathcal{D}` consisting of :math:`n` trajectories, each of which is generated by a behavior policy :math:`\pi_0` as follows.

.. math::

    \tau := \{ (s_t, a_t, s_{t+1}, r_t) \}_{t=0}^{T-1} \sim p(s_0) \prod_{t=0}^{T-1} \pi_0(a_t | s_t) \mathcal{T}(s_{t+1} | s_t, a_t) P_r (r_t | s_t, a_t)

    \mathcal{D} = \{\tau_i\}_{i=1}^n \sim p_{\pi_0}

We aim at evaluating the *policy value* or the expected trajectory-wise reward of the given evaluation policy :math:`\pi`:

.. math::

    J(\pi) := \mathbb{E}_{\tau} \left [ \sum_{t=0}^{T-1} \gamma^t r_{t} \mid \pi \right ],


We aim to develop an estimator :math:`\hat{J}` to estimate the value of an evaluation policy :math:`\pi` (which is different from :math:`\pi_0`) using only the logged data in :math:`\mathcal{D}`. The accuracy of :math:`\hat{J}` is quantified by mean squared error (MSE)

.. math::
    
    \begin{aligned}
        \operatorname{MSE}(\hat{J}(\pi; \mathcal{D})): & =\mathbb{E}_{\tau  \sim p_{\pi}}\left[(J(\pi)-\hat{J}(\pi ; \mathcal{D}))^2\right] \\
        & =\operatorname{Bias}(\hat{J}(\pi; \mathcal{D}))^2+\mathbb{V}_{\tau  \sim p_{\pi}}[\hat{J}(\pi ; \mathcal{D})]
    \end{aligned}

MSE can be decomposed into bias squared and variance. Let's introduce the property of OPE estimators in terms of bias and variance.

OPE estimators
----------

**We introduce OPE estimators and explain it both theoretically and through simple experiments.** 

- Direct Method (DM)
- Trajectory-wise Importance Sampling (TIS)
- Per-Decision Importance Sampling (PDIS)
- Doubly Robust (DR)
- Self-Normalized 
- Marginalized Importance Sampling (MIS)
- Spectrum of Off-Policy (SOPE)
- Double Reinforcement Learning (DRL)


Direct Method (DM)
----------

DM :cite:`beygelzimer2009offset` is a model-based approach that uses the initial state value (estimated by e.g., Fitted Q Evaluation (FQE) :cite:`le2019batch`).
It first learns the Q-function and then leverages the learned Q-function as follows.

.. math::

    \hat{J}_{\mathrm{DM}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_{0}^{(i)}) \hat{Q^{\pi}}(s_{0}^{(i)}, a) = \frac{1}{n} \sum_{i=1}^n \hat{V^{\pi}}(s_{0}^{(i)})
    

:math:`T` indicates step per episode. :math:`\hat{Q}(s_t, a_t) \simeq \mE_{\tau_{t:T-1}\sim p_{\pi}(\tau_{t:T-1}|s_t, a_t)}\left[\sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}\right]` is the estimated state-action value and :math:`\hat{V}(s_t) \simeq \mE_{\tau_{t:T-1} \sim p_{\pi}(\tau_{t:T-1}|s_t)}\left[\sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}\right]` is the estimated state value.
Now that you understand the definition of DM, the next step is to check an important property, bias of DM , by calculation.

.. math::

    \begin{align*}
            \operatorname{Bias}[\hat{J}_{\mathrm{DM}}(\pi;D)] & = J(\pi)-  \mathbb{E}_{s_0\sim P(d_0)}\left[\hat{V}^{\pi}(s_0)\right]\\
            & = \mathbb{E}_{s_0\sim P(d_0)}\left[\sum_{a\in\cal{A}}\pi(a | s_0)Q^{\pi}(s_0, a)\right]-  \mathbb{E}_{s_0\sim P(d_0)}\left[\sum_{a\in\cal{A}}\pi(a | s_0)\hat{Q}^{\pi}(s_0, a)\right]\\
            & = \mathbb{E}_{s_0\sim P(d_0)}\left[\sum_{a\in\cal{A}}\pi(a | s_0)\left(Q^{\pi}(s_0, a)- \hat{Q}^{\pi}(s_0, a)\right) \right]
    \end{align*}

DM has lower variance compared to other estimators but can produce large bias caused by approximation errors :math:`Q^{\pi}(s_0, a)- \hat{Q}^{\pi}(s_0, a)`.

.. _implementation_tis:

Trajectory-wise Importance Sampling (TIS)
----------
TIS :cite:`precup2000eligibility` uses a importance sampling technique to correct the distribution shift between :math:`\pi` and :math:`\pi_0` as follows.

.. math::

    \hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right]

where :math:`w_{0:T-1} := \prod_{t=0}^{T-1} (\pi(a_t | s_t) / \pi_0(a_t | s_t))` is the trajectory-wise importance weight. TIS is unbiased under the assumption of common support :math:`\forall(s_0, a_0, ..., s_{T-1}, a_{T-1}) \in S^{T-1} \times A^{T-1},  \prod_{t=0}^{T-1}\pi(a_t \mid s_t) > 0 \rightarrow \prod_{t=0}^{T-1}\pi_0(a_t \mid s_t) > 0`.

.. math::

    \mathbb{E}_{\tau \sim p_{\pi_0}(\tau)}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::

        &\mathbb{E}_{\tau}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]\\
        &=\mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1} \gamma^t w_{0:T-1} r_t \right] \\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_0|s_0)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{T-1}|s_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p(s_0)\pi(a_0|s_0)P_r(r_0|s_0, a_0)\mathcal{T}(s_{1}|s_0, a_0)\cdots \pi(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})}
        {p(s_0)\pi_0(a_0|s_0)P_r(r_0|s_0, a_0)\mathcal{T}(s_{1}|s_0, a_0)\cdots \pi_0(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=\sum_{\tau}p_{\pi_0}(\tau)\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\\
        &=\sum_{\tau}p_{\pi}(\tau)\sum_{t=0}^{T-1} \gamma^{t}r_t\\
        &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=J(\pi)


By the importance weight trick TIS enables an unbiased estimation of the policy value. After examining bias, we will next focus on another important property, variacne. To facilitate the derivation of variances, we will first express TIS recursively.

.. math::

    J_{\mathrm{TIS}}^{T-1-t} := w_t(w_{t+1:T-1} r_t + \gamma J_{\mathrm{TIS}}^{T-1-(t+1)})


.. dropdown:: proof

    .. math::

        J_{\mathrm{TIS}}^{T-1-t} &= \sum_{t' = t}^{T-1}\gamma^{t' -t}w_{t:T-1}r_{t'}\\
        &=w_{t:T}r_t + \sum_{t' = t+1}^{T-1}\gamma^{t' -t}w_{t:T-1}r_{t'}\\
        &=w_t\left(w_{t+1:T-1}r_t + \sum_{t' = t+1}^{T-1}\gamma^{t' -t}w_{t+1:T-1}r_{t'}\right)\\
        &=w_t\left(w_{t+1:T-1}r_t + \gamma\sum_{t' = t+1}^{T-1}\gamma^{t' -(t+1)}w_{t+1:T-1}r_{t'}\right)\\
        &=w_t\left(w_{t+1:T-1}r_t + \gamma J_{\mathrm{TIS}}^{T-1-(t+1)}\right)\\

The term $(T-1-t)$ in $J_{\mathrm{TIS}}^{T-1-t}$ represents the remaining trajectory length at time $t$, and when $t=T-1$, $J_{\mathrm{TIS}}^0 = 0$, and when $t=0$, $J_{\mathrm{TIS}}^{T-1} = J_{\mathrm{TIS}}$ holds true. 

.. math::

    \mathbb{V}_{\tau}[\hat{J}_{\mathrm{TIS}}(\pi; \mathcal{D})] = \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]+\mathbb{V}_{s}\left[ w_{1:T-1} V(s)\right]


.. dropdown:: proof

    .. math::

        &\mathbb{V}_{\tau}[\hat{J}_{\mathrm{TIS}}(\pi; \mathcal{D})]\\
        &=\mathbb{V}_{\tau} \left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right]\\
        &=\mathbb{E}_{s, a}\left[\mathbb{V}_{r}\left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right] \right ]+ \mathbb{V}_{s, a}\left[\mathbb{E}_{r}\left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right]\right]\\
        &= \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{V}_{s, a}\left[ w_{1:T-1} Q(s, a)\right]\\
        &= \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]+\mathbb{V}_{s}\left[\mathbb{E}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]\\
        &= \mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]+ \mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]+\mathbb{V}_{s}\left[ w_{1:T-1} V(s)\right]\\

The variance consists of three terms. The first term :math:`\mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]` includes the square of the trajectory-wise importance weight and the third term :math:`\mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]` includes the variance involving the trajectory-wise importance weights. Therefore, given a wide range of trajectory-wise importance weights, the variance is large.
In particular, when the trajectory length :math:`T` is large, TIS suffers from high variance due to the product of importance weights.

DM vs TIS Bias-Variance Trade-off 
^^^^^

.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_tis.png

        Bias with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/variance_tis.png

        Variance with varying number of trajectories


    .. grid-item-card:: 
        :img-top: ./images/mse_tis.png

        MSE with varying number of trajectories


DM works well with a small number of trajectories :math:`n`, TIS is getting better with a large number of trajectories. 
DM(high bias, low variance) and TIS(low bias, high variance) are a trade-off between bias and variance.


DM vs TIS Curse of Horizon
^^^^^^
.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_step_per_trajectory_tis.png

        Bias with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/variance_step_per_trajectory_tis.png

        Variance with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/mse_step_per_trajectory_tis.png

        MSE with varying the trajectory length


.. TIS tends to have less bias than DM, and the bias decreases as the trajectory length :math:`T` increases. 
TIS tends to have less bias than DM,
On the other hand, the variance of TIS tends to be larger than that of DM, and the larger the trajectory length :math:`T`, the larger the variance.

.. _implementation_pdis:

Per-Decision Importance Sampling (PDIS)
----------
PDIS :cite:`precup2000eligibility` leverages the sequential nature of the MDP to reduce the variance of TIS.
Specifically, since :math:`s_t` only depends on :math:`s_0, \ldots, s_{t-1}` and :math:`a_0, \ldots, a_{t-1}` and is independent of :math:`s_{t+1}, \ldots, s_{T}` and :math:`a_{t+1}, \ldots, a_{T}`,
PDIS only considers the importance weight of the past interactions when estimating :math:`r_t` as follows.

.. math::

    \hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[ \sum_{t=0}^{T-1} \gamma^t w_{0:t} r_t \right],

where :math:`w_{0:t} := \prod_{t'=0}^t (\pi(a_{t'} | s_{t'}) / \pi_0(a_{t'} | s_{t'}))` is the importance weight of past interactions.

PDIS is unbiased under 

.. math::

    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::

        \mathbb{E}_{\tau}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{\pi(a_1|s_1)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]\\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}} \left[ \frac{\pi(a_1|s_1)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t  \right] \\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_1|s_1)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]
        \underbrace{\mathbb{E}_{\pi_0(a_1|s_1)\cdots\pi_0(a_t|s_t)}\left[\sum_{a_{t+1}}\cdots\sum_{a_{T-1}}\pi(a_{t+1}|s_{t+1})\cdots\pi(a_{T-1}|s_{T-1})\right]}_{=1} \\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_1|s_1)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]
        \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_{t+1}|s_{t+1})\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_{t+1}|s_{t+1})\cdots \pi_0(a_{T-1}|s_{T-1})}\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{\pi(a_1|s_1)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_1|s_1)\cdots \pi_0(a_{T-1}|s_{T-1})} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=J(\pi)

Variance Analysis

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{H+1-t}(\pi; \mathcal{D})] = \mathbb{V}[J(s_t)] + \mathbb{E}_t[{w_t}^2\mathbb{V}_{t+1}[r_t]]+ \mathbb{E}_t[\mathbb{V}_t[w_tQ(s_t, a_t)]] + \mathbb{E}_t[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{PDIS}}^{H-t}(\pi; \mathcal{D})]] 

where :math:`w_{t} := \pi(a_{t'} | s_{t'}) / \pi_0(a_{t'} | s_{t'})`, 
:math:`\mathbb{E}_{t}:= \mathbb{E}_{s_t, a_t, r_t}[\cdot \mid s_0, a_0, r_0, ..., s_{t-1}, a_{t-1}, r_{t-1}]`

.. dropdown:: proof

    .. math::
        &\mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{H+1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{PDIS}}^{H+1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(w_t\left(r_t+\gamma \hat{J}_{\mathrm{PDIS}}^{H-t} \right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{PDIS}}^{H-t}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(w_tQ(s_t, a_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{PDIS}}^{H-t}-\mathbb{E}_{t+1}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{PDIS}}^{H-t}]\right]+ \mathbb{V}_t[V(s_t)]\\

PDIS variance is decomposed by the calculation into four terms. The first three terms correspond to variances resulting from distinct sources of randomness at time step :math:`t`: :math:`\mathbb{V}[J(s_t)]` randomness in state transitions, :math:`\mathbb{E}_t[{w_t}^2\mathbb{V}_{t+1}[r_t]]` action stochasticity in reward randomness :math:`\pi_0`, and :math:`\mathbb{E}_t[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{PDIS}}^{H-t}(\pi; \mathcal{D})]]` variance in rewards. The fourth is a term that includes variances from future steps. PDIS remains unbiased while reducing the variance of TIS. 


TIS vs PDIS
^^^^^^

.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_step_per_trajectory_pdis.png

        Bias with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/variance_step_per_trajectory_pdis.png

        Variance with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/mse_step_per_trajectory_pdis.png

        MSE with varying the trajectory length


The PDIS has less variance than the TIS. When the trajectory length :math:`T` is large, it still suffers from variance.


.. _implementation_dr:

Doubly Robust (DR)
----------
DR :cite:`jiang2016doubly` :cite:`thomas2016data` is a hybrid of model-based estimation and importance sampling.
It introduces :math:`\hat{Q}` as a baseline estimation in the recursive form of PDIS and applies importance weighting only on its residual.

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)])\right],

Unbiased Estimator

.. math::

    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::
        &\mathbb{E}_{\tau}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t \left (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right)\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t} r_t \right ] - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi_0(a | s_t)}\left[\frac{\pi(a \mid s_t)}{\pi_0(a \mid s_t)}\hat{Q}(s_t, a)\right]\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim { (s_{t'}, s_{t'+1}, r_{t'}) \}_{t'=0}^{T-1}}} \prod_{t' = 0}^{T-1}\mathbb{E}_{a \sim \pi_0(\cdot | s_{t'})}\left [\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi_0(a | s_t)}\left[\frac{\pi(a \mid s_t)}{\pi_0(a \mid s_t)}\hat{Q}(s_t, a)\right]\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim { (s_{t'}, s_{t'+1}, r_{t'}) \}_{t'=0}^{T-1}}} \prod_{t' = 0}^{T-1}\mathbb{E}_{a \sim \pi_0(\cdot | s_{t'})}\left [\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \frac{\pi(a_t \mid s_t)}{\pi_0(a_t \mid s_t)}\hat{Q}(s_t, a_t)\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t)) \right] \\
        &= J(\pi)

Variance Analysis

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{H+1-t}(\pi; \mathcal{D})] = \mathbb{V}[J(s_t)] + \mathbb{E}_t\left[{w_t}^2\mathbb{V}_{t+1}[r_t]\right] + \mathbb{E}_t\left[\mathbb{V}_t[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t))]\right] + \mathbb{E}_t\left[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{DR}}^{H-t}(\pi; \mathcal{D})]\right] 

.. dropdown:: proof

    .. math::
        &\mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{H+1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{DR}}^{H+1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(\hat{V}(s_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{DR}}^{H-t} - \hat{Q}(s_t, a_t)\right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)-w_t\hat{Q}(s_t, a_t)+\hat{V}(s_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{DR}}^{H-t}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{H-t} -\mathbb{E}_{t+1}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(-w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{DR}}^{H-t}-\mathbb{E}_{t+1}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ -w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{H-t}]\right]+ \mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{V}_{a_t, r_t}\left[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t)) \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[{w_t}^2\mathbb{V}_{r_{t+1}}[r_t]\right] + \mathbb{E}_{s_t, a_t}\left[\gamma^2{w_t}^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{H-t}]\right] + \mathbb{V}_t[V(s_t)] 

3 terms are the same as PDIS, but 3rd term :math:`\mathbb{E}_t\left[\mathbb{V}_t[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t))]\right]` differs from PDIS. DR reduces the variance of PDIS when :math:`\hat{Q}(\cdot)` is reasonably accurate to satisfy :math:`0 < \hat{Q}(\cdot) < 2 Q(\cdot)`. 

DR vs PDIS
^^^^^^

.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_dr.png

        Bias with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/variance_dr.png

        Variance with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/mse_dr.png

        MSE with varying number of trajectories


DR has less variance than PDIS consequently DR has a smaller mse than PDIS. However, DR also depends on importance weights such as PDIS, so when the trajectory length :math:`T` is large, DR can still incur high variance.


Self-Normalized estimators
----------
Self-normalized estimators :cite:`kallus2019intrinsically` aim to reduce the scale of importance weight for the variance reduction purpose.
Specifically, it substitutes importance weight :math:`w_{\ast}` as follows.

.. math::

    \tilde{w}_{\ast} := w_{\ast} / \mathbb{E}_{n}[w_{\ast}]

where :math:`\tilde{w}_{\ast}` is the self-normalized importance weight.

Self-normalized estimators are no longer unbiased but have variance bounded by :math:`r_{max}^2` while also being consistent.


.. _implementation_sntis:

Self-normalized Trajectory-wise Importance Sampling (SNTIS)
----------

.. math::

    \hat{J}_{\mathrm{SNTIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t \frac{w_{1:T-1}}{\mathbb{E}_n[w_{1:T-1}]} r_t \right]   
.

.. _implementation_snpdis:

Self-normalized Per-Decision Importance Sampling (SNPDIS)
----------
.. math::

    \hat{J}_{\mathrm{SNPDIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[ \sum_{t=0}^{T-1} \gamma^t \frac{w_{0:t}}{\mathbb{E}_n[w_{0:t}]} r_t \right]
.

.. _implementation_sndr:

Self-normalized Doubly Robust (SNDR)
----------
.. math::

    \hat{J}_{\mathrm{SNDR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t \left(\frac{w_{0:t}}{\mathbb{E}_n[w_{0:t}]} (r_t - \hat{Q}(s_t, a_t)) + \frac{w_{0:t-1}}{\mathbb{E}_n[w_{0:t-1}]} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right)\right]
.

.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_sntis.png

        Bias with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/variance_sntis.png

        Variance with varying number of trajectories

    .. grid-item-card:: 
        :img-top: ./images/mse_sntis.png

        MSE with varying number of trajectories

SNTIS is able to reduce the variance while keeping the bias much the same compared to TIS, resulting in a reduced MSE.


.. _implementation_marginal_ope:

Marginalized Importance Sampling Estimators
----------
(State Marginal Estimators)

(State-Action Marginal Estimators)



When the length of the trajectory :math:`T` is large, even per-decision importance weights can be exponentially large in the latter part of the trajectory.
To alleviate this, state marginal or state-action marginal importance weights can be used instead of the per-decision importance weight as follows :cite:`liu2018breaking` :cite:`uehara2020minimax`.

.. math::

    w_{s, a}(s, a) &:= d^{\pi}(s, a) / d^{\pi_0}(s, a) \\
    w_s(s) &:= d^{\pi}(s) / d^{\pi_0}(s)

Then, the importance weight is replaced as follows.

.. math::

    w(s_t, a_t) &= w_{s, a}(s_t, a_t) \\
    w(s_t, a_t) &= w_{s}(s_t) w_{t}(s_t, a_t)
    
    
where :math:`w_t(s_t, a_t) = \pi(a_t | s_t) / \pi_0(a_t | s_t)` is the immediate importance weight.

This estimator is particularly useful when policy visits the same or similar states among different trajectories or different timesteps.
(e.g., when the state transition is something like :math:`\cdots \rightarrow s_1 \rightarrow s_2 \rightarrow s_1 \rightarrow s_2 \rightarrow \cdots` or when the trajectories always visit some particular state as :math:`\cdots \rightarrow s_{*} \rightarrow s_{1} \rightarrow s_{*} \rightarrow \cdots`)

.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_samis.png

        Bias with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/variance_samis.png

        Variance with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/mse_samis.png

        MSE with varying the trajectory length

SAMIS requires estimating state-action marginal importance weights, which introduces a bias, but it can reduce variance more than PDIS.

.. _implementation_drl:

Double Reinforcement Learning (DRL)
----------
Comparing DR in the standard and marginal OPE, we notice that their formulation is slightly different as follows.

(DR in standard OPE)

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]) \right],

(DR in marginal OPE)

.. math::

    \hat{J}_{\mathrm{SAM-DR}} (\pi; \mathcal{D})
    &:= \mathbb{E}_{n} [\mathbb{E}_{a_0 \sim \pi(a_0 | s_0)} \hat{Q}(s_0, a_0)] \\
    & \quad \quad + \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{s, a}(s_t, a_t) (r_t + \gamma \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_{t+1}, a)] - \hat{Q}(s_t, a_t)) \right],

Then, a natural question arises, would it be possible to use marginal importance weight in DR in the standard formulation?

DRL :cite:`kallus2020double` leverages the marginal importance sampling in the standard OPE formulation as follows.

.. math::

    \hat{J}_{\mathrm{DRL}} (\pi; \mathcal{D})
    & := \frac{1}{n} \sum_{k=1}^K \sum_{i=1}^{n_k} \sum_{t=0}^{T-1} (w_s^j(s_{i,t}, a_{i, t}) (r_{i, t} - Q^j(s_{i, t}, a_{i, t})) \\
    & \quad \quad + w_s^j(s_{i, t-1}, a_{i, t-1}) \mathbb{E}_{a \sim \pi(a | s_t)}[Q^j(s_{i, t}, a)] )

DRL achieves the semiparametric efficiency with a consistent value predictor :math:`Q`. 
Therefore, to alleviate the potential bias introduced in :math:`Q`, DRL uses the "cross-fitting" technique to estimate the value function.
Specifically, let :math:`K` is the number of folds and :math:`\mathcal{D}_j` is the :math:`j`-th split of logged data consisting of :math:`n_k` samples.
Cross-fitting trains :math:`w^j` and :math:`Q^j` on the subset of data used for OPE, i.e., :math:`\mathcal{D} \setminus \mathcal{D}_j`.


.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_drl.png

        Bias with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/variance_drl.png

        Variance with varying the trajectory length

    .. grid-item-card:: 
        :img-top: ./images/mse_drl.png

        MSE with varying the trajectory length

DRL can suppress the variance even when the length of the trajectory is large by using marginal importance weight, theoretically satisfying efficiency and robustness. The better the estimation of the Q function of DRL, the smaller the variance.

.. _implementation_sope:

Spectrum of Off-Policy Estimators (SOPE)
----------
While state marginal or state-action marginal importance weight effectively alleviates the variance of per-decision importance weight, the estimation error of marginal importance weights
may introduce some bias in estimation. To alleviate this and control the bias-variance tradeoff more flexibly, SOPE uses the following interpolated importance weights :cite:`yuan2021sope`.

.. math::

    w(s_t, a_t) &= 
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        w_{s, a}(s_{t-k}, a_{t-k}) \prod_{t'=t-k+1}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases} \\
    w(s_t, a_t) &= 
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        w_{s}(s_{t-k}) \prod_{t'=t-k}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases}
    
where SOPE uses per-decision importance weight :math:`w_t(s_t, a_t) := \pi(a_t | s_t) / \pi_0(a_t | s_t)` for the :math:`k` most recent timesteps.
    
.. grid:: 1 1 2 3

    .. grid-item-card:: 
        :img-top: ./images/bias_sope.png

        Bias with varying number of n_step_pdis

    .. grid-item-card:: 
        :img-top: ./images/variance_sope.png

        Variance with varying number of n_step_pdis

    .. grid-item-card:: 
        :img-top: ./images/mse_sope.png

        MSE with varying number of n_step_pdis


SOPE can control the balance between marginal and per-decision estimators by changing n_step_pdis. As seen in the figure, SOPE is equal to SAMIS when n_step_pdis is 0 and is equal to pdis when n_step_pdis is the trajectory length :math:`T`. If n_step_pdis is large, bias can be reduced, if it is small, variance can be reduced. SOPE reduces MSE with less bias than SAMIS and less variance than PDIS. 


.. raw:: html

    <div class="white-space-5px"></div>

.. grid::

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: installation
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Quickstart**

    .. grid-item::
        :columns: 8
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: _autogallery/index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Tutorial**

            .. grid-item-card::
                :link: index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Documentation**

