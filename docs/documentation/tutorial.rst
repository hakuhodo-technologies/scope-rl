Tutorial
==========

.. _overview_ope:

Off-Policy Evaluation
~~~~~~~~~~
Off-policy evaluation (OPE) estimates the performance of a policy (referred to as *policy value*) using data collected by a behavior policy, which is often different from evaluation policies. 
It is particularly useful when we aim to evaluate the performance of candidate policies planned for deployment without actually executing them in an online environment.


.. card::
    :width: 75%
    :margin: auto
    :img-background: ./images/ope.png
    :text-align: center


Setup
----------
We consider a general reinforcement learning setup, which is formalized by Markov Decision Process (MDP) as :math:`\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, P_r, \gamma \rangle`.
:math:`\mathcal{S}` is the state space and :math:`\mathcal{A}` is the action space, which is either discrete or continuous.
Let :math:`\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{S})` is the state transition probability where :math:`\mathcal{T}(s' | s,a)` is the probability of observing state :math:`s'` after taking action :math:`a` given state :math:`s`.
:math:`P_r: \mathcal{S} \times \mathcal{A} \times \mathbb{R} \rightarrow [0,1]` is the probability distribution of the immediate reward.
Given :math:`P_r`, :math:`R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}` is the expected reward function where :math:`R(s,a) := \mathbb{E}_{r \sim P_r (r | s, a)}[r]` is the expected reward when taking action :math:`a` for state :math:`s`.
We also let :math:`\gamma \in (0,1]` be a discount factor. Finally, :math:`\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})` denotes a *policy* where :math:`\pi(a| s)` is the probability of taking action :math:`a` at a given state :math:`s`.
Note that we also denote :math:`d_0` as the initial state distribution.

In OPE, we are given a logged dataset :math:`\mathcal{D}` consisting of :math:`n` trajectories, each of which is generated by a behavior policy :math:`\pi_0` as follows.

.. math::

    \tau := \{ (s_t, a_t, s_{t+1}, r_t) \}_{t=0}^{T-1} \sim \underbrace{p(s_0) \prod_{t=0}^{T-1} \pi_0(a_t | s_t) \mathcal{T}(s_{t+1} | s_t, a_t) P_r (r_t | s_t, a_t)}_{:= p_{\pi_0}}

.. math::

    \mathcal{D} = \{\tau_i\}_{i=1}^n \sim p_{\pi_0}

The most typical OPE aims at the expected trajectory-wise given evaluation policy :math:`\pi` as the policy value:

.. math::

    J(\pi) := \mathbb{E}_{\tau \sim p_{\pi}} \left [ \sum_{t=0}^{T-1} \gamma^t r_{t} \right ]


The goal of OPE research is to estimate :math:`J(\pi)` using the logged data :math:`\mathcal{D}` collected by a different policy :math:`\pi_0` with an OPE estimator :math:`\hat{J}`. 
One can tell the accuracy of :math:`\hat{J}` by mean squared error (MSE).

.. math::
    
    \begin{aligned}
        \operatorname{MSE}(\hat{J}(\pi  ; \mathcal{D})): & =\mathbb{E}_{\tau  \sim \mathcal{D}}\left[(J(\pi)-\hat{J}(\pi ; \mathcal{D}))^2\right] \\
        & = \left(J(\pi)- \mathbb{E}_{\tau  \sim \mathcal{D}}\left[\hat{J}(\pi  ; \mathcal{D})\right] \right) ^2 + \mathbb{E}_{\tau  \sim \mathcal{D}}\left[ \left(\hat{J}(\pi  ; \mathcal{D})
        - \mathbb{E}_{\tau  \sim \mathcal{D}}\left[\hat{J}(\pi  ; \mathcal{D})\right] \right) ^2\right] \\\
        & =\operatorname{Bias}(\hat{J}(\pi  ; \mathcal{D}))^2+\mathbb{V}_{\tau  \sim p_{\pi}}[\hat{J}(\pi ; \mathcal{D})]
    \end{aligned}

MSE can be decomposed into bias squared and variance. Therefore, to reduce the MSE, it is important to reduce both of them. 
Thus, the following sections discuss the properties of OPE estimators wrt their bias and variance.

OPE estimators
----------

We discuss the properties of the following OPE estimator first in a theoretical manner and demonstrate the results with some toy experiments.

- Direct Method (DM)
- Trajectory-wise Importance Sampling (TIS)
- Per-Decision Importance Sampling (PDIS)
- Doubly Robust (DR)
- Self-Normalized 
- Marginalized Importance Sampling (MIS)
- Double Reinforcement Learning (DRL)
- Spectrum of Off-Policy (SOPE)


Direct Method (DM)
----------

DM :cite:`beygelzimer2009offset` is a model-based approach that uses the initial state value (estimated by e.g., Fitted Q Evaluation (FQE) :cite:`le2019batch`).
It first learns the Q-function and then leverages the learned Q-function as follows.

.. math::

    \hat{J}_{\mathrm{DM}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_{0}^{(i)}) \hat{Q^{\pi}}(s_{0}^{(i)}, a) = \frac{1}{n} \sum_{i=1}^n \hat{V^{\pi}}(s_{0}^{(i)})
    

:math:`\hat{Q}^{\pi}(s_t, a_t) \simeq \mathbb{E}_{\tau_{t:T-1}\sim p_{\pi}(\tau_{t:T-1}|s_t, a_t)}\left[\sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}\right]` is the estimated state-action value and :math:`\hat{V}^{\pi}(s_t) \simeq \mathbb{E}_{\tau_{t:T-1} \sim p_{\pi}(\tau_{t:T-1}|s_t)}\left[\sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}\right]` is the estimated state value.

.. math::

    \hat{Q}^{\pi}(s_t, a_t):=\arg \min _{Q(s_t, a_t) \in \mathcal{Q}} \left(Q^{\pi}(s_t, a_t)- \left(r_{t}+\mathbb{E}_{\pi\left(a \mid s_{t+1}\right)}\left[\hat{Q}^{\pi}(s_{t+1}, a)\right]\right)\right)^2
    
.. 1. $\hat{Q}_{H+1}(s, a):=0, \forall(s, a)$
.. 2. For $h=H, H-1, \ldots, 0$
.. - $y_{i, h}:=r_{i, h}+\mathbb{E}_{\pi\left(a_h \mid s_{i, h}\right)}\left[\hat{Q}_{h+1}\left(s_{i, h}, a_h\right)\right], \forall i$
.. - $\hat{Q}_h:=\arg \min _{Q_t \in \mathcal{Q}} \frac{1}{n} \sum_{i=1}^n\left(Q_h\left(s_{i, h}, a_{i, h}\right)-y_{i, h}\right)^2$

In FQE, the estimated state action value :math:`\hat{Q}^{\pi}(s_t, a_t)` is obtained by gradually decreasing the time :math:`t=T` and optimizing it as described in the above equation.

Now that you understand the definition of DM, the next step is to check an important property, bias of DM.

.. math::

    \begin{align*}
            \operatorname{Bias}[\hat{J}_{\mathrm{DM}}(\pi;D)] & = J(\pi)-  \mathbb{E}_{s_0\sim p(d_0)}\left[\hat{V}^{\pi}(s_0)\right]\\
            & = \mathbb{E}_{s_0\sim p(d_0)}\left[\sum_{a\in\mathcal{A}}\pi(a | s_0)Q^{\pi}(s_0, a)\right]-  \mathbb{E}_{s_0\sim p(d_0)}\left[\sum_{a\in\mathcal{A}}\pi(a | s_0)\hat{Q}^{\pi}(s_0, a)\right]\\
            & = \mathbb{E}_{s_0\sim p(d_0)}\left[\sum_{a\in\mathcal{A}}\pi(a | s_0)\left(Q^{\pi}(s_0, a)- \hat{Q}^{\pi}(s_0, a)\right) \right]
    \end{align*}

DM has lower variance compared to other estimators but can produce large bias caused by approximation errors :math:`Q^{\pi}(s_0, a)- \hat{Q}^{\pi}(s_0, a)`.

.. _implementation_tis:

Trajectory-wise Importance Sampling (TIS)
----------
TIS :cite:`precup2000eligibility` uses a importance sampling technique to correct the distribution shift between :math:`\pi` and :math:`\pi_0` as follows.

.. math::

    \hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t w_{0:T-1}^{(i)} r_t^{(i)}

where :math:`w_{0:T-1} := \prod_{t=0}^{T-1} (\pi(a_t | s_t) / \pi_0(a_t | s_t))` is the trajectory-wise importance weight. TIS is unbiased under the assumption of common support :math:`\forall(s_0, a_0, ..., s_{T-1}, a_{T-1}) \in S^{T-1} \times A^{T-1},  \prod_{t=0}^{T-1}\pi(a_t \mid s_t) > 0 \rightarrow \prod_{t=0}^{T-1}\pi_0(a_t \mid s_t) > 0`.

.. math::

    \mathbb{E}_{\tau \sim p_{\pi_0}(\tau)}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::

        &\mathbb{E}_{\tau}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]\\
        &=\mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1} \gamma^t w_{0:T-1} r_t \right] \\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_0|s_0)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{T-1}|s_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p(s_0)\pi(a_0|s_0)P_r(r_0|s_0, a_0)\mathcal{T}(s_{1}|s_0, a_0)\cdots \pi(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})}
        {p(s_0)\pi_0(a_0|s_0)P_r(r_0|s_0, a_0)\mathcal{T}(s_{1}|s_0, a_0)\cdots \pi_0(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=\sum_{\tau}p_{\pi_0}(\tau)\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\\
        &=\sum_{\tau}p_{\pi}(\tau)\sum_{t=0}^{T-1} \gamma^{t}r_t\\
        &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=J(\pi)


By using the importance sampling technique, TIS enables an unbiased estimation of the policy value. After examining unbiasedness, we will next focus on another important property, variance. To facilitate the derivation of variances, we will first express TIS recursively.

.. math::

    J_{\mathrm{TIS}}^{T-1-t} := w_t(w_{t+1:T-1} r_t + \gamma J_{\mathrm{TIS}}^{T-1-(t+1)})


.. dropdown:: proof

    .. math::

        J_{\mathrm{TIS}}^{T-1-t} &= \sum_{t' = t}^{T-1}\gamma^{t' -t}w_{t:T-1}r_{t'}\\
        &=w_{t:T}r_t + \sum_{t' = t+1}^{T-1}\gamma^{t' -t}w_{t:T-1}r_{t'}\\
        &=w_t\left(w_{t+1:T-1}r_t + \sum_{t' = t+1}^{T-1}\gamma^{t' -t}w_{t+1:T-1}r_{t'}\right)\\
        &=w_t\left(w_{t+1:T-1}r_t + \gamma\sum_{t' = t+1}^{T-1}\gamma^{t' -(t+1)}w_{t+1:T-1}r_{t'}\right)\\
        &=w_t\left(w_{t+1:T-1}r_t + \gamma J_{\mathrm{TIS}}^{T-1-(t+1)}\right)\\

The term :math:`(T-1-t)` in :math:`J_{\mathrm{TIS}}^{T-1-t}` represents the remaining trajectory length at time :math:`t`. When :math:`t=T-1`, :math:`J_{\mathrm{TIS}}^0 = 0`, and when :math:`t=0`, :math:`J_{\mathrm{TIS}}^{T-1} = J_{\mathrm{TIS}}`. 
With the recursive representation, variance of TIS are as follows. Here, we define 
:math:`w_{t} := \pi(a_{t} | s_{t}) / \pi_0(a_{t} | s_{t})`, and :math:`\mathbb{E}_t` is defined as follows.

.. math::
    \mathbb{E}_t= \mathbb{E}_{s_t, a_t, r_t}:= \mathbb{E}_{s_t, a_t, r_t}[\cdot \mid s_0, a_0, r_0, ..., s_{t-1}, a_{t-1}, r_{t-1}]


.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{TIS}}^{T-1-t}(\pi; \mathcal{D})] = \mathbb{V}_t[V(s_t)] + \mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[w_{t+1:T-1}r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}]\right]


.. dropdown:: proof

    .. math::

        &\mathbb{V}_{t}[\hat{J}_{\mathrm{TIS}}^{T-1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{TIS}}^{T-1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(w_t\left(w_{t+1:T-1}r_t+\gamma \hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} \right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(w_{t+1:T-1}r_t+\gamma \hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(w_{t+1:T-1}r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\because Q(s_t, a_t) =  R(s_t, a_t) + \mathbb{E}_{s_{t+1}}\left[\gamma V(s_{t+1})\right]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_tQ(s_t, a_t)+w_t\left(w_{t+1:T-1}r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\ 
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(w_tQ(s_t, a_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(w_{t+1:T-1}r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}-\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)] \because  w_tQ(s_t, a_t) \perp w_t\left(w_{t+1:T-1}r_t-R(s_t, a_t)\right) \perp w_t\gamma \left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right) \Biggm\vert s_t, a_t\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[w_{t+1:T-1}r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}]\right]+ \mathbb{V}_t[V(s_t)]\\

Variance of TIS includes the squared importance weight :math:`w_t` and variances of it, especially the third term :math:`\mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[w_{t+1:T-1}r_t ]\right]` contains :math:`w_{t+1:T-1}` (product of future importance weight up to :math:`T-1`). TIS has a very large variance due to these factors.
In short, TIS is unbiased, but suffers from hight variance.

.. The variance consists of three terms. The first term :math:`\mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]` includes the square of the trajectory-wise importance weight and the third term :math:`\mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]` includes the variance involving the trajectory-wise importance weights. Therefore, given a wide range of trajectory-wise importance weights, the variance is large.
.. In particular, when the trajectory length :math:`T` is large, TIS suffers from high variance due to the product of importance weights.


DM vs TIS Bias-Variance Trade-off 
^^^^^

In the previous part, we explained that, theoretically, DM tends to have a small variance but a large bias, while TIS is unbiased but tends to have a large variance. In this part, we will confirm these properties in a simple experiment.

.. card::
    :width:  100%
    :margin: auto
    :img-background: ./images/result_fig_n_trajectories_dm_tis.png
    :text-align: center

.. raw:: html

   <div style="height: 1em;"></div> 

The results are shown comparing bias, variance, and MSE for varying number of trajectories. The experiment uses the square of the bias instead of the bias to scale with the variance. 

Experiments use empirical bias rather than true bias. Therefore, experimental results often differ from the results of theoretical analysis. However, with a sufficiently large number of trials, the bias can be seen with a certain degree of accuracy.

DM works well with a small number of trajectories :math:`n`, whereas TIS is getting better with a large number of trajectories. DM(high bias, low variance) and TIS(low bias, high variance) are a trade-off between bias and variance.


DM vs TIS Curse of Horizon
^^^^^^

The following results show a comparison of DM and TIS when the trajectory length :math:`T` is varied. Note that the vertical axis is on a log scale, unlike the previous example.

.. card::
    :width: 100%
    :margin: auto
    :img-background: ./images/result_fig_step_per_trajectory_dm_tis.png
    :text-align: center

.. raw:: html

   <div style="height: 1em;"></div> 


.. TIS tends to have less bias than DM, and the bias decreases as the trajectory length :math:`T` increases. 
TIS tends to have less bias than DM,
On the other hand, the variance of TIS tends to be larger than that of DM, and the larger the trajectory length :math:`T` is, the larger the variance is.

Variance of TIS grows exponentially as the trajectory gets longer. We have looked at two estimators, DM and TIS, which are the most basic and leave much room for improvement. In particular, we will now introduce an estimator that reduces variance while maintaining the advantageous properties of TIS in terms of bias.


.. _implementation_pdis:

Per-Decision Importance Sampling (PDIS)
----------
PDIS :cite:`precup2000eligibility` leverages the sequential nature of the MDP to reduce the variance of TIS.
Specifically, since :math:`s_t` only depends on :math:`s_0, \ldots, s_{t-1}` and :math:`a_0, \ldots, a_{t-1}` and is independent of :math:`s_{t+1}, \ldots, s_{T}` and :math:`a_{t+1}, \ldots, a_{T}`,
PDIS only considers the importance weight of the past interactions when estimating :math:`r_t` as follows.

.. math::

    \hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t w_{0:t}^{(i)} r_t^{(i)}

where :math:`w_{0:t} := \prod_{t'=0}^t (\pi(a_{t'} | s_{t'}) / \pi_0(a_{t'} | s_{t'}))` is the importance weight of past interactions.
Like TIS, PDIS also satisfies the condition of unbiasedness under the assumption of common support.

.. math::

    \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::

        \mathbb{E}_{\tau}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]\\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}} \left[ \frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t  \right] \\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]
        \underbrace{\mathbb{E}_{\pi_0(a_0|s_0)\cdots\pi_0(a_t|s_t)}\left[\sum_{a_{t+1}}\cdots\sum_{a_{T-1}}\pi(a_{t+1}|s_{t+1})\cdots\pi(a_{T-1}|s_{T-1})\right]}_{=1} \\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]
        \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_{t+1}|s_{t+1})\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_{t+1}|s_{t+1})\cdots \pi_0(a_{T-1}|s_{T-1})}\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{\pi(a_0|s_0)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{T-1}|s_{T-1})} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=J(\pi)

As before, we represent PDIS recursively to calculate the variances.
when :math:`t=T-1`, :math:`J_{\mathrm{TIS}}^0 = 0`, and when :math:`t=0`, :math:`J_{\mathrm{TIS}}^{T-1} = J_{\mathrm{TIS}}`. 

Variance of PDIS is calculated as follows.

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{T-1-t}(\pi; \mathcal{D})] = \mathbb{V}_t[V(s_t)] +\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right] +\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)}]\right]


.. dropdown:: proof

    .. math::
        &\mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{T-1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{PDIS}}^{T-1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(w_t\left(r_t+\gamma \hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} \right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\because Q(s_t, a_t) =  R(s_t, a_t) + \mathbb{E}_{s_{t+1}}\left[\gamma V(s_{t+1})\right]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(w_tQ(s_t, a_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t-1)}-\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\because  w_tQ(s_t, a_t) \perp w_t\left(r_t-R(s_t, a_t)\right) \perp w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right) \Biggm\vert s_t, a_t\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)}]\right]+ \mathbb{V}_t[V(s_t)]\\

.. PDIS variance is decomposed by the calculation into four terms. The first three terms correspond to variances resulting from distinct sources of randomness at time step :math:`t`: :math:`\mathbb{V}[J(s_t)]` randomness in state transitions, :math:`\mathbb{E}_t[{w_t}^2\mathbb{V}_{t+1}[r_t]]` action stochasticity in reward randomness :math:`\pi_0`, and :math:`\mathbb{E}_t[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{PDIS}}^{T-t}(\pi; \mathcal{D})]]` variance in rewards. The fourth is a term that includes variances from future steps. PDIS remains unbiased while reducing the variance of TIS. 

Compared with the TIS variance calculated earlier, only the third term is different, indicating that PDIS excludes the coefficient :math:`w_{t+1:T-1}` of :math:`r_t` in TIS.
PDIS has a smaller variance than TIS by the product of these future importance weights :math:`w_{t+1:T-1}`. 

TIS vs PDIS
^^^^^^

Results are shown for TIS and its modified PDIS for varying the trajectory length :math:`T`.

.. card::
    :width: 100%
    :margin: auto
    :img-background: ./images/result_fig_step_per_trajectory_tis_pdis.png
    :text-align: center

.. raw:: html

   <div style="height: 1em;"></div> 


PDIS has less variance than TIS. When the trajectory length :math:`T` is large, it still suffers from variance.


.. _implementation_dr:

Doubly Robust (DR)
----------
DR :cite:`jiang2016doubly` :cite:`thomas2016data` is a hybrid of model-based estimation and importance sampling.
It introduces :math:`\hat{Q}` as a baseline estimation in the recursive form of PDIS and applies importance weighting only on its residual.

.. math::
    J_{\mathrm{DR}}^{T-1-t} := \sum_{a\in \mathcal{A}}\pi(a|s_t)\hat{Q}(s_t, a) + w_t(r_t + \gamma J_{\mathrm{DR}}^{T-1-(t+1)} - \hat{Q}(s_t, a_t))

The following DR is proposed, which restores the recursive DR defined in this way to its original form.

.. dropdown:: proof

    .. math::

        J_{\mathrm{DR}}^{T-1-0} &= \sum_{a\in \mathcal{A}}\pi(a|s_0)\hat{Q}(s_0, a) + w_0(r_0 + \gamma J_{\mathrm{DR}}^{T-1-1} - \hat{Q}(s_0, a_0))\\
        &= \sum_{a\in \mathcal{A}}\pi(a|s_0)\hat{Q}(s_0, a) + w_0(r_0  - \hat{Q}(s_0, a_0)) + w_0 \gamma \left(\sum_{a\in \mathcal{A}}\pi(a|s_{1})\hat{Q}(s_{1}, a) + w_{1}(r_{1} + \gamma J_{\mathrm{DR}}^{T-1-2} - \hat{Q}(s_1, a_1))\right)\\
        &= \sum_{a\in \mathcal{A}}\pi(a|s_0)\hat{Q}(s_0, a) + w_0\gamma\sum_{a\in \mathcal{A}}\pi(a|s_1)\hat{Q}(s_1, a) + w_0(r_0 -\hat{Q}(s_0, a_0)) + w_{0:1} \gamma (r_{1} - \hat{Q}(s_1, a_1)) + w_{0:1} \gamma J_{\mathrm{DR}}^{T-1-2}\\
        & \quad \quad \vdots\\
        &=\sum_{t=0}^{T-1} \gamma^tw_{0:t-1}\sum_{a \in \mathcal{A}} \pi(a | s_t) \hat{Q}(s_t, a) + \sum_{t=0}^{T-1} \gamma^t w_{0:t} (r_t - \hat{Q}(s_t, a_t))  \\
        &=\sum_{t=0}^{T-1} \gamma^t \left(w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1}\sum_{a \in \mathcal{A}} \pi(a | s_t) \hat{Q}(s_t, a) \right)\\


.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \left(w_{0:t}^{(i)} (r_t^{(i)} - \hat{Q}(s_t^{(i)}, a_t^{(i)})) + w_{0:t-1}^{(i)} \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_t^{(i)}, a) \right)

DR takes over the properties of PDIS and is unbiased under the assumption of common support.

.. math::

    \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::

        &\mathbb{E}_{\tau}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t \left (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right)\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi_0(a | s_t)}\left[\frac{\pi(a \mid s_t)}{\pi_0(a \mid s_t)}\hat{Q}(s_t, a)\right]\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim {p(d_0) \prod_{t=0}^{T-1} \pi_0(a_t | s_t) \mathcal{T}(s_{t+1} | s_t, a_t) P_r (r_t | s_t, a_t)}} \left [\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi_0(a | s_t)}\left[\frac{\pi(a \mid s_t)}{\pi_0(a \mid s_t)}\hat{Q}(s_t, a)\right]\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim { p(d_0) \prod_{t=0}^{T-1} \pi_0(a_t | s_t) \mathcal{T}(s_{t+1} | s_t, a_t) P_r (r_t | s_t, a_t)}} \left [\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \frac{\pi(a_t \mid s_t)}{\pi_0(a_t \mid s_t)}\hat{Q}(s_t, a_t)\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] \\
        &= J(\pi)

The following is a comparison of the properties of DR with PDIS.

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{T-1-t}(\pi; \mathcal{D})] = \mathbb{V}_t[V(s_t)]+ \mathbb{E}_{s_t}\left[\mathbb{V}_{a_t, r_t}\left[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t)) \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[{w_t}^2\mathbb{V}_{r_{t+1}}[r_t]\right] + \mathbb{E}_{s_t, a_t}\left[\gamma^2{w_t}^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}]\right] 

.. dropdown:: proof

    .. math::
        &\mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{T-1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{DR}}^{T-1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(\hat{V}(s_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{DR}}^{T-1-(t+1)} - \hat{Q}(s_t, a_t)\right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)-w_t\hat{Q}(s_t, a_t)+\hat{V}(s_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{DR}}^{T-1-(t+1)}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\because Q(s_t, a_t) =  R(s_t, a_t) + \mathbb{E}_{s_{t+1}}\left[\gamma V(s_{t+1})\right]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(-w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}-\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\because  -w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t)) \perp w_t\left(r_t-R(s_t, a_t)\right) \perp w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right) \Biggm\vert s_t, a_t\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ -w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}]\right]+ \mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{V}_{a_t, r_t}\left[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t)) \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[{w_t}^2\mathbb{V}_{r_{t+1}}[r_t]\right] + \mathbb{E}_{s_t, a_t}\left[\gamma^2{w_t}^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}]\right] + \mathbb{V}_t[V(s_t)] 

3 terms are the same as PDIS, but the second term :math:`\mathbb{E}_t\left[\mathbb{V}_t[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t))]\right]` differs from PDIS. DR reduces the variance of PDIS when :math:`\hat{Q}(\cdot)` is reasonably accurate to satisfy :math:`0 < \hat{Q}(\cdot) < 2 Q(\cdot)`. 


PDIS vs DR
^^^^^^
The results of the comparison between PDIS and DR for varying trajectory length :math:`T` are shown.

.. card::
    :width: 100%
    :margin: auto
    :img-background: ./images/result_fig_step_per_trajectory_pdis_dr.png
    :text-align: center

.. raw:: html

   <div style="height: 1em;"></div> 


DR has less variance than PDIS, and consequently, it also has a smaller MSE than PDIS. However, like PDIS, DR also depends on importance weights, so when the trajectory length :math:`T` is large, DR can still incur high variance.


Self-Normalized estimators
----------
Self-normalized estimators :cite:`kallus2019intrinsically` aim to reduce the scale of importance weight for the variance reduction purpose.
Specifically, it substitutes importance weight :math:`w_{\ast}` as follows.

.. math::

    \tilde{w}_{\ast} := \frac{w_{\ast}}{\sum_{i=1}^n w_{\ast}}

where :math:`\tilde{w}_{\ast}` is the self-normalized importance weight.

Self-normalized estimators are no longer unbiased but have variance bounded by :math:`r_{max}^2` while also being consistent.
Self-Normalized estimators can be specifically defined as Self-Normalized TIS estimator (SNTIS) and Self-Normalized PDIS estimator (SNPDIS), respectively, as follows.


.. math::

    \hat{J}_{\mathrm{SNTIS}} (\pi; \mathcal{D}) := \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \frac{w_{0:T-1}^{(i)}}{\sum_{i'=1}^n w_{0:T-1}^{(i')}} r_t^{(i)}

.. math::

    \hat{J}_{\mathrm{SNPDIS}} (\pi; \mathcal{D}) := \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \frac{w_{0:t}^{(i)}}{\sum_{i'=1}^n w_{0:t}^{(i')}} r_t^{(i)}


TIS vs SNTIS
^^^^^^

Here we compare TIS with SNTIS, which is a Self-Normalized version of TIS.

.. card::
    :width: 100%
    :margin: auto
    :img-background: ./images/result_fig_step_per_trajectory_tis_sntis.png
    :text-align: center

.. raw:: html

   <div style="height: 1em;"></div> 


SNTIS is able to reduce the variance while keeping the bias much the same compared to TIS, resulting in a reduced MSE.


We have seen TIS → PDIS → DR → Self-Normalized, and each of these estimators considered how to transform the importance weights, which were the cause of the variance. However, since the importance weights are dependent on the trajectory length, they do not provide a fundamental solution when the trajectory length becomes large.


.. _implementation_marginal_ope:

Marginalized Importance Sampling Estimators
----------


When the length of the trajectory :math:`T` is large, even per-decision importance weights can be exponentially large in the latter part of the trajectory.
To alleviate this, state marginal or state-action marginal importance weights can be used instead of the per-decision importance weight as follows :cite:`liu2018breaking` :cite:`uehara2020minimax`.

.. math::

    \rho(s_t, a_t) &:= \frac{d_t^{\pi}(s_t, a_t) }{ d_t^{\pi_0}(s_t, a_t) }\\
    \rho(s_t) &:= \frac{d_{t}^{\pi}(s_t)\pi(a_t|s_t)}{ d_t^{\pi_0}(s_t)\pi_0(a_t|s_t)}


.. This estimator is particularly useful when policy visits the same or similar states among different trajectories or different timesteps.
.. (e.g., when the state transition is something like :math:`\cdots \rightarrow s_1 \rightarrow s_2 \rightarrow s_1 \rightarrow s_2 \rightarrow \cdots` or when the trajectories always visit some particular state as :math:`\cdots \rightarrow s_{*} \rightarrow s_{1} \rightarrow s_{*} \rightarrow \cdots`)

When the importance weights are defined as above, the State-Action Marginal Importance Sampling estimator (SAMIS) and the State Marginal Importance Sampling estimator (SMIS) are defined as follows.

.. math::

    \hat{J}_{\mathrm{SAMIS}} (\pi; \mathcal{D})
    := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}, a_t^{(i)}) r_t^{(i)}

.. math::

    \hat{J}_{\mathrm{SMIS}} (\pi; \mathcal{D})
    := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}) w_t(s_t^{(i)}, a_t^{(i)}) r_t^{(i)}

Although the definition of importance weights is changed, SAMIS and SMIS satisfy unbiasedness as well as PDIS under the assumption of common support.

.. math::
    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SAMIS}} (\pi; \mathcal{D})]= J(\pi)

.. dropdown:: proof

    .. math::

        \text{we use the following formula: } d^{\pi}(s, a) := \left(\sum_{t=0}^{T-1} \gamma^{t} d_t^\pi(s, a)\right) /\left(\sum_{t=0}^{T-1}\gamma^{t}\right)

        \begin{aligned}
                \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SAMIS}} (\pi; \mathcal{D})]
                &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{d_t^{\pi}(s_t, a_t)}
                {d_t^{\pi_0}(s_t, a_t)} \gamma^{t}r_t \right]\\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi_0}(s_t, a_t)\frac{d^{\pi}(s, a)}
                {d^{\pi_0}(s, a)} \gamma^{t}R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi_0}(s, a)\frac{d^{\pi}(s, a)}
                {d^{\pi_0}(s, a)} R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi}(s, a) R(s, a) \\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi}(s_t, a_t)\gamma^{t}R(s, a) \\
                &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
                &=J(\pi)
        \end{aligned}

.. math::

    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SMIS}} (\pi; \mathcal{D})]= J(\pi)

.. dropdown:: proof

    .. math::

        \text{we use the following formula: } d^{\pi}(s) := \left(\sum_{t=0}^{T-1} \gamma^{t} d_t^\pi(s)\right) /\left(\sum_{t=0}^{T-1} \gamma^{t}\right)

        \begin{aligned}
                \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SMIS}} (\pi; \mathcal{D})]
                &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{d_t^{\pi}(s_t)\pi(a_t | s_t)}
                {d_t^{\pi_0}(s_t)\pi_0(a_t | s_t)} \gamma^{t}r_t \right]\\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi_0}(s_t, a_t)\pi_0(a_t | s_t)\frac{d^{\pi}(s)\pi(a_t | s_t)}
                {d^{\pi_0}(s)\pi_0(a_t | s_t)} \gamma^{t}R(s, a) \\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi_0}(s_t, a_t)\pi(a_t | s_t)\frac{d^{\pi}(s)}
                {d^{\pi_0}(s)} \gamma^{t}R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi_0}(s)\frac{d^{\pi}(s)}
                {d^{\pi_0}(s)} \pi(a | s)R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi}(s)  \pi(a | s)R(s, a) \\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi}(s_t)\pi(a_t|s_t)\gamma^{t}R(s, a) \\
                &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
                &=J(\pi)
        \end{aligned}


The Marginal estimator is an unbiased estimator when the marginalized importance weights :math:`\rho(s,a), \rho(s)` are known, but the true marginalized importance weights are often not available. Therefore, we have to estimate the marginal importance weights and use the estimated weights. In this case, there will be a bias due to the estimation error of the marginal importance weights. 

PDIS vs SAMIS
^^^^^^

We will now compare SAMIS and PDIS for varying trajectory length :math:`T`.

.. card::
    :width: 100%
    :margin: auto
    :img-background: ./images/result_fig_step_per_trajectory_pdis_samis.png
    :text-align: center

.. raw:: html

   <div style="height: 1em;"></div> 

    
SAMIS requires estimating state-action marginal importance weights, which introduces a bias, but it can reduce variance more than PDIS.

Finally, we introduce two advanced estimators.

.. _implementation_drl:

Double Reinforcement Learning (DRL)
----------
Comparing DR in the standard and marginal OPE, we notice that their formulation is slightly different as follows.

(DR in standard OPE)

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]) \right],

(DR in marginal OPE)

.. math::

    \hat{J}_{\mathrm{SAM-DR}} (\pi; \mathcal{D})
    &:= \mathbb{E}_{n} [\mathbb{E}_{a_0 \sim \pi(a_0 | s_0)} \hat{Q}(s_0, a_0)] \\
    & \quad \quad + \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{s, a}(s_t, a_t) (r_t + \gamma \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_{t+1}, a)] - \hat{Q}(s_t, a_t)) \right],

Then, a natural question arises, would it be possible to use marginal importance weight in DR in the standard formulation?

DRL :cite:`kallus2020double` leverages the marginal importance sampling in the standard OPE formulation as follows.

.. math::

    \hat{J}_{\mathrm{DRL}} (\pi; \mathcal{D})
    & := \frac{1}{n} \sum_{k=1}^K \sum_{i=1}^{n_k} \sum_{t=0}^{T-1} (w_s^j(s_{i,t}, a_{i, t}) (r_{i, t} - Q^j(s_{i, t}, a_{i, t})) \\
    & \quad \quad + w_s^j(s_{i, t-1}, a_{i, t-1}) \mathbb{E}_{a \sim \pi(a | s_t)}[Q^j(s_{i, t}, a)] )

DRL achieves the semiparametric efficiency with a consistent value predictor :math:`Q`. 
Therefore, to alleviate the potential bias introduced in :math:`Q`, DRL uses the "cross-fitting" technique to estimate the value function.
Specifically, let :math:`K` is the number of folds and :math:`\mathcal{D}_j` is the :math:`j`-th split of logged data consisting of :math:`n_k` samples.
Cross-fitting trains :math:`w^j` and :math:`Q^j` on the subset of data used for OPE, i.e., :math:`\mathcal{D} \setminus \mathcal{D}_j`.

DR vs DRL
^^^^^^

.. card::
    :width: 100%
    :margin: auto
    :img-background: ./images/result_fig_step_per_trajectory_dr_drl.png
    :text-align: center
    
.. raw:: html

   <div style="height: 1em;"></div> 

DRL can suppress the variance even when the length of the trajectory is large by using marginal importance weight, theoretically satisfying efficiency and robustness. The better the estimation of the Q function of DRL, the smaller the variance.

.. _implementation_sope:

Spectrum of Off-Policy Estimators (SOPE)
----------
While state marginal or state-action marginal importance weight effectively alleviates the variance of per-decision importance weight, the estimation error of marginal importance weights may introduce some bias in estimation. To alleviate this and control the bias-variance tradeoff more flexibly, SOPE uses the following interpolated importance weights :cite:`yuan2021sope`.

.. math::

    w_{\mathrm{SOPE}}(s_t, a_t) &=
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        \rho(s_{t-k}, a_{t-k}) \prod_{t'=t-k+1}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases} \\

    w_{\mathrm{SOPE}}(s_t, a_t) &=
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        \rho(s_{t-k}) \prod_{t'=t-k}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases}
    
where SOPE uses per-decision importance weight :math:`w_t(s_t, a_t) := \pi(a_t | s_t) / \pi_0(a_t | s_t)` for the :math:`k` most recent timesteps.
    
Specifically, SOPE-SAMIS, a combination of SAMIS and PDIS, can be defined as follows.

.. math::

    &\hat{J}_{\mathrm{SOPE-SAMIS}} (\pi; \mathcal{D})\\
    &:= \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{k-1} \gamma^t w_{\mathrm{SOPE}}^{(i)}(s_t, a_t) r_t^{(i)}\\
    &=\frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{k-1} \gamma^t w_{0:t}^{(i)} r_t^{(i)}
    + \frac{1}{n} \sum_{i=1}^n \sum_{t=k}^{T-1} \gamma^t \rho(s_{t-k}^{(i)}, a_{t-k}^{(i)}) w_{t-k+1:t}^{(i)} r_t^{(i)}



The figure below represents the concepts of PDIS, SOPE, and SAMIS. PDIS considers the trajectory up to time :math:`t` using decision unit importance weights :math:`w_{0:t}`, and the rewards :math:`r_t` are weighted accordingly (indicated in orange). On the other hand, SAMIS takes into account the probability of occurrence of :math:`(s_t, a_t)` through marginal importance weights :math:`\rho`, and weights the rewards :math:`r_t` (indicated in purple).

.. card::
    :width: 75%
    :margin: auto
    :img-background: ./images/figure_pdis_sope_samis.png
    :text-align: center


PDIS vs SAMIS vs SOPE
^^^^^^

In the simple experiment, SOPE is compared against SAMIS and PDIS. The horizontal axis represents :math:`k`, indicating how far SOPE uses the same importance weights as PDIS, thus labeled as number of step pdis. By varying :math:`k`, the balance between Marginal estimators and PDIS estimators can be controlled.

.. card::
    :width: 100%
    :margin: auto
    :img-background: ./images/result_fig_n_step_pdis_samis_pdis_sope.png
    :text-align: center

.. raw:: html

   <div style="height: 1em;"></div> 


.. SOPE can control the balance between marginal and per-decision estimators by changing n_step_pdis. As seen in the figure, SOPE is equal to SAMIS when n_step_pdis is 0 and is equal to pdis when n_step_pdis is the trajectory length :math:`T`. If n_step_pdis is large, bias can be reduced, if it is small, variance can be reduced. SOPE reduces MSE with less bias than SAMIS and less variance than PDIS. 

.. As can be seen from the figure, SOPE coincides with SAMIS when $k=0$, and aligns with PDIS at $k=T$ (the length of the trajectory). Increasing $k$ helps to reduce bias, while decreasing $k$ can control the variance. SOPE allows for consideration of the trade-off between bias and variance by adjusting $k$, and it can be designed to achieve a lower Mean Squared Error (MSE) compared to SAMIS and PDIS.

.. As can be seen from the figure, SOPE coincides with SAMIS when :math:`k=0`, and coincides with PDIS at :math:`k=T` (the length of the trajectory). Increasing :math:`k` helps to reduce bias, while decreasing :math:`k` helps to reduce variance. SOPE allows for consideration of the trade-off between bias and variance by adjusting :math:`k`, and it can be designed to achieve a lower Mean Squared Error (MSE) compared to SAMIS and PDIS.

As illustrated in the figure, SOPE coincides with SAMIS when :math:`k=0` and with PDIS when :math:`k=T` (the length of the trajectory). Increasing :math:`k` reduces bias, whereas decreasing :math:`k` reduces variance. SOPE, by adjusting :math:`k`, can consider the trade-off between bias and variance, and is designed to achieve a smaller Mean Squared Error (MSE) than SAMIS and PDIS.

.. raw:: html

    <div class="white-space-5px"></div>

.. grid::

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: installation
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Quickstart**

    .. grid-item::
        :columns: 8
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: _autogallery/index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Tutorial**

            .. grid-item-card::
                :link: index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Documentation**

