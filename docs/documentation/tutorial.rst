Tutorial
==========

.. _overview_ope:

Off-Policy Evaluation
~~~~~~~~~~
Off-policy evaluation is to estimate the policy value based on data collected from a behavior policy which is different from evaluation policies. It is particularly useful when we aim to evaluate the performance of candidate policies planned for deployment without actually executing it in an online environment.

Setup
----------
We consider a general reinforcement learning setup, which is formalized by Markov Decision Process (MDP) as :math:`\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, P_r, \gamma \rangle`.
:math:`\mathcal{S}` is the state space and :math:`\mathcal{A}` is the action space, which is either discrete or continuous.
Let :math:`\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{S})` is the state transition probability where :math:`\mathcal{T}(s' | s,a)` is the probability of observing state :math:`s'` after taking action :math:`a` given state :math:`s`.
:math:`P_r: \mathcal{S} \times \mathcal{A} \times \mathbb{R} \rightarrow [0,1]` is the probability distribution of the immediate reward.
Given :math:`P_r`, :math:`R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}` is the expected reward function where :math:`R(s,a) := \mathbb{E}_{r \sim P_r (r | s, a)}[r]` is the expected reward when taking action :math:`a` for state :math:`s`.
We also let :math:`\gamma \in (0,1]` be a discount factor. Finally, :math:`\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})` denotes a *policy* where :math:`\pi(a| s)` is the probability of taking action :math:`a` at a given state :math:`s`.
Note that we also denote :math:`d_0` as the initial state distribution.

In OPE, we are given a logged dataset :math:`\mathcal{D}` consisting of :math:`n` trajectories, each of which is generated by a behavior policy :math:`\pi_0` as follows.

.. math::

    \tau := \{ (s_t, a_t, s_{t+1}, r_t) \}_{t=0}^{T-1} \sim p(s_0) \prod_{t=0}^{T-1} \pi_0(a_t | s_t) \mathcal{T}(s_{t+1} | s_t, a_t) P_r (r_t | s_t, a_t)

.. math::

    \mathcal{D} = \{\tau_i\}_{i=1}^n \sim p_{\pi_0}

We aim at evaluating the *policy value* or the expected trajectory-wise reward of the given evaluation policy :math:`\pi`:

.. math::

    J(\pi) := \mathbb{E}_{\tau} \left [ \sum_{t=0}^{T-1} \gamma^t r_{t} \mid \pi \right ],


We aim to develop an estimator :math:`\hat{J}` to estimate the value of an evaluation policy :math:`\pi` (which is different from :math:`\pi_0`) using only the logged data in :math:`\mathcal{D}`. The accuracy of :math:`\hat{J}` is quantified by mean squared error (MSE)

.. math::
    
    \begin{aligned}
        \operatorname{MSE}(\hat{J}(\pi  ; \mathcal{D})): & =\mathbb{E}_{\tau  \sim p_{\pi}}\left[(J(\pi)-\hat{J}(\pi ; \mathcal{D}))^2\right] \\
        & = \left(J(\pi)- \mathbb{E}_{\tau  \sim p_{\pi}}\left[\hat{J}(\pi  ; \mathcal{D})\right] \right) ^2 + \mathbb{E}_{\tau  \sim p_{\pi}}\left[ \left(\hat{J}(\pi  ; \mathcal{D})
        - \mathbb{E}_{\tau  \sim p_{\pi}}\left[\hat{J}(\pi  ; \mathcal{D})\right] \right) ^2\right] \\\
        & =\operatorname{Bias}(\hat{J}(\pi  ; \mathcal{D}))^2+\mathbb{V}_{\tau  \sim p_{\pi}}[\hat{J}(\pi ; \mathcal{D})]
    \end{aligned}

MSE can be decomposed into bias squared and variance. Therefore, to reduce the MSE, it is important to reduce both the bias and the variance. 
Let's introduce the property of OPE estimators in terms of bias and variance.

OPE estimators
----------

**We introduce OPE estimators and explain it both theoretically and through simple experiments.** 

- Direct Method (DM)
- Trajectory-wise Importance Sampling (TIS)
- Per-Decision Importance Sampling (PDIS)
- Doubly Robust (DR)
- Self-Normalized 
- Marginalized Importance Sampling (MIS)
- Spectrum of Off-Policy (SOPE)
- Double Reinforcement Learning (DRL)


Direct Method (DM)
----------

DM :cite:`beygelzimer2009offset` is a model-based approach that uses the initial state value (estimated by e.g., Fitted Q Evaluation (FQE) :cite:`le2019batch`).
It first learns the Q-function and then leverages the learned Q-function as follows.

.. math::

    \hat{J}_{\mathrm{DM}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_{0}^{(i)}) \hat{Q^{\pi}}(s_{0}^{(i)}, a) = \frac{1}{n} \sum_{i=1}^n \hat{V^{\pi}}(s_{0}^{(i)})
    

:math:`\hat{Q}(s_t, a_t) \simeq \mathbb{E}_{\tau_{t:T-1}\sim p_{\pi}(\tau_{t:T-1}|s_t, a_t)}\left[\sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}\right]` is the estimated state-action value and :math:`\hat{V}(s_t) \simeq \mathbb{E}_{\tau_{t:T-1} \sim p_{\pi}(\tau_{t:T-1}|s_t)}\left[\sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}\right]` is the estimated state value.
Now that you understand the definition of DM, the next step is to check an important property, bias of DM.

.. math::

    \begin{align*}
            \operatorname{Bias}[\hat{J}_{\mathrm{DM}}(\pi;D)] & = J(\pi)-  \mathbb{E}_{s_0\sim p(d_0)}\left[\hat{V}^{\pi}(s_0)\right]\\
            & = \mathbb{E}_{s_0\sim p(d_0)}\left[\sum_{a\in\mathcal{A}}\pi(a | s_0)Q^{\pi}(s_0, a)\right]-  \mathbb{E}_{s_0\sim p(d_0)}\left[\sum_{a\in\mathcal{A}}\pi(a | s_0)\hat{Q}^{\pi}(s_0, a)\right]\\
            & = \mathbb{E}_{s_0\sim p(d_0)}\left[\sum_{a\in\mathcal{A}}\pi(a | s_0)\left(Q^{\pi}(s_0, a)- \hat{Q}^{\pi}(s_0, a)\right) \right]
    \end{align*}

DM has lower variance compared to other estimators but can produce large bias caused by approximation errors :math:`Q^{\pi}(s_0, a)- \hat{Q}^{\pi}(s_0, a)`.

.. _implementation_tis:

Trajectory-wise Importance Sampling (TIS)
----------
TIS :cite:`precup2000eligibility` uses a importance sampling technique to correct the distribution shift between :math:`\pi` and :math:`\pi_0` as follows.

.. math::

    \hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right]

where :math:`w_{0:T-1} := \prod_{t=0}^{T-1} (\pi(a_t | s_t) / \pi_0(a_t | s_t))` is the trajectory-wise importance weight. TIS is unbiased under the assumption of common support :math:`\forall(s_0, a_0, ..., s_{T-1}, a_{T-1}) \in S^{T-1} \times A^{T-1},  \prod_{t=0}^{T-1}\pi(a_t \mid s_t) > 0 \rightarrow \prod_{t=0}^{T-1}\pi_0(a_t \mid s_t) > 0`.

.. math::

    \mathbb{E}_{\tau \sim p_{\pi_0}(\tau)}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::

        &\mathbb{E}_{\tau}[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D})]\\
        &=\mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1} \gamma^t w_{0:T-1} r_t \right] \\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_0|s_0)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{T-1}|s_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p(s_0)\pi(a_0|s_0)P_r(r_0|s_0, a_0)\mathcal{T}(s_{1}|s_0, a_0)\cdots \pi(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})}
        {p(s_0)\pi_0(a_0|s_0)P_r(r_0|s_0, a_0)\mathcal{T}(s_{1}|s_0, a_0)\cdots \pi_0(a_{T-1}|s_{T-1})P_r(r_{T-1}|s_{T-1}, a_{T-1})} \sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=\sum_{\tau}p_{\pi_0}(\tau)\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\\
        &=\sum_{\tau}p_{\pi}(\tau)\sum_{t=0}^{T-1} \gamma^{t}r_t\\
        &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=J(\pi)


By the importance weight trick TIS enables an unbiased estimation of the policy value. After examining bias, we will next focus on another important property, variacne. To facilitate the derivation of variances, we will first express TIS recursively.

.. math::

    J_{\mathrm{TIS}}^{T-1-t} := w_t(w_{t+1:T-1} r_t + \gamma J_{\mathrm{TIS}}^{T-1-(t+1)})


.. dropdown:: proof

    .. math::

        J_{\mathrm{TIS}}^{T-1-t} &= \sum_{t' = t}^{T-1}\gamma^{t' -t}w_{t:T-1}r_{t'}\\
        &=w_{t:T}r_t + \sum_{t' = t+1}^{T-1}\gamma^{t' -t}w_{t:T-1}r_{t'}\\
        &=w_t\left(w_{t+1:T-1}r_t + \sum_{t' = t+1}^{T-1}\gamma^{t' -t}w_{t+1:T-1}r_{t'}\right)\\
        &=w_t\left(w_{t+1:T-1}r_t + \gamma\sum_{t' = t+1}^{T-1}\gamma^{t' -(t+1)}w_{t+1:T-1}r_{t'}\right)\\
        &=w_t\left(w_{t+1:T-1}r_t + \gamma J_{\mathrm{TIS}}^{T-1-(t+1)}\right)\\

The term :math:`(T-1-t)` in :math:`J_{\mathrm{TIS}}^{T-1-t}` represents the remaining trajectory length at time :math:`t`, and when :math:`t=T-1`, :math:`J_{\mathrm{TIS}}^0 = 0`, and when :math:`t=0`, :math:`J_{\mathrm{TIS}}^{T-1} = J_{\mathrm{TIS}}`. 
With the recursive representation, variance of TIS are as follows. Here, we define 
:math:`w_{t} := \pi(a_{t} | s_{t}) / \pi_0(a_{t} | s_{t})`, and :math:`\mathbb{E}_t` is defined as follows.

.. math::
    \mathbb{E}_t= \mathbb{E}_{s_t, a_t, r_t}:= \mathbb{E}_{s_t, a_t, r_t}[\cdot \mid s_0, a_0, r_0, ..., s_{t-1}, a_{t-1}, r_{t-1}]


.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{TIS}}^{T-1-t}(\pi; \mathcal{D})] = \mathbb{V}_t[V(s_t)] + \mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[w_{t+1:T-1}r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}]\right]


.. dropdown:: proof

    .. math::

        &\mathbb{V}_{t}[\hat{J}_{\mathrm{TIS}}^{T-1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{TIS}}^{T-1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(w_t\left(w_{t+1:T-1}r_t+\gamma \hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} \right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(w_{t+1:T-1}r_t+\gamma \hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(w_{t+1:T-1}r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\because Q(s_t, a_t) =  R(s_t, a_t) + \mathbb{E}_{s_{t+1}}\left[\gamma V(s_{t+1})\right]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_tQ(s_t, a_t)+w_t\left(w_{t+1:T-1}r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\ 
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(w_tQ(s_t, a_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(w_{t+1:T-1}r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}-\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)] \because  w_tQ(s_t, a_t) \perp w_t\left(w_{t+1:T-1}r_t-R(s_t, a_t)\right) \perp w_t\gamma \left(\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right) \Biggm\vert s_t, a_t\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[w_{t+1:T-1}r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{TIS}}^{T-1-(t+1)}]\right]+ \mathbb{V}_t[V(s_t)]\\

Variance of TIS includes squared importance weight :math:`w_t` and variances of it, especially the third term :math:`\mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[w_{t+1:T-1}r_t ]\right]` contains :math:`w_{t+1:T-1}` (product of future importance weight up to :math:`T-1`). TIS has a very large variance due to them.
In short, TIS is unbiased, but suffers from hight variance.

.. The variance consists of three terms. The first term :math:`\mathbb{E}_{s, a}\left[w_{1:T-1}^2\mathbb{V}_{r}\left[ V(s)\right] \right ]` includes the square of the trajectory-wise importance weight and the third term :math:`\mathbb{E}_{s}\left[\mathbb{V}_{a}\left[ w_{1:T-1} Q(s, a)\right]\right]` includes the variance involving the trajectory-wise importance weights. Therefore, given a wide range of trajectory-wise importance weights, the variance is large.
.. In particular, when the trajectory length :math:`T` is large, TIS suffers from high variance due to the product of importance weights.


DM vs TIS Bias-Variance Trade-off 
^^^^^

In the previous part, we explained that theoretically, DM tends to have a small variance but a large bias, while TIS is unbiased but tends to have a large variance. In this part, we will confirm the properties of each in a simple experiment.

.. card::
    :width:  100%
    :margin: auto
    :img-top: ./images/result_fig_n_trajectories_dm_tis.png
    :text-align: center

    DM vs TIS

The results are shown comparing bias, variance, and MSE for varying the number of trajectories. The experiment uses the square of the bias instead of the bias to scale with the variance. 

Experiments use empirical bias rather than true bias. Therefore, experimental results often differ from the results of theoretical analysis, but with a sufficiently large number of trials, the bias can be seen with a certain degree of accuracy.

DM works well with a small number of trajectories :math:`n`, TIS is getting better with a large number of trajectories. DM(high bias, low variance) and TIS(low bias, high variance) are a trade-off between bias and variance.


DM vs TIS Curse of Horizon
^^^^^^

The following results are for a comparison of DM and TIS when the trajectory length :math:`T` is varied. Note that the vertical axis is on a log scale, unlike the previous example.

    .. card::
        :width: 100%
        :margin: auto
        :img-top: ./images/result_fig_step_per_trajectory_dm_tis.png
        :text-align: center
        
        DM vs TIS


.. TIS tends to have less bias than DM, and the bias decreases as the trajectory length :math:`T` increases. 
TIS tends to have less bias than DM,
On the other hand, the variance of TIS tends to be larger than that of DM, and the larger the trajectory length :math:`T` is, the larger the variance is.

Variance of TIS grows exponentially as the trajectory gets longer. We have looked at two estimators, DM and TIS, which are the most basic and leave much room for improvement. In particular, we will now introduce an estimator that reduces the variance while maintaining the good properties of TIS with respect to the bias.


.. _implementation_pdis:

Per-Decision Importance Sampling (PDIS)
----------
PDIS :cite:`precup2000eligibility` leverages the sequential nature of the MDP to reduce the variance of TIS.
Specifically, since :math:`s_t` only depends on :math:`s_0, \ldots, s_{t-1}` and :math:`a_0, \ldots, a_{t-1}` and is independent of :math:`s_{t+1}, \ldots, s_{T}` and :math:`a_{t+1}, \ldots, a_{T}`,
PDIS only considers the importance weight of the past interactions when estimating :math:`r_t` as follows.

.. math::

    \hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t w_{0:t}^{(i)} r_t^{(i)}

where :math:`w_{0:t} := \prod_{t'=0}^t (\pi(a_{t'} | s_{t'}) / \pi_0(a_{t'} | s_{t'}))` is the importance weight of past interactions.
PDIS, like TIS, satisfies unbiasedness under the assumption of common support.

.. math::

    \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::

        \mathbb{E}_{\tau}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]\\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}} \left[ \frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t  \right] \\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]
        \underbrace{\mathbb{E}_{\pi_0(a_0|s_0)\cdots\pi_0(a_t|s_t)}\left[\sum_{a_{t+1}}\cdots\sum_{a_{T-1}}\pi(a_{t+1}|s_{t+1})\cdots\pi(a_{T-1}|s_{T-1})\right]}_{=1} \\
        &= \sum_{t=0}^{T-1} \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_0|s_0)\cdots \pi(a_{t}|s_{t})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{t}|s_{t})} \gamma^{t}r_t \right]
        \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{\pi(a_{t+1}|s_{t+1})\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_{t+1}|s_{t+1})\cdots \pi_0(a_{T-1}|s_{T-1})}\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{\pi(a_0|s_0)\cdots \pi(a_{T-1}|s_{T-1})}
        {\pi_0(a_0|s_0)\cdots \pi_0(a_{T-1}|s_{T-1})} \gamma^{t}r_t \right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\frac{p_{\pi}(\tau)}{p_{\pi_0}(\tau)}\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
        &=J(\pi)

As before, we represent PDIS recursively to calculate the variances.
when :math:`t=T-1`, :math:`J_{\mathrm{TIS}}^0 = 0`, and when :math:`t=0`, :math:`J_{\mathrm{TIS}}^{T-1} = J_{\mathrm{TIS}}`. 

Variance of PDIS is calculated as follows.

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{T-1-t}(\pi; \mathcal{D})] = \mathbb{V}_t[V(s_t)] +\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}[r_t]}\right] +\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)}]\right]


.. dropdown:: proof

    .. math::
        &\mathbb{V}_{t}[\hat{J}_{\mathrm{PDIS}}^{T-1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{PDIS}}^{T-1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(w_t\left(r_t+\gamma \hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} \right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\because Q(s_t, a_t) =  R(s_t, a_t) + \mathbb{E}_{s_{t+1}}\left[\gamma V(s_{t+1})\right]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_tQ(s_t, a_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(w_tQ(s_t, a_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t-1)}-\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\because  w_tQ(s_t, a_t) \perp w_t\left(r_t-R(s_t, a_t)\right) \perp w_t\gamma \left(\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right) \Biggm\vert s_t, a_t\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ w_tQ(s_t, a_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{PDIS}}^{T-1-(t+1)}]\right]+ \mathbb{V}_t[V(s_t)]\\

PDIS variance is decomposed by the calculation into four terms. The first three terms correspond to variances resulting from distinct sources of randomness at time step :math:`t`: :math:`\mathbb{V}[J(s_t)]` randomness in state transitions, :math:`\mathbb{E}_t[{w_t}^2\mathbb{V}_{t+1}[r_t]]` action stochasticity in reward randomness :math:`\pi_0`, and :math:`\mathbb{E}_t[\gamma^2{w_t}^2\mathbb{V}_{t+1}[\hat{J}_{\mathrm{PDIS}}^{H-t}(\pi; \mathcal{D})]]` variance in rewards. The fourth is a term that includes variances from future steps. PDIS remains unbiased while reducing the variance of TIS. 


TIS vs PDIS
^^^^^^

Results are shown for TIS and its modified PDIS for varying the trajectory length :math:`T`.

    .. card::
        :width: 100%
        :margin: auto
        :img-top: ./images/result_fig_step_per_trajectory_tis_pdis.png
        :text-align: center
        
        TIS vs PDIS


The PDIS has less variance than the TIS. When the trajectory length :math:`T` is large, it still suffers from variance.


.. _implementation_dr:

Doubly Robust (DR)
----------
DR :cite:`jiang2016doubly` :cite:`thomas2016data` is a hybrid of model-based estimation and importance sampling.
It introduces :math:`\hat{Q}` as a baseline estimation in the recursive form of PDIS and applies importance weighting only on its residual.

.. math::
    J_{\mathrm{DR}}^{T-1-t} := \sum_{a\in \mathcal{A}}\pi(a|s_t)\hat{Q}(s_t, a) + w_t(r_t + \gamma J_{\mathrm{DR}}^{T-1-(t+1)} - \hat{Q}(s_t, a_t))

The following DR is proposed, which restores the recursive DR defined in this way to its original form.

.. dropdown:: proof

    .. math::
        J_{\mathrm{DR}}^{T-1-0} &= \sum_{a\in \mathcal{A}}\pi(a|s_0)\hat{Q}(s_0, a) + w_0(r_0 + \gamma J_{\mathrm{DR}}^{T-1-1} - \hat{Q}^{\pi}(s_0, a_0))\\
        &= \sum_{a\in \mathcal{A}}\pi(a|s_0)\hat{Q}(s_0, a) + w_0(r_0  - \hat{Q}^{\pi}(s_0, a_0)) + w_0 \gamma \left(\sum_{a\in \mathcal{A}}\pi(a|s_{1})\hat{Q}(s_{1}, a) + w_{1}(r_{1} + \gamma J_{\mathrm{DR}}^{T-1-2} - \hat{Q}^{\pi}(s_1, a_1))\right)\\
        &= \sum_{a\in \mathcal{A}}\pi(a|s_0)\hat{Q}(s_0, a) + w_0\gamma\sum_{a\in \mathcal{A}}\pi(a|s_1)\hat{Q}(s_1, a) + w_0(r_0 -\hat{Q}^{\pi}(s_0, a_0)) + w_{0:1} \gamma (r_{1} - \hat{Q}^{\pi}(s_1, a_1)) + w_{0:1} \gamma J_{\mathrm{DR}}^{T-1-2}\\
        & \quad \quad \vdots\\
        &=\sum_{t=0}^{T-1} \gamma^tw_{0:t-1}\sum_{a \in \mathcal{A}} \pi(a | s_t) \hat{Q}(s_t, a) + \sum_{t=0}^{T-1} \gamma^t w_{0:t} (r_t - \hat{Q}(s_t, a_t))  \\
        &=\sum_{t=0}^{T-1} \gamma^t \left(w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1}\sum_{a \in \mathcal{A}} \pi(a | s_t) \hat{Q}(s_t, a) \right)\\


.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \left(w_{0:t}^{(i)} (r_t^{(i)} - \hat{Q}(s_t^{(i)}, a_t^{(i)})) + w_{0:t-1}^{(i)} \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_t^{(i)}, a) \right)

DR takes over the properties of PDIS and is unbiased under the assumption of common support.

.. math::

    \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})] = J(\pi)

.. dropdown:: proof

    .. math::
        &\mathbb{E}_{\tau}[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t \left (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]\right)\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi_0(a | s_t)}\left[\frac{\pi(a \mid s_t)}{\pi_0(a \mid s_t)}\hat{Q}(s_t, a)\right]\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim { \{(s_{t'}, s_{t'+1}, r_{t'}) \}_{t'=0}^{T-1}}} \prod_{t' = 0}^{T-1}\mathbb{E}_{a \sim \pi_0(\cdot | s_{t'})}\left [\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \mathbb{E}_{a \sim \pi_0(a | s_t)}\left[\frac{\pi(a \mid s_t)}{\pi_0(a \mid s_t)}\hat{Q}(s_t, a)\right]\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim { \{(s_{t'}, s_{t'+1}, r_{t'}) \}_{t'=0}^{T-1}}} \prod_{t' = 0}^{T-1}\mathbb{E}_{a \sim \pi_0(\cdot | s_{t'})}\left [\sum_{t=0}^{T-1} \gamma^t w_{0:t-1} \frac{\pi(a_t \mid s_t)}{\pi_0(a_t \mid s_t)}\hat{Q}(s_t, a_t)\right]\\
        &= \mathbb{E}_{\tau \sim p_{\pi_0}}[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D})]  - \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] + \mathbb{E}_{\tau \sim p_{\pi_0}} \left[\sum_{t=0}^{T-1} \gamma^t w_{0:t}\hat{Q}(s_t, a_t) \right] \\
        &= J(\pi)

The following is a comparison of the properties of DR with PDIS.

.. math::

    \mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{T-1-t}(\pi; \mathcal{D})] = \mathbb{V}_t[V(s_t)]+ \mathbb{E}_{s_t}\left[\mathbb{V}_{a_t, r_t}\left[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t)) \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[{w_t}^2\mathbb{V}_{r_{t+1}}[r_t]\right] + \mathbb{E}_{s_t, a_t}\left[\gamma^2{w_t}^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}]\right] 

.. dropdown:: proof

    .. math::
        &\mathbb{V}_{t}[\hat{J}_{\mathrm{DR}}^{T-1-t}(\pi; \mathcal{D})]\\
        &=\mathbb{E}_{t}\left[\left(\hat{J}_{\mathrm{DR}}^{T-1-t}\right)^2\right]-\Bigl(\mathbb{E}_{t}[V(s_t)]\Bigr)^2 \\
        &=\mathbb{E}_{t}\left[\left(\hat{V}(s_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{DR}}^{T-1-(t+1)} - \hat{Q}(s_t, a_t)\right)\right)^2\right]-\mathbb{E}_{t}[V(s_t)^2]+\mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_tQ(s_t, a_t)-w_t\hat{Q}(s_t, a_t)+\hat{V}(s_t)+w_t\left(r_t+\gamma \hat{J}_{\mathrm{DR}}^{T-1-(t+1)}-Q(s_t, a_t)\right)\right)^2-V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{t}\left[\left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right]+\mathbb{V}_{t}[V(s_t)]\because Q(s_t, a_t) =  R(s_t, a_t) + \mathbb{E}_{s_{t+1}}\left[\gamma V(s_{t+1})\right]\\
        &=\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_t}\left[
        \left(w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)+w_t\left(r_t-R(s_t, a_t)\right)+w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)\right)^2 -V(s_t)^2\right] \biggm\vert s_t, a_t\right]+\mathbb{V}_{t}[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{E}_{a_t, r_t}\left[
        \left(-w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t)\right)^2 - V(s_t)^2 \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_{t}^2\left(r_t -R(s_t, a_t)\right)^2\right]\right]\\
        &+\mathbb{E}_{s_t, a_t}\left[\mathbb{E}_{r_{t+1}}\left[w_t^2\gamma^2\left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}-\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right)^2\right]\right]+\mathbb{V}_{t}[V(s_t)]\because  -w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t)) \perp w_t\left(r_t-R(s_t, a_t)\right) \perp w_t\gamma \left(\hat{J}_{\mathrm{DR}}^{T-1-(t+1)} -\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\right) \Biggm\vert s_t, a_t\\
        &=\mathbb{E}_{s_t} \left[ \mathbb{V}_{a_t, r_t} \left [ -w_t(Q(s_t, a_t)-\hat{Q}(s_t, a_t))+\hat{V}(s_t) \mid s_t \right] \right ] + \mathbb{E}_{s_t,a_t} \left[w_t^2\mathbb{V}_{r_{t+1}}[r_t]\right]+\mathbb{E}_{s_t, a_t}\left[ w_t^2 \gamma^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}]\right]+ \mathbb{V}_t[V(s_t)]\\
        &=\mathbb{E}_{s_t}\left[\mathbb{V}_{a_t, r_t}\left[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t)) \mid s_t\right]\right]+\mathbb{E}_{s_t, a_t}\left[{w_t}^2\mathbb{V}_{r_{t+1}}[r_t]\right] + \mathbb{E}_{s_t, a_t}\left[\gamma^2{w_t}^2\mathbb{V}_{r_{t+1}}[\hat{J}_{\mathrm{DR}}^{T-1-(t+1)}]\right] + \mathbb{V}_t[V(s_t)] 

3 terms are the same as PDIS, but 3rd term :math:`\mathbb{E}_t\left[\mathbb{V}_t[w_t(\hat{Q}(s_t, a_t)-Q(s_t, a_t))]\right]` differs from PDIS. DR reduces the variance of PDIS when :math:`\hat{Q}(\cdot)` is reasonably accurate to satisfy :math:`0 < \hat{Q}(\cdot) < 2 Q(\cdot)`. 

PDIS vs DR
^^^^^^
The results of the comparison between PDIS and DR for varying trajectory length :math:`T` are shown.

    .. card::
        :width: 100%
        :margin: auto
        :img-top: ./images/result_fig_step_per_trajectory_pdis_dr.png
        :text-align: center
        
        PDIS vs DR


DR has less variance than PDIS consequently DR has a smaller mse than PDIS. However, DR also depends on importance weights such as PDIS, so when the trajectory length :math:`T` is large, DR can still incur high variance.


Self-Normalized estimators
----------
Self-normalized estimators :cite:`kallus2019intrinsically` aim to reduce the scale of importance weight for the variance reduction purpose.
Specifically, it substitutes importance weight :math:`w_{\ast}` as follows.

.. math::

    \tilde{w}_{\ast} := \frac{w_{\ast}}{\sum_{i=1}^n w_{\ast}}

where :math:`\tilde{w}_{\ast}` is the self-normalized importance weight.

Self-normalized estimators are no longer unbiased but have variance bounded by :math:`r_{max}^2` while also being consistent.
Self-Normalized estimators can be specifically defined as Self-Normalized TIS estimator (SNTIS) and Self-Normalized PDIS estimator (SNPDIS), respectively, as follows.


.. math::

    \hat{J}_{\mathrm{SNTIS}} (\pi; \mathcal{D}) := \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \frac{w_{0:T-1}^{(i)}}{\sum_{i'=1}^n w_{0:T-1}^{(i')}} r_t^{(i)}

.. math::

    \hat{J}_{\mathrm{SNPDIS}} (\pi; \mathcal{D}) := \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \frac{w_{0:t}^{(i)}}{\sum_{i'=1}^n w_{0:t}^{(i')}} r_t^{(i)}


TIS vs SNTIS
^^^^^^

Here we compare TIS with SNTIS, which is a Self-Normalized version of TIS.

    .. card::
        :width: 100%
        :margin: auto
        :img-top: ./images/result_fig_step_per_trajectory_tis_sntis.png
        :text-align: center
        
        TIS vs SNTIS


SNTIS is able to reduce the variance while keeping the bias much the same compared to TIS, resulting in a reduced MSE.


We have seen TIS → PDIS → DR → Self-Normalized, and each of these estimators considered how to transform the importance weights, which were the cause of the variance. However, since the importance weights are dependent on the trajectory length, they do not provide a fundamental solution when the trajectory length becomes long.


.. _implementation_marginal_ope:

Marginalized Importance Sampling Estimators
----------


When the length of the trajectory :math:`T` is large, even per-decision importance weights can be exponentially large in the latter part of the trajectory.
To alleviate this, state marginal or state-action marginal importance weights can be used instead of the per-decision importance weight as follows :cite:`liu2018breaking` :cite:`uehara2020minimax`.

.. math::

    \rho(s_t, a_t) &:= \frac{d_t^{\pi}(s_t, a_t) }{ d_t^{\pi_0}(s_t, a_t) }\\
    \rho(s_t) &:= \frac{d_{t}^{\pi}(s_t)\pi(a_t|s_t)}{ d_t^{\pi_0}(s_t)\pi_0(a_t|s_t)}


This estimator is particularly useful when policy visits the same or similar states among different trajectories or different timesteps.
(e.g., when the state transition is something like :math:`\cdots \rightarrow s_1 \rightarrow s_2 \rightarrow s_1 \rightarrow s_2 \rightarrow \cdots` or when the trajectories always visit some particular state as :math:`\cdots \rightarrow s_{*} \rightarrow s_{1} \rightarrow s_{*} \rightarrow \cdots`)

When the importance weights are defined as above, the State-Action Marginal Importance Sampling estimator (SAMIS) and the State Marginal Importance Sampling estimator (SMIS) are defined as follows.

.. math::

    \hat{J}_{\mathrm{SAMIS}} (\pi; \mathcal{D})
    := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}, a_t^{(i)}) r_t^{(i)}

.. math::

    \hat{J}_{\mathrm{SMIS}} (\pi; \mathcal{D})
    := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}) w_t(s_t^{(i)}, a_t^{(i)}) r_t^{(i)}

Although the definition of importance weights is changed, SAMIS and SMIS satisfy unbiasedness as well as PDIS under the assumption of common support.

.. math::
    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SAMIS}} (\pi; \mathcal{D})]= J(\pi)

.. dropdown:: proof

    .. math::

        d^{\pi}(s, a) := \left(\sum_{t=0}^{T-1} \gamma^{t} d_t^\pi(s, a)\right) /\left(\sum_{t=0}^{T-1}\gamma^{t}\right)を利用します

        \begin{align*}
                \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SAMIS}} (\pi; \mathcal{D})]
                &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{d_t^{\pi}(s_t, a_t)}
                {d_t^{\pi_0}(s_t, a_t)} \gamma^{t}r_t \right]\\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi_0}(s_t, a_t)\frac{d^{\pi}(s, a)}
                {d^{\pi_0}(s, a)} \gamma^{t}R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi_0}(s, a)\frac{d^{\pi}(s, a)}
                {d^{\pi_0}(s, a)} R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi}(s, a) R(s, a) \\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi}(s_t, a_t)\gamma^{t}R(s, a) \\
                &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
                &=J(\pi)
        \end{align*}

.. math::

    \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SMIS}} (\pi; \mathcal{D})]= J(\pi)

.. dropdown:: proof

    .. math::

        d^{\pi}(s) := \left(\sum_{t=0}^{T-1} \gamma^{t} d_t^\pi(s)\right) /\left(\sum_{t=0}^{T-1} \gamma^{t}\right)を利用します

        \begin{align*}
                \mathbb{E}_{\tau}[\hat{J}_{\mathrm{SMIS}} (\pi; \mathcal{D})]
                &= \mathbb{E}_{\tau \sim p_{\pi_0}}\left[\sum_{t=0}^{T-1}\frac{d_t^{\pi}(s_t)\pi(a_t | s_t)}
                {d_t^{\pi_0}(s_t)\pi_0(a_t | s_t)} \gamma^{t}r_t \right]\\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi_0}(s_t, a_t)\pi_0(a_t | s_t)\frac{d^{\pi}(s)\pi(a_t | s_t)}
                {d^{\pi_0}(s)\pi_0(a_t | s_t)} \gamma^{t}R(s, a) \\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi_0}(s_t, a_t)\pi(a_t | s_t)\frac{d^{\pi}(s)}
                {d^{\pi_0}(s)} \gamma^{t}R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi_0}(s)\frac{d^{\pi}(s)}
                {d^{\pi_0}(s)} \pi(a | s)R(s, a) \\
                &=\left( \sum_{t=0}^{T-1}\gamma^{t}\right)\sum_{s, a}d^{\pi}(s)  \pi(a | s)R(s, a) \\
                &= \sum_{s, a}\sum_{t=0}^{T-1}d_t^{\pi}(s_t)\pi(a_t|s_t)\gamma^{t}R(s, a) \\
                &= \mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t=0}^{T-1} \gamma^{t}r_t\right]\\
                &=J(\pi)
        \end{align*}


The Marginal estimator is an unbiased estimator when the marginalized importance weights :math:`\rho(s,a), \rho(s)` are known, but the true marginalized importance weights are often not available. Therefore, we have to estimate the marginal importance weights** and use the estimated weights. In this case, there will be a bias due to the estimation error of the marginal importance weights. 

PDIS vs SAMIS
^^^^^^

We will now compare SAMIS and PDIS for varying trajectory length :math:`T`.

    .. card::
        :width: 100%
        :margin: auto
        :img-top: ./images/result_fig_step_per_trajectory_pdis_samis.png
        :text-align: center
        
        PDIS vs SAMIS

SAMIS requires estimating state-action marginal importance weights, which introduces a bias, but it can reduce variance more than PDIS.

Finally, we introduce two advanced estimators.

.. _implementation_drl:

Double Reinforcement Learning (DRL)
----------
Comparing DR in the standard and marginal OPE, we notice that their formulation is slightly different as follows.

(DR in standard OPE)

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)]) \right],

(DR in marginal OPE)

.. math::

    \hat{J}_{\mathrm{SAM-DR}} (\pi; \mathcal{D})
    &:= \mathbb{E}_{n} [\mathbb{E}_{a_0 \sim \pi(a_0 | s_0)} \hat{Q}(s_0, a_0)] \\
    & \quad \quad + \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{s, a}(s_t, a_t) (r_t + \gamma \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_{t+1}, a)] - \hat{Q}(s_t, a_t)) \right],

Then, a natural question arises, would it be possible to use marginal importance weight in DR in the standard formulation?

DRL :cite:`kallus2020double` leverages the marginal importance sampling in the standard OPE formulation as follows.

.. math::

    \hat{J}_{\mathrm{DRL}} (\pi; \mathcal{D})
    & := \frac{1}{n} \sum_{k=1}^K \sum_{i=1}^{n_k} \sum_{t=0}^{T-1} (w_s^j(s_{i,t}, a_{i, t}) (r_{i, t} - Q^j(s_{i, t}, a_{i, t})) \\
    & \quad \quad + w_s^j(s_{i, t-1}, a_{i, t-1}) \mathbb{E}_{a \sim \pi(a | s_t)}[Q^j(s_{i, t}, a)] )

DRL achieves the semiparametric efficiency with a consistent value predictor :math:`Q`. 
Therefore, to alleviate the potential bias introduced in :math:`Q`, DRL uses the "cross-fitting" technique to estimate the value function.
Specifically, let :math:`K` is the number of folds and :math:`\mathcal{D}_j` is the :math:`j`-th split of logged data consisting of :math:`n_k` samples.
Cross-fitting trains :math:`w^j` and :math:`Q^j` on the subset of data used for OPE, i.e., :math:`\mathcal{D} \setminus \mathcal{D}_j`.

DR vs DRL
^^^^^^

    .. card::
        :width: 100%
        :margin: auto
        :img-top: ./images/result_fig_step_per_trajectory_dr_drl.png
        :text-align: center
        
        DR vs DRL

DRL can suppress the variance even when the length of the trajectory is large by using marginal importance weight, theoretically satisfying efficiency and robustness. The better the estimation of the Q function of DRL, the smaller the variance.

.. _implementation_sope:

Spectrum of Off-Policy Estimators (SOPE)
----------
While state marginal or state-action marginal importance weight effectively alleviates the variance of per-decision importance weight, the estimation error of marginal importance weights may introduce some bias in estimation. To alleviate this and control the bias-variance tradeoff more flexibly, SOPE uses the following interpolated importance weights :cite:`yuan2021sope`.

.. math::

    w_{\mathrm{SOPE}}(s_t, a_t) &=
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        \rho(s_{t-k}, a_{t-k}) \prod_{t'=t-k+1}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases} \\

    w_{\mathrm{SOPE}}(s_t, a_t) &=
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        \rho(s_{t-k}) \prod_{t'=t-k}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases}
    
where SOPE uses per-decision importance weight :math:`w_t(s_t, a_t) := \pi(a_t | s_t) / \pi_0(a_t | s_t)` for the :math:`k` most recent timesteps.
    
Specifically, SOPE-SAMIS, a combination of SAMIS and PDIS, can be defined as follows.

.. math::

    &\hat{J}_{\mathrm{SOPE-SAMIS}} (\pi; \mathcal{D})\\
    &:= \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{k-1} \gamma^t w_{\mathrm{SOPE}}^{(i)}(s_t, a_t) r_t^{(i)}\\
    &=\frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{k-1} \gamma^t w_{0:t}^{(i)} r_t^{(i)}
    + \frac{1}{n} \sum_{i=1}^n \sum_{t=k}^{T-1} \gamma^t \rho(s_{t-k}^{(i)}, a_{t-k}^{(i)}) w_{t-k+1:t}^{(i)} r_t^{(i)}


PDIS vs SAMIS vs SOPE
^^^^^^

    .. card::
        :width: 100%
        :margin: auto
        :img-top: ./images/result_fig_n_step_pdis_samis_pdis_sope.png
        :text-align: center
        
        PDIS vs SAMIS vs SOPE


SOPE can control the balance between marginal and per-decision estimators by changing n_step_pdis. As seen in the figure, SOPE is equal to SAMIS when n_step_pdis is 0 and is equal to pdis when n_step_pdis is the trajectory length :math:`T`. If n_step_pdis is large, bias can be reduced, if it is small, variance can be reduced. SOPE reduces MSE with less bias than SAMIS and less variance than PDIS. 


.. raw:: html

    <div class="white-space-5px"></div>

.. grid::

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: installation
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                <<< Prev
                **Quickstart**

    .. grid-item::
        :columns: 8
        :margin: 0
        :padding: 0

    .. grid-item::
        :columns: 2
        :margin: 0
        :padding: 0

        .. grid::
            :margin: 0

            .. grid-item-card::
                :link: _autogallery/index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Tutorial**

            .. grid-item-card::
                :link: index
                :link-type: doc
                :shadow: none
                :margin: 0
                :padding: 0

                Next >>>
                **Documentation**

